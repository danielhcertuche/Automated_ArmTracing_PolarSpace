{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6959a43a-0219-443f-a2a1-640c608dafaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def process_spiral_arms_polar_intermediate(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    eps_polar=0.7,\n",
    "    min_samples_polar=18,\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,  # Para detectar un gap y dividir el grupo en 2, si es muy grande\n",
    "    do_subdivide=True    # Activar o desactivar la subdivisión\n",
    "):\n",
    "    \"\"\"\n",
    "    Versión \"intermedia\" que:\n",
    "      1. Aplica DBSCAN en (r, theta).\n",
    "      2. Fusiona clusters cercanos.\n",
    "      3. Ajuste lineal iterativo (bondadoso), pero controlando la variación de la pendiente (20%).\n",
    "      4. Reasigna descartados.\n",
    "      5. (Opcional) Subdivide un grupo en 2 si detecta un gap grande en theta.\n",
    "      6. Filtra por PA al final.\n",
    "\n",
    "    Parámetros:\n",
    "      - slope_variation_threshold: variación relativa de pendiente máxima permitida (ej: 0.20 para 20%).\n",
    "      - max_theta_gap: umbral para dividir un grupo en 2 segmentos si hay un salto grande en theta.\n",
    "      - do_subdivide: si es True, se subdivide al final si se detecta un gap grande.\n",
    "\n",
    "    Retorna:\n",
    "      - valid_groups: Lista de grupos (diccionarios) con sus parámetros ajustados.\n",
    "      - df: DataFrame original con las columnas añadidas (r, θ, cluster).\n",
    "    \"\"\"\n",
    "    # 1. Cargar datos\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "\n",
    "    # 2. Convertir a coordenadas polares\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "\n",
    "    # 3. Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 4. DBSCAN en el plano (r, θ)\n",
    "    X_polar = df_filtered[['r', 'theta']].values\n",
    "    dbscan = DBSCAN(eps=eps_polar, min_samples=min_samples_polar)\n",
    "    df_filtered['cluster'] = dbscan.fit_predict(X_polar)\n",
    "\n",
    "    # 5. Fusión de clusters cercanos\n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "\n",
    "    # Crear diccionario de clusters\n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "\n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "\n",
    "    # 6. Inicializar grupos\n",
    "    model = LinearRegression()\n",
    "\n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "\n",
    "    def calculate_pa(slope, intercept):\n",
    "        if intercept != 0:\n",
    "            return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "        return np.nan\n",
    "\n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "\n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']].values\n",
    "            y_ = pts['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "\n",
    "        groups.append({\n",
    "            'cluster_id': cid,\n",
    "            'points': pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "\n",
    "    # 7. Asignación iterativa de puntos (incluye control de pendiente)\n",
    "    df_local = df_filtered.copy()\n",
    "    discarded_points = []\n",
    "\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "\n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                # Verificar distancia a los puntos\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    # Recalcular ajuste\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "\n",
    "                    # Controlar variación relativa de la pendiente\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= slope_variation_threshold:\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "\n",
    "    # 8. Reasignar descartados\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    # Verificar PA\n",
    "                    if g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                        X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                        y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                        model.fit(X_new, y_new)\n",
    "                        new_slope = model.coef_[0]\n",
    "                        old_slope = g['slope']\n",
    "                        if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= slope_variation_threshold:\n",
    "                            g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                            g['slope'] = new_slope\n",
    "                            g['intercept'] = model.intercept_\n",
    "                            g['pa'] = calculate_pa(new_slope, g['intercept'])\n",
    "                            assigned = True\n",
    "                            break\n",
    "            if not assigned:\n",
    "                # Crear grupo nuevo\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'cluster_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "\n",
    "    # 9. Subdivisión en 2 segmentos si hay un gap muy grande (opcional)\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "\n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        \"\"\"\n",
    "        Subdivide en 2 si encuentra un gap > max_gap en theta.\n",
    "        \"\"\"\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            # No hay gap grande\n",
    "            return [group]\n",
    "        # Tomar el primer gap para dividir\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "\n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        # Subdividir cada grupo en 2 si hay gap\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "\n",
    "    # 10. Filtrar por PA\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "\n",
    "    return valid_groups, df\n",
    "\n",
    "###############################################################################\n",
    "# EJEMPLO DE EJECUCIÓN Y GRAFICACIÓN\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.155\n",
    "    radius_tolerance = 0.45\n",
    "    point_distance_tolerance = 0.59\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    eps_polar = 0.7\n",
    "    min_samples_polar = 18\n",
    "    slope_variation_threshold = 0.20  # 20%\n",
    "    max_theta_gap = 15.0\n",
    "    do_subdivide = True\n",
    "\n",
    "    valid_groups, df = process_spiral_arms_polar_intermediate(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        point_distance_tolerance=point_distance_tolerance,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        eps_polar=eps_polar,\n",
    "        min_samples_polar=min_samples_polar,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        do_subdivide=do_subdivide\n",
    "    )\n",
    "\n",
    "    print(\"Total de grupos finales:\", len(valid_groups))\n",
    "\n",
    "    # Gráfica en (θ vs r)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['theta'], df['r'], s=1, color='silver', alpha=0.5, label='Datos Originales')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_vals = pts['theta']\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(theta_vals, r_vals, s=10, alpha=0.8, color=color,\n",
    "                    label=f\"Grupo {idx+1}, PA={pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(theta_vals.min(), theta_vals.max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Ajustes intermedios en (r, θ) - Halo {id_halo}\")\n",
    "    #plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfica polar\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(10, 8))\n",
    "    theta_all_rad = np.radians(df['theta'])\n",
    "    ax.scatter(theta_all_rad, df['r'], s=1, color='gray', alpha=0.5, label='Datos Originales')\n",
    "\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_rad = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_rad, r_vals, s=10, alpha=0.8, color=color,\n",
    "                   label=f\"Grupo {idx+1} PA: {pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "\n",
    "    ax.set_title(f\"Ajustes intermedios en proyección polar - Halo {id_halo}\", fontsize=14, pad=20)\n",
    "    #ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=9)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d07e3f53-a4fd-47b5-a9d4-720f2e356273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bfab9a-b35a-4a77-a225-389b4f9974b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0f7d46-0df4-47d4-bac6-4dcfba30eba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "def process_spiral_arms_interactive(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    eps_polar=0.7,\n",
    "    min_samples_polar=18,\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10  # cada cuántas iteraciones se guarda un estado\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa los datos y retorna una lista de estados intermedios para visualización interactiva.\n",
    "    Solo se consideran los puntos con theta entre 50 y 250 grados.\n",
    "    \n",
    "    Retorna:\n",
    "       - states: lista de estados intermedios (cada estado es un diccionario con grupos, descartados, y puntos restantes)\n",
    "       - final_groups: grupos finales después de todo el proceso.\n",
    "       - df: DataFrame original filtrado.\n",
    "    \"\"\"\n",
    "    # 1. Cargar datos\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    \n",
    "    # 2. Convertir a coordenadas polares\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    \n",
    "    # Filtrar theta entre 50 y 250\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    # 3. Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 4. DBSCAN en el plano (r, θ)\n",
    "    X_polar = df_filtered[['r', 'theta']].values\n",
    "    dbscan = DBSCAN(eps=eps_polar, min_samples=min_samples_polar)\n",
    "    df_filtered['cluster'] = dbscan.fit_predict(X_polar)\n",
    "    \n",
    "    # 5. Fusión de clusters cercanos\n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "    \n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "    \n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "    \n",
    "    # 6. Inicializar grupos con ajuste lineal (por cada cluster)\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "    \n",
    "    def calculate_pa(slope, intercept):\n",
    "        if intercept != 0:\n",
    "            return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "        return np.nan\n",
    "    \n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "    \n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']].values\n",
    "            y_ = pts['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'cluster_id': cid,\n",
    "            'points': pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # 7. Asignación iterativa de puntos, guardando estados intermedios\n",
    "    df_local = df_filtered.copy()\n",
    "    discarded_points = []\n",
    "    states = []  # se guardarán estados intermedios\n",
    "    iteration = 0\n",
    "    \n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "        \n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= slope_variation_threshold:\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        \n",
    "        iteration += 1\n",
    "        # Guardar el estado cada 'snapshot_interval' iteraciones o al finalizar\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            groups_copy = copy.deepcopy(groups)\n",
    "            discarded_copy = copy.deepcopy(discarded_points)\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': groups_copy,\n",
    "                'discarded': discarded_copy,\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 8. Reasignar descartados (etapa final, no interactiva)\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                        X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                        y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                        model.fit(X_new, y_new)\n",
    "                        new_slope = model.coef_[0]\n",
    "                        old_slope = g['slope']\n",
    "                        if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= slope_variation_threshold:\n",
    "                            g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                            g['slope'] = new_slope\n",
    "                            g['intercept'] = model.intercept_\n",
    "                            g['pa'] = calculate_pa(new_slope, g['intercept'])\n",
    "                            assigned = True\n",
    "                            break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'cluster_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "    \n",
    "    # 9. (Opcional) Subdividir grupos si hay un gap grande en theta\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    # 10. Filtrar grupos por PA válido\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    # Agregar estado final\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "def plot_intermediate_state(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica el estado intermedio en el plano polar:\n",
    "      - Fondo: todos los datos en gris pálido.\n",
    "      - Grupos asignados: cada grupo con un color y su línea de ajuste (si se ha calculado).\n",
    "      - Puntos descartados: marcados con una 'x' negra.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(subplot_kw={}, figsize=(10, 8))\n",
    "    \n",
    "    # Fondo: todos los datos\n",
    "    theta_all = np.radians(df_all['theta'])\n",
    "    r_all = df_all['r']\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    # Graficar cada grupo\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        theta_vals = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    \n",
    "    # Graficar puntos descartados\n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        theta_desc = np.radians(df_desc['theta'])\n",
    "        r_desc = df_desc['r']\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723a4f70-b07b-4438-93b9-3988857adc1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ejemplo de ejecución interactiva:\n",
    "states, final_groups, df_filtered = process_spiral_arms_interactive(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.155,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    eps_polar=0.7,\n",
    "    min_samples_polar=18,\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    ")\n",
    "\n",
    "print(\"Total de estados guardados:\", len(states))\n",
    "print(\"Total de grupos finales:\", len(final_groups))\n",
    "\n",
    "# Widget interactivo para ver el proceso iterativo\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e59f44-b912-49eb-ab07-5cbb56338339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d0c5b8-79b4-4dbb-961a-d0b1b4b18959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0279d22d-6391-4d50-a317-0589e76ef9c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f899a3-a77d-451f-997b-1eb5c8afef92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93750c6b-4123-47f4-af17-2d863151c06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### edición código - JUANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e23346-2b62-446b-8f63-3e36d7b7745b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# Función auxiliar para calcular distancia respecto a una línea (ajuste lineal)\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "# Función auxiliar para calcular PA\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "# Función para agregar un punto a un grupo temporal (si coincide en θ y en r)\n",
    "def add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window):\n",
    "    assigned = False\n",
    "    for tg in temp_groups:\n",
    "        tg_theta_min = tg['points']['theta'].min()\n",
    "        tg_theta_max = tg['points']['theta'].max()\n",
    "        tg_r_min = tg['points']['r'].min()\n",
    "        tg_r_max = tg['points']['r'].max()\n",
    "        if ((point['theta'] >= tg_theta_min - theta_fit_window/2) and \n",
    "            (point['theta'] <= tg_theta_max + theta_fit_window/2)) and \\\n",
    "           ((point['r'] >= tg_r_min - r_fit_window/2) and \n",
    "            (point['r'] <= tg_r_max + r_fit_window/2)):\n",
    "            tg['points'] = pd.concat([tg['points'], point.to_frame().T], ignore_index=True)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        new_tg = {'points': point.to_frame().T.copy()}\n",
    "        temp_groups.append(new_tg)\n",
    "\n",
    "# Función para revisar y fusionar (o promover) los grupos temporales\n",
    "def review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range):\n",
    "    new_temp_groups = []\n",
    "    for tg in temp_groups:\n",
    "        if len(tg['points']) < 3:\n",
    "            new_temp_groups.append(tg)\n",
    "            continue\n",
    "        X_tg = tg['points'][['theta']]\n",
    "        y_tg = tg['points']['r']\n",
    "        model.fit(X_tg, y_tg)\n",
    "        tg_slope = model.coef_[0]\n",
    "        tg_intercept = model.intercept_\n",
    "        tg_mean = tg['points']['theta'].mean()\n",
    "        merged = False\n",
    "        for mg in groups:\n",
    "            mg_min = mg['points']['theta'].min()\n",
    "            mg_max = mg['points']['theta'].max()\n",
    "            if mg_min <= tg_mean <= mg_max:\n",
    "                dist_values = tg['points'].apply(lambda row: calculate_distance(mg['slope'], mg['intercept'], row['theta'], row['r']), axis=1)\n",
    "                if dist_values.mean() < radius_tolerance:\n",
    "                    mg['points'] = pd.concat([mg['points'], tg['points']], ignore_index=True)\n",
    "                    model.fit(mg['points'][['theta']], mg['points']['r'])\n",
    "                    new_slope = model.coef_[0]\n",
    "                    if abs(new_slope - mg['slope'])/(abs(mg['slope'])+1e-12) < slope_variation_threshold:\n",
    "                        mg['slope'] = new_slope\n",
    "                        mg['intercept'] = model.intercept_\n",
    "                        mg['pa'] = calculate_pa(mg['slope'], mg['intercept'])\n",
    "                    merged = True\n",
    "                    break\n",
    "        if not merged:\n",
    "            new_group = {\n",
    "                'cluster_id': None,\n",
    "                'points': tg['points'],\n",
    "                'slope': tg_slope,\n",
    "                'intercept': tg_intercept,\n",
    "                'pa': calculate_pa(tg_slope, tg_intercept)\n",
    "            }\n",
    "            groups.append(new_group)\n",
    "    temp_groups.clear()\n",
    "    for tg in new_temp_groups:\n",
    "        temp_groups.append(tg)\n",
    "\n",
    "def process_spiral_arms_interactive(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    eps_polar=0.7,\n",
    "    min_samples_polar=18,\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10,     # Cada cuántas iteraciones se guarda un estado\n",
    "    theta_fit_window=30.0,    # Ventana en grados para ajustar la recta localmente\n",
    "    r_fit_window=10.0,        # Ventana fija para r\n",
    "    fusion_theta_threshold=7.0,  # Diferencia máxima fija en θ para fusionar\n",
    "    fusion_r_threshold=7.0       # Diferencia máxima fija en r para fusionar\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa los datos y retorna una lista de estados intermedios para visualización interactiva.\n",
    "    Se filtran los puntos con θ entre 50 y 250 grados.\n",
    "    \n",
    "    La fusión de grupos se realiza utilizando condiciones fijas:\n",
    "      - Se fusionan si la diferencia entre el límite inferior de un grupo y el límite superior de otro \n",
    "        (y viceversa) es menor que fusion_theta_threshold, y\n",
    "      - la diferencia en los límites de r es menor que fusion_r_threshold.\n",
    "    \"\"\"\n",
    "    # 1. Cargar datos\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    \n",
    "    # 2. Convertir a coordenadas polares\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    # 3. Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 4. DBSCAN en el plano (r, θ)\n",
    "    X_polar = df_filtered[['r', 'theta']].values\n",
    "    dbscan = DBSCAN(eps=eps_polar, min_samples=min_samples_polar)\n",
    "    df_filtered['cluster'] = dbscan.fit_predict(X_polar)\n",
    "    \n",
    "    # 5. Fusión de clusters cercanos\n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "    \n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "    \n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "    \n",
    "    # 6. Inicializar grupos principales a partir de los clusters obtenidos\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']].values\n",
    "            y_ = pts['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'cluster_id': cid,\n",
    "            'points': pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Preparar estados y grupos temporales\n",
    "    df_local = df_filtered.copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    states = []\n",
    "    temp_groups = []\n",
    "    iteration = 0\n",
    "    temp_review_interval = snapshot_interval * 15\n",
    "    \n",
    "    # 7. Asignación iterativa de puntos\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Buscar grupo candidato que contenga el punto\n",
    "        for g in groups:\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            if not (g_theta_min <= theta_pt <= g_theta_max and g_r_min <= r_pt <= g_r_max):\n",
    "                continue\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            if len(subset) == 0:\n",
    "                continue\n",
    "            r_min = subset['r'].min()\n",
    "            r_max = subset['r'].max()\n",
    "            if not (r_min <= r_pt <= r_max):\n",
    "                continue\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            if old_slope is None:\n",
    "                continue\n",
    "            if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) < 0.10:\n",
    "                continue\n",
    "            dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "            if dist > radius_tolerance:\n",
    "                continue\n",
    "            if best_distance is None or dist < best_distance:\n",
    "                best_distance = dist\n",
    "                best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = best_group['points'][(best_group['points']['theta'] >= window_min) & (best_group['points']['theta'] <= window_max)]\n",
    "            if len(subset) < 2:\n",
    "                subset = best_group['points']\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "            \n",
    "            # Fusión fija con grupos anteriores: se fusionan solo si las diferencias en ambos límites son menores que los umbrales fijos.\n",
    "            merged_flag = True\n",
    "            while merged_flag:\n",
    "                merged_flag = False\n",
    "                for other_group in groups:\n",
    "                    if other_group is best_group:\n",
    "                        continue\n",
    "                    best_theta_min = best_group['points']['theta'].min()\n",
    "                    best_theta_max = best_group['points']['theta'].max()\n",
    "                    best_r_min = best_group['points']['r'].min()\n",
    "                    best_r_max = best_group['points']['r'].max()\n",
    "                    \n",
    "                    other_theta_min = other_group['points']['theta'].min()\n",
    "                    other_theta_max = other_group['points']['theta'].max()\n",
    "                    other_r_min = other_group['points']['r'].min()\n",
    "                    other_r_max = other_group['points']['r'].max()\n",
    "                    \n",
    "                    if (abs(best_theta_min - other_theta_max) < fusion_theta_threshold and\n",
    "                        abs(best_theta_max - other_theta_min) < fusion_theta_threshold and\n",
    "                        abs(best_r_min - other_r_max) < fusion_r_threshold and\n",
    "                        abs(best_r_max - other_r_min) < fusion_r_threshold):\n",
    "                        merged_points = pd.concat([best_group['points'], other_group['points']], ignore_index=True)\n",
    "                        model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                        best_group['points'] = merged_points\n",
    "                        best_group['slope'] = model.coef_[0]\n",
    "                        best_group['intercept'] = model.intercept_\n",
    "                        best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "                        # Eliminar other_group por identidad\n",
    "                        for idx, grp in enumerate(groups):\n",
    "                            if grp is other_group:\n",
    "                                groups.pop(idx)\n",
    "                                break\n",
    "                        merged_flag = True\n",
    "                        break\n",
    "                        \n",
    "        else:\n",
    "            # Si no hay grupo candidato, intentar fusionar con el último grupo principal sin filtro de pendiente\n",
    "            merged_with_last = False\n",
    "            if len(groups) > 0:\n",
    "                last_g = groups[-1]\n",
    "                last_theta_min = last_g['points']['theta'].min()\n",
    "                last_theta_max = last_g['points']['theta'].max()\n",
    "                last_r_min = last_g['points']['r'].min()\n",
    "                last_r_max = last_g['points']['r'].max()\n",
    "                if (last_theta_min <= theta_pt <= last_theta_max and last_r_min <= r_pt <= last_r_max):\n",
    "                    window_min = theta_pt - theta_fit_window/2\n",
    "                    window_max = theta_pt + theta_fit_window/2\n",
    "                    subset_last = last_g['points'][(last_g['points']['theta'] >= window_min) & (last_g['points']['theta'] <= window_max)]\n",
    "                    if len(subset_last) > 0:\n",
    "                        r_min_l = subset_last['r'].min()\n",
    "                        r_max_l = subset_last['r'].max()\n",
    "                        if r_min_l <= r_pt <= r_max_l:\n",
    "                            subset_last = pd.concat([subset_last, point.to_frame().T], ignore_index=True)\n",
    "                            model.fit(subset_last[['theta']], subset_last['r'])\n",
    "                            new_slope_l = model.coef_[0]\n",
    "                            new_intercept_l = model.intercept_\n",
    "                            dist_l = calculate_distance(new_slope_l, new_intercept_l, theta_pt, r_pt)\n",
    "                            if dist_l < radius_tolerance:\n",
    "                                last_g['points'] = pd.concat([last_g['points'], point.to_frame().T], ignore_index=True)\n",
    "                                model.fit(last_g['points'][['theta']], last_g['points']['r'])\n",
    "                                last_g['slope'] = model.coef_[0]\n",
    "                                last_g['intercept'] = model.intercept_\n",
    "                                last_g['pa'] = calculate_pa(last_g['slope'], last_g['intercept'])\n",
    "                                merged_with_last = True\n",
    "            if not merged_with_last:\n",
    "                add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window)\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        if iteration % temp_review_interval == 0:\n",
    "            review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "        \n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_groups': copy.deepcopy(temp_groups),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    if len(temp_groups) > 0:\n",
    "        review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "    \n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'temp_groups': copy.deepcopy(temp_groups),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if state['temp_groups']:\n",
    "        for idx, tg in enumerate(state['temp_groups']):\n",
    "            pts = tg['points']\n",
    "            plt.scatter(pts['theta'], pts['r'], s=10, color='black', marker='x', label=f\"Temp {idx+1}\")\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a32e1c2-7e50-4bbb-9569-7f5ff872af3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecución interactiva:\n",
    "states, final_groups, df_filtered = process_spiral_arms_interactive(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.155,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    eps_polar=0.7,\n",
    "    min_samples_polar=18,\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10,\n",
    "    theta_fit_window=25.10,\n",
    "    r_fit_window=2.50 \n",
    ")\n",
    "\n",
    "print(\"Total de estados guardados:\", len(states))\n",
    "print(\"Total de grupos finales:\", len(final_groups))\n",
    "\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state_cartesian(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d68b770a-cd0c-4576-aab5-b0bd33921fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d2b27a1-1584-4db7-a6e3-e56e178d0a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ---------------------------\n",
    "# Funciones auxiliares\n",
    "# ---------------------------\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"Calcula la diferencia absoluta entre r observado y r predicho por la línea.\"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"Calcula el ángulo de inclinación (PA) en grados, a partir de la pendiente e intercepto.\"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def update_group_fit(group, model):\n",
    "    \"\"\"Recalcula el ajuste lineal (pendiente, intercepto y PA) para los puntos de un grupo.\"\"\"\n",
    "    X = group['points'][['theta']]\n",
    "    y = group['points']['r']\n",
    "    if len(group['points']) < 2:\n",
    "        return\n",
    "    model.fit(X, y)\n",
    "    group['slope'] = model.coef_[0]\n",
    "    group['intercept'] = model.intercept_\n",
    "    group['pa'] = calculate_pa(group['slope'], group['intercept'])\n",
    "\n",
    "def merge_groups_if_close(groups, fusion_theta_threshold, fusion_r_threshold, model):\n",
    "    \"\"\"\n",
    "    Fusiona grupos si la diferencia entre los límites de sus cajas (bounding boxes)\n",
    "    en θ y en r es menor a los umbrales fijos.\n",
    "    \"\"\"\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged = False\n",
    "        for i in range(len(groups)):\n",
    "            for j in range(i + 1, len(groups)):\n",
    "                g1 = groups[i]\n",
    "                g2 = groups[j]\n",
    "                # Obtener límites de g1\n",
    "                g1_theta_min = g1['points']['theta'].min()\n",
    "                g1_theta_max = g1['points']['theta'].max()\n",
    "                g1_r_min = g1['points']['r'].min()\n",
    "                g1_r_max = g1['points']['r'].max()\n",
    "                # Límites de g2\n",
    "                g2_theta_min = g2['points']['theta'].min()\n",
    "                g2_theta_max = g2['points']['theta'].max()\n",
    "                g2_r_min = g2['points']['r'].min()\n",
    "                g2_r_max = g2['points']['r'].max()\n",
    "                # Verificar diferencias fijas en los límites:\n",
    "                theta_diff_lower = abs(g1_theta_min - g2_theta_max)\n",
    "                theta_diff_upper = abs(g1_theta_max - g2_theta_min)\n",
    "                r_diff_lower = abs(g1_r_min - g2_r_max)\n",
    "                r_diff_upper = abs(g1_r_max - g2_r_min)\n",
    "                if (theta_diff_lower < fusion_theta_threshold and theta_diff_upper < fusion_theta_threshold and\n",
    "                    r_diff_lower < fusion_r_threshold and r_diff_upper < fusion_r_threshold):\n",
    "                    # Fusionar g2 en g1 y actualizar\n",
    "                    merged_points = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    g1['points'] = merged_points\n",
    "                    update_group_fit(g1, model)\n",
    "                    groups.pop(j)\n",
    "                    merged = True\n",
    "                    break\n",
    "            if merged:\n",
    "                break\n",
    "\n",
    "def add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window):\n",
    "    \"\"\"\n",
    "    Agrega un punto a un grupo temporal si este punto se encuentra dentro de\n",
    "    ventanas fijas (theta_fit_window y r_fit_window) de un grupo temporal existente.\n",
    "    \"\"\"\n",
    "    assigned = False\n",
    "    for tg in temp_groups:\n",
    "        tg_theta_min = tg['points']['theta'].min()\n",
    "        tg_theta_max = tg['points']['theta'].max()\n",
    "        tg_r_min = tg['points']['r'].min()\n",
    "        tg_r_max = tg['points']['r'].max()\n",
    "        if ((point['theta'] >= tg_theta_min - theta_fit_window/2) and \n",
    "            (point['theta'] <= tg_theta_max + theta_fit_window/2)) and \\\n",
    "           ((point['r'] >= tg_r_min - r_fit_window/2) and \n",
    "            (point['r'] <= tg_r_max + r_fit_window/2)):\n",
    "            tg['points'] = pd.concat([tg['points'], point.to_frame().T], ignore_index=True)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        new_tg = {'points': point.to_frame().T.copy()}\n",
    "        temp_groups.append(new_tg)\n",
    "\n",
    "def review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold):\n",
    "    \"\"\"\n",
    "    Revisa los grupos temporales; si tienen al menos 3 puntos se ajusta la recta y se\n",
    "    intenta fusionarlos con algún grupo principal si la distancia promedio es menor a radius_tolerance.\n",
    "    \"\"\"\n",
    "    new_temp_groups = []\n",
    "    for tg in temp_groups:\n",
    "        if len(tg['points']) < 3:\n",
    "            new_temp_groups.append(tg)\n",
    "            continue\n",
    "        X_tg = tg['points'][['theta']]\n",
    "        y_tg = tg['points']['r']\n",
    "        model.fit(X_tg, y_tg)\n",
    "        tg_slope = model.coef_[0]\n",
    "        tg_intercept = model.intercept_\n",
    "        tg_mean = tg['points']['theta'].mean()\n",
    "        merged = False\n",
    "        for mg in groups:\n",
    "            mg_theta_min = mg['points']['theta'].min()\n",
    "            mg_theta_max = mg['points']['theta'].max()\n",
    "            if mg_theta_min <= tg_mean <= mg_theta_max:\n",
    "                dist_values = tg['points'].apply(lambda row: calculate_distance(mg['slope'], mg['intercept'], row['theta'], row['r']), axis=1)\n",
    "                if dist_values.mean() < radius_tolerance:\n",
    "                    mg['points'] = pd.concat([mg['points'], tg['points']], ignore_index=True)\n",
    "                    update_group_fit(mg, model)\n",
    "                    merged = True\n",
    "                    break\n",
    "        if not merged:\n",
    "            new_group = {\n",
    "                'cluster_id': None,\n",
    "                'points': tg['points'],\n",
    "                'slope': tg_slope,\n",
    "                'intercept': tg_intercept,\n",
    "                'pa': calculate_pa(tg_slope, tg_intercept)\n",
    "            }\n",
    "            groups.append(new_group)\n",
    "    temp_groups.clear()\n",
    "    for tg in new_temp_groups:\n",
    "        temp_groups.append(tg)\n",
    "\n",
    "# ---------------------------\n",
    "# Función principal optimizada\n",
    "# ---------------------------\n",
    "def process_spiral_arms_interactive(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    eps_polar=0.7,\n",
    "    min_samples_polar=18,\n",
    "    slope_variation_threshold=0.20,  # Variación máxima de pendiente del 20%\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10,       # Cada cuántas iteraciones se guarda un estado\n",
    "    theta_fit_window=30.0,      # Ventana en grados para ajuste local\n",
    "    r_fit_window=10.0,          # Ventana fija para r\n",
    "    fusion_theta_threshold=7.0, # Diferencia máxima fija en θ para fusión\n",
    "    fusion_r_threshold=7.0      # Diferencia máxima fija en r para fusión\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa los datos, agrupa puntos y retorna estados intermedios para visualización.\n",
    "    \n",
    "    Se agrupan puntos en grupos principales manteniendo que la variación en la pendiente al actualizar\n",
    "    el ajuste local no sea mayor al 20%. Cada grupo calcula su PA de forma independiente, y al final\n",
    "    se filtran los grupos cuyo PA (valor absoluto) esté dentro del rango flexible_pa_range.\n",
    "    \n",
    "    Además, se fusionan grupos contiguos (con umbrales fijos) si sus límites en θ y en r están muy próximos.\n",
    "    \"\"\"\n",
    "    # Cargar y convertir datos\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    # Filtrado por percentil\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # DBSCAN en (r, theta)\n",
    "    X_polar = df_filtered[['r', 'theta']].values\n",
    "    dbscan = DBSCAN(eps=eps_polar, min_samples=min_samples_polar)\n",
    "    df_filtered['cluster'] = dbscan.fit_predict(X_polar)\n",
    "    \n",
    "    # Fusionar clusters cercanos\n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "    \n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "    \n",
    "    # Inicializar grupos principales a partir de los clusters\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']]\n",
    "            y_ = pts['r']\n",
    "            model.fit(X_, y_)\n",
    "            groups.append({\n",
    "                'cluster_id': cid,\n",
    "                'points': pts.copy(),\n",
    "                'slope': model.coef_[0],\n",
    "                'intercept': model.intercept_,\n",
    "                'pa': calculate_pa(model.coef_[0], model.intercept_)\n",
    "            })\n",
    "        else:\n",
    "            groups.append({\n",
    "                'cluster_id': cid,\n",
    "                'points': pts.copy(),\n",
    "                'slope': None,\n",
    "                'intercept': None,\n",
    "                'pa': None\n",
    "            })\n",
    "    \n",
    "    # Preparar para la asignación iterativa\n",
    "    df_local = df_filtered.copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    states = []\n",
    "    temp_groups = []  # Puntos que no se asignaron aún a ningún grupo\n",
    "    iteration = 0\n",
    "    temp_review_interval = snapshot_interval * 15\n",
    "    \n",
    "    # Asignación iterativa de puntos\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Buscar grupo candidato: el punto debe estar dentro de los límites (theta y r) del grupo\n",
    "        for g in groups:\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            if not (g_theta_min <= theta_pt <= g_theta_max and g_r_min <= r_pt <= g_r_max):\n",
    "                continue\n",
    "            # Tomar una ventana centrada en el punto\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            if len(subset) == 0:\n",
    "                continue\n",
    "            # Verificar que el valor de r del punto esté dentro de la ventana\n",
    "            if not (subset['r'].min() <= r_pt <= subset['r'].max()):\n",
    "                continue\n",
    "            # Actualizar ajuste local con el nuevo punto\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            if g['slope'] is None:\n",
    "                continue\n",
    "            # Solo se acepta si la variación en la pendiente es menor o igual al 20%\n",
    "            if abs(new_slope - g['slope'])/(abs(g['slope'])+1e-12) > slope_variation_threshold:\n",
    "                continue\n",
    "            dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "            if dist > radius_tolerance:\n",
    "                continue\n",
    "            if best_distance is None or dist < best_distance:\n",
    "                best_distance = dist\n",
    "                best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            update_group_fit(best_group, model)\n",
    "            # Intentar fusionar con otros grupos usando umbrales fijos\n",
    "            merge_groups_if_close(groups, fusion_theta_threshold, fusion_r_threshold, model)\n",
    "        else:\n",
    "            # Si no se encontró grupo candidato, se intenta fusionar con el último grupo (sin filtro de pendiente)\n",
    "            merged_with_last = False\n",
    "            if len(groups) > 0:\n",
    "                last_g = groups[-1]\n",
    "                if (last_g['points']['theta'].min() <= theta_pt <= last_g['points']['theta'].max() and\n",
    "                    last_g['points']['r'].min() <= r_pt <= last_g['points']['r'].max()):\n",
    "                    last_g['points'] = pd.concat([last_g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    update_group_fit(last_g, model)\n",
    "                    merged_with_last = True\n",
    "            if not merged_with_last:\n",
    "                add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % temp_review_interval == 0:\n",
    "            review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold)\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_groups': copy.deepcopy(temp_groups),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    if len(temp_groups) > 0:\n",
    "        review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold)\n",
    "    \n",
    "    # Opcional: subdividir grupos por gaps en theta (como en versiones anteriores)\n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = None, None, None\n",
    "        s2, i2, pa2 = None, None, None\n",
    "        if len(pts1) >= 2:\n",
    "            model.fit(pts1[['theta']], pts1['r'])\n",
    "            s1 = model.coef_[0]\n",
    "            i1 = model.intercept_\n",
    "            pa1 = calculate_pa(s1, i1)\n",
    "        if len(pts2) >= 2:\n",
    "            model.fit(pts2[['theta']], pts2['r'])\n",
    "            s2 = model.coef_[0]\n",
    "            i2 = model.intercept_\n",
    "            pa2 = calculate_pa(s2, i2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            final_groups.extend(subdivide_if_gap(g, max_theta_gap))\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    # Filtrar grupos según su PA de forma independiente\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'temp_groups': copy.deepcopy(temp_groups),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if state['temp_groups']:\n",
    "        for idx, tg in enumerate(state['temp_groups']):\n",
    "            pts = tg['points']\n",
    "            plt.scatter(pts['theta'], pts['r'], s=10, color='black', marker='x', label=f\"Temp {idx+1}\")\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3991949b-b6ce-44a7-9e83-bad44bbf89f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecución interactiva:\n",
    "states, final_groups, df_filtered = process_spiral_arms_interactive(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.155,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    eps_polar=0.7,\n",
    "    min_samples_polar=18,\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10,\n",
    "    theta_fit_window=25.10,\n",
    "    r_fit_window=2.50 \n",
    ")\n",
    "\n",
    "print(\"Total de estados guardados:\", len(states))\n",
    "print(\"Total de grupos finales:\", len(final_groups))\n",
    "\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state_cartesian(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "968a208d-1c2a-4e8e-8d54-1fe5f1afd24a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "167923f3-c866-4483-a1ff-619dbefb9657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica un estado en el plano (theta, r).\n",
    "    - state['groups'] es una lista de diccionarios con 'points' (DataFrame).\n",
    "    - df_all es el DataFrame original filtrado, para mostrar todos los puntos en gris.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Puntos completos (en gris)\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    \n",
    "    # Colores para cada grupo\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'darkblue']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "    \n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def add_point_to_groups(point, groups, theta_fit_window, r_fit_window):\n",
    "    \"\"\"\n",
    "    Intenta agregar 'point' a uno de los 'groups' ya existentes.\n",
    "    La condición es que el punto esté dentro de la 'caja delimitadora' \n",
    "    del grupo extendida por las ventanas fijas (theta_fit_window, r_fit_window).\n",
    "    \n",
    "    Retorna True si se agregó a algún grupo, False en caso contrario.\n",
    "    \"\"\"\n",
    "    theta_pt = point['theta']\n",
    "    r_pt = point['r']\n",
    "    \n",
    "    for g in groups:\n",
    "        # Caja delimitadora actual del grupo\n",
    "        theta_min = g['theta_min']\n",
    "        theta_max = g['theta_max']\n",
    "        r_min = g['r_min']\n",
    "        r_max = g['r_max']\n",
    "        \n",
    "        # Ventana extendida\n",
    "        if ((theta_pt >= theta_min - theta_fit_window/2) and \n",
    "            (theta_pt <= theta_max + theta_fit_window/2) and\n",
    "            (r_pt >= r_min - r_fit_window/2) and \n",
    "            (r_pt <= r_max + r_fit_window/2)):\n",
    "            # Se agrega el punto al grupo\n",
    "            g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "            # Actualizar bounding box del grupo\n",
    "            g['theta_min'] = g['points']['theta'].min()\n",
    "            g['theta_max'] = g['points']['theta'].max()\n",
    "            g['r_min'] = g['points']['r'].min()\n",
    "            g['r_max'] = g['points']['r'].max()\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def merge_all_groups(groups, theta_merge_threshold, r_merge_threshold):\n",
    "    \"\"\"\n",
    "    Fusiona grupos cuyos rangos en theta y r estén muy próximos.\n",
    "    Se hace en un bucle while hasta que no haya más fusiones posibles.\n",
    "    \"\"\"\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                g1 = groups[i]\n",
    "                g2 = groups[j]\n",
    "                # Diferencias de límites\n",
    "                # g1\n",
    "                g1_th_min = g1['theta_min']\n",
    "                g1_th_max = g1['theta_max']\n",
    "                g1_r_min  = g1['r_min']\n",
    "                g1_r_max  = g1['r_max']\n",
    "                # g2\n",
    "                g2_th_min = g2['theta_min']\n",
    "                g2_th_max = g2['theta_max']\n",
    "                g2_r_min  = g2['r_min']\n",
    "                g2_r_max  = g2['r_max']\n",
    "                \n",
    "                # Verificar si están dentro de thresholds fijos en los límites\n",
    "                # Queremos que ambos grupos se \"toquen\" en theta y en r\n",
    "                cond_theta = (abs(g1_th_min - g2_th_max) < theta_merge_threshold and\n",
    "                              abs(g1_th_max - g2_th_min) < theta_merge_threshold)\n",
    "                cond_r = (abs(g1_r_min - g2_r_max) < r_merge_threshold and\n",
    "                          abs(g1_r_max - g2_r_min) < r_merge_threshold)\n",
    "                \n",
    "                if cond_theta and cond_r:\n",
    "                    # Fusionar g2 en g1\n",
    "                    g1['points'] = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    g1['theta_min'] = g1['points']['theta'].min()\n",
    "                    g1['theta_max'] = g1['points']['theta'].max()\n",
    "                    g1['r_min'] = g1['points']['r'].min()\n",
    "                    g1['r_max'] = g1['points']['r'].max()\n",
    "                    \n",
    "                    # Eliminar g2\n",
    "                    groups.pop(j)\n",
    "                    merged_flag = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "\n",
    "def process_spiral_arms_by_proximity(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    # Ventanas para añadir puntos a un grupo\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    # Umbrales para fusionar grupos\n",
    "    theta_merge_threshold=7.0,\n",
    "    r_merge_threshold=7.0,\n",
    "    # Cada cuántas iteraciones guardamos un \"estado\"\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa datos y agrupa puntos solo por cercanía en (theta, r):\n",
    "      1) Se filtran por un percentil (quartile_threshold).\n",
    "      2) Se recorren en orden creciente de theta.\n",
    "      3) Se intenta agregar cada punto a un grupo si cae dentro de la \n",
    "         bounding box del grupo extendida por (theta_fit_window, r_fit_window).\n",
    "      4) Si no cabe, se crea un grupo nuevo.\n",
    "      5) Al final se fusionan grupos cercanos si sus bounding boxes \n",
    "         difieren menos de (theta_merge_threshold, r_merge_threshold).\n",
    "    \n",
    "    Retorna:\n",
    "      - states: lista de estados intermedios para graficar\n",
    "      - groups: lista final de grupos\n",
    "      - df_filtered: DataFrame filtrado\n",
    "    \"\"\"\n",
    "    # 1. Cargar datos\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    \n",
    "    # 2. Convertir a coordenadas polares\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    \n",
    "    # Filtrar theta entre 50 y 250\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    # 3. Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    \n",
    "    # Ordenar por theta ascendente\n",
    "    df_filtered.sort_values(by='theta', inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 4. Agrupación por cercanía\n",
    "    groups = []  # cada elemento tendrá: { 'points': df, 'theta_min':..., 'theta_max':..., 'r_min':..., 'r_max':... }\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    df_local = df_filtered.copy()\n",
    "    \n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        \n",
    "        # Intentar añadir el punto a un grupo existente\n",
    "        assigned = add_point_to_groups(point, groups, theta_fit_window, r_fit_window)\n",
    "        \n",
    "        if not assigned:\n",
    "            # Crear un grupo nuevo\n",
    "            new_grp = {\n",
    "                'points': point.to_frame().T.copy(),\n",
    "                'theta_min': point['theta'],\n",
    "                'theta_max': point['theta'],\n",
    "                'r_min': point['r'],\n",
    "                'r_max': point['r']\n",
    "            }\n",
    "            groups.append(new_grp)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            # Guardar un estado intermedio\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 5. Fusión final de grupos cercanos\n",
    "    merge_all_groups(groups, theta_merge_threshold, r_merge_threshold)\n",
    "    \n",
    "    # Guardar estado final\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(groups),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, groups, df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d437a7e4-b237-4b0b-876a-1e5491030523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "states, final_groups, df_filtered = process_spiral_arms_by_proximity(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.155,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=2.50,\n",
    "    theta_merge_threshold=7.0,\n",
    "    r_merge_threshold=7.0,\n",
    "    snapshot_interval=10\n",
    ")\n",
    "\n",
    "print(\"Total de estados guardados:\", len(states))\n",
    "print(\"Total de grupos finales:\", len(final_groups))\n",
    "\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state_cartesian(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ee94df4-734a-4efe-be05-5ddc4eb8e52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4841c1-344a-49e1-b5cc-560d5ba14987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "def plot_groups(groups, df_all, title=\"Grupos en (theta, r)\"):\n",
    "    \"\"\"\n",
    "    Grafica los grupos resultantes en el plano (theta, r),\n",
    "    junto con los puntos originales en gris.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Graficar todos los puntos en gris\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    \n",
    "    # Colores de ejemplo\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    \n",
    "    for idx, grp in enumerate(groups):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "    \n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def build_graph(points, max_theta_diff, max_r_diff, r_gap_exclusion=2.0):\n",
    "    \"\"\"\n",
    "    Construye un grafo (lista de adyacencia) donde cada nodo es un punto,\n",
    "    y hay arista entre i y j si:\n",
    "      - |theta_i - theta_j| <= max_theta_diff\n",
    "      - |r_i - r_j| <= max_r_diff\n",
    "      - |r_i - r_j| <= r_gap_exclusion (para evitar saltos > 2 en r)\n",
    "    Retorna:\n",
    "      - graph: lista de listas de índices adyacentes\n",
    "      - n: número total de puntos\n",
    "    \"\"\"\n",
    "    n = len(points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    \n",
    "    # Convertimos a arrays para vectorizar\n",
    "    theta_arr = points['theta'].values\n",
    "    r_arr = points['r'].values\n",
    "    \n",
    "    # Para optimizar, se puede hacer un barrido lineal o 2D-tree, \n",
    "    # pero aquí haremos un doble bucle simple (O(n^2)) para mayor claridad.\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dtheta = abs(theta_arr[i] - theta_arr[j])\n",
    "            dr = abs(r_arr[i] - r_arr[j])\n",
    "            if dtheta <= max_theta_diff and dr <= max_r_diff and dr <= r_gap_exclusion:\n",
    "                # Son vecinos\n",
    "                graph[i].append(j)\n",
    "                graph[j].append(i)\n",
    "    \n",
    "    return graph, n\n",
    "\n",
    "def bfs_connected_components(graph, n, points_df):\n",
    "    \"\"\"\n",
    "    Realiza BFS para encontrar componentes conexas en el grafo.\n",
    "    graph: lista de adyacencia\n",
    "    n: número de nodos\n",
    "    points_df: DataFrame original de puntos\n",
    "    Retorna una lista de grupos, cada grupo es un dict con:\n",
    "      { 'points': DataFrame con filas correspondientes a la componente }\n",
    "    \"\"\"\n",
    "    visited = [False]*n\n",
    "    groups = []\n",
    "    \n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            # Iniciar BFS desde start\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            \n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            \n",
    "            # Crear un grupo con los puntos de comp_indices\n",
    "            grp_points = points_df.iloc[comp_indices].copy()\n",
    "            grp_points.reset_index(drop=True, inplace=True)\n",
    "            groups.append({'points': grp_points})\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def process_spiral_arms_by_advanced_proximity(\n",
    "    id_halo,\n",
    "    # Filtrado\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    # Parámetros para cercanía\n",
    "    max_theta_diff=5.0,   # en grados\n",
    "    max_r_diff=2.0,       # en r\n",
    "    r_gap_exclusion=2.0,  # si hay un salto > 2 en r, no se conectan\n",
    "):\n",
    "    \"\"\"\n",
    "    Carga y filtra puntos, luego construye un grafo de vecindad\n",
    "    en (theta, r) donde dos puntos se conectan si:\n",
    "      - |theta_i - theta_j| <= max_theta_diff\n",
    "      - |r_i - r_j| <= max_r_diff\n",
    "      - |r_i - r_j| <= r_gap_exclusion  (evita saltos > 2 en r)\n",
    "    Finalmente extrae los grupos como componentes conexas (BFS).\n",
    "    Retorna:\n",
    "      - groups: lista de dicts con 'points' (DataFrame de cada grupo)\n",
    "      - df_filtered: DataFrame de los puntos usados\n",
    "    \"\"\"\n",
    "    # 1. Cargar datos\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    \n",
    "    # 2. Convertir a polares\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    \n",
    "    # 3. Filtrar theta\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # 4. Filtrar por percentil\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 5. Construir grafo\n",
    "    graph, n = build_graph(df_filtered, max_theta_diff, max_r_diff, r_gap_exclusion=r_gap_exclusion)\n",
    "    \n",
    "    # 6. Extraer componentes conexas\n",
    "    groups = bfs_connected_components(graph, n, df_filtered)\n",
    "    \n",
    "    return groups, df_filtered\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Llamada de ejemplo\n",
    "    groups, df_filtered = process_spiral_arms_by_advanced_proximity(\n",
    "        id_halo=\"17\",\n",
    "        quartile_threshold=0.155,\n",
    "        theta_min=50.0,\n",
    "        theta_max=250.0,\n",
    "        max_theta_diff=5.0,\n",
    "        max_r_diff=2.0,\n",
    "        r_gap_exclusion=2.0\n",
    "    )\n",
    "    \n",
    "    print(f\"Se formaron {len(groups)} grupos.\")\n",
    "    # Graficar\n",
    "    plot_groups(groups, df_filtered, title=\"Grupos por BFS y salto en r<=2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61a67faa-1749-4305-976e-db1495dcd9d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13ed981-0ddc-4f63-973b-548f7a990224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull, distance_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree, connected_components\n",
    "\n",
    "def process_by_MST_convex(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    edge_threshold=5.0,   # Umbral máximo para conservar una arista en el MST\n",
    "    min_cluster_size=5    # Tamaño mínimo de un grupo\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa los datos y agrupa los puntos en (theta, r) utilizando el MST:\n",
    "      1. Carga y filtra los datos (por rango de theta y percentil en 'rho_resta_final_exp').\n",
    "      2. Calcula la matriz de distancias entre puntos en (theta, r) (sin escalado adicional).\n",
    "      3. Obtiene el MST y \"poda\" las aristas con longitud > edge_threshold.\n",
    "      4. Obtiene los componentes conexos (grupos) del grafo resultante.\n",
    "      5. Se conservan únicamente los grupos con al menos min_cluster_size puntos.\n",
    "    \n",
    "    Retorna:\n",
    "      - clusters: lista de diccionarios, cada uno con 'points' (DataFrame) para ese grupo.\n",
    "      - df_filtered: DataFrame filtrado.\n",
    "      - labels: array de etiquetas para cada punto (útil para ver a qué grupo pertenece cada uno).\n",
    "    \"\"\"\n",
    "    # 1. Cargar datos\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    \n",
    "    # 2. Convertir a coordenadas polares\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    \n",
    "    # Filtrar por theta\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Construir la matriz de distancias en el espacio (theta, r)\n",
    "    # Si las unidades de theta y r son muy diferentes, se podrían escalar.\n",
    "    X = df_filtered[['theta', 'r']].values\n",
    "    D = distance_matrix(X, X)\n",
    "    \n",
    "    # 4. Calcular el MST\n",
    "    mst = minimum_spanning_tree(D)\n",
    "    mst = mst.toarray()\n",
    "    \n",
    "    # 5. Podar (prune) el MST: ponemos a cero las aristas con longitud mayor a edge_threshold\n",
    "    pruned_mst = np.where(mst > edge_threshold, 0, mst)\n",
    "    # Para construir el grafo, hacemos simétrico (ya que MST es dirigido)\n",
    "    graph = pruned_mst + pruned_mst.T\n",
    "    \n",
    "    # 6. Extraer componentes conexas\n",
    "    n_components, labels = connected_components(csgraph=graph, directed=False, return_labels=True)\n",
    "    \n",
    "    clusters = []\n",
    "    for i in range(n_components):\n",
    "        indices = np.where(labels == i)[0]\n",
    "        if len(indices) >= min_cluster_size:\n",
    "            cluster_df = df_filtered.iloc[indices].copy()\n",
    "            cluster_df.reset_index(drop=True, inplace=True)\n",
    "            clusters.append({'points': cluster_df})\n",
    "    \n",
    "    return clusters, df_filtered, labels\n",
    "\n",
    "def plot_MST_clusters(clusters, df_filtered, labels, title=\"Clustering usando MST y Convex Hull\"):\n",
    "    \"\"\"\n",
    "    Grafica los grupos obtenidos y, para cada grupo, dibuja su envolvente convexa.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Graficar todos los puntos en gris\n",
    "    plt.scatter(df_filtered['theta'], df_filtered['r'], s=5, color='lightgrey', alpha=0.5, label='Datos')\n",
    "    \n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    \n",
    "    for idx, grp in enumerate(clusters):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if len(pts) >= 3:\n",
    "            points = pts[['theta', 'r']].values\n",
    "            hull = ConvexHull(points)\n",
    "            for simplex in hull.simplices:\n",
    "                plt.plot(points[simplex, 0], points[simplex, 1], color=color, lw=2)\n",
    "    \n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Ejemplo de ejecución:\n",
    "if __name__ == \"__main__\":\n",
    "    # Ajusta los parámetros según necesites\n",
    "    clusters, df_filtered, labels = process_by_MST_convex(\n",
    "        id_halo=\"17\",\n",
    "        quartile_threshold=0.155,\n",
    "        theta_min=50.0,\n",
    "        theta_max=70.0,\n",
    "        edge_threshold=2.0,\n",
    "        min_cluster_size=5\n",
    "    )\n",
    "    \n",
    "    print(f\"Se han formado {len(clusters)} grupos (componentes conexas).\")\n",
    "    plot_MST_clusters(clusters, df_filtered, labels, title=\"Clustering MST + Convex Hull\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf026c05-c440-48bf-ac83-6c2372441732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import copy\n",
    "\n",
    "def process_spiral_arms_polar_intermediate(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    eps_polar=0.7,\n",
    "    min_samples_polar=18,\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,  # Para detectar un gap y dividir el grupo en 2, si es muy grande\n",
    "    do_subdivide=True    # Activar o desactivar la subdivisión\n",
    "):\n",
    "    \"\"\"\n",
    "    Versión \"intermedia\" que:\n",
    "      1. Aplica DBSCAN en (r, theta).\n",
    "      2. Fusiona clusters cercanos.\n",
    "      3. Ajuste lineal iterativo (bondadoso), controlando la variación de la pendiente (20%).\n",
    "      4. Reasigna descartados.\n",
    "      5. (Opcional) Subdivide un grupo en 2 si se detecta un gap grande en theta.\n",
    "      6. Filtra por PA al final.\n",
    "    \n",
    "    Se ha añadido una condición extra en la asignación iterativa para que el nuevo punto\n",
    "    deba estar también dentro del rango horizontal (θ) actual del grupo, extendido en ±theta_margin.\n",
    "    \n",
    "    Retorna:\n",
    "      - valid_groups: Lista de grupos (diccionarios) con sus parámetros ajustados.\n",
    "      - df: DataFrame original con las columnas añadidas (r, θ, cluster).\n",
    "    \"\"\"\n",
    "    # Parámetro extra: margen horizontal para la asignación (por ejemplo, la mitad de theta_fit_window)\n",
    "    theta_margin = 15.0  # Puedes ajustar este valor (por ejemplo, theta_fit_window/2)\n",
    "    \n",
    "    # 1. Cargar datos\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "\n",
    "    # 2. Convertir a coordenadas polares\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "\n",
    "    # 3. Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 4. DBSCAN en el plano (r, theta)\n",
    "    X_polar = df_filtered[['r', 'theta']].values\n",
    "    dbscan = DBSCAN(eps=eps_polar, min_samples=min_samples_polar)\n",
    "    df_filtered['cluster'] = dbscan.fit_predict(X_polar)\n",
    "\n",
    "    # 5. Fusión de clusters cercanos\n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "\n",
    "    # Crear diccionario de clusters\n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "\n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "\n",
    "    # 6. Inicializar grupos\n",
    "    model = LinearRegression()\n",
    "\n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "\n",
    "    def calculate_pa(slope, intercept):\n",
    "        if intercept != 0:\n",
    "            return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "        return np.nan\n",
    "\n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "\n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']].values\n",
    "            y_ = pts['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "\n",
    "        groups.append({\n",
    "            'cluster_id': cid,\n",
    "            'points': pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "\n",
    "    # 7. Asignación iterativa de puntos (incluye control de pendiente y vecindad horizontal)\n",
    "    df_local = df_filtered.copy()\n",
    "    discarded_points = []\n",
    "\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "\n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "\n",
    "            # Control extra: el nuevo punto debe estar dentro del rango horizontal del grupo extendido en ±theta_margin\n",
    "            group_theta_min = g['points']['theta'].min()\n",
    "            group_theta_max = g['points']['theta'].max()\n",
    "            if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                continue\n",
    "\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                # Verificar que la distancia a TODOS los puntos del grupo sea menor a point_distance_tolerance\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    # Actualizar ajuste lineal con el nuevo punto\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= slope_variation_threshold:\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "\n",
    "    # 8. Reasignar descartados\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                # Exigir que el punto esté dentro del rango horizontal del grupo\n",
    "                group_theta_min = g['points']['theta'].min()\n",
    "                group_theta_max = g['points']['theta'].max()\n",
    "                if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance and g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= slope_variation_threshold:\n",
    "                        g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(new_slope, g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "            if not assigned:\n",
    "                # Crear grupo nuevo para este punto\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'cluster_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "\n",
    "    # 9. Subdivisión (opcional) en caso de gaps grandes en theta\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "\n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "\n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "\n",
    "    # 10. Filtrar por PA\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "\n",
    "    return valid_groups, df\n",
    "\n",
    "###############################################################################\n",
    "# EJEMPLO DE EJECUCIÓN Y GRAFICACIÓN\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.155\n",
    "    radius_tolerance = 0.45\n",
    "    point_distance_tolerance = 0.59\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    eps_polar = 0.7\n",
    "    min_samples_polar = 18\n",
    "    slope_variation_threshold = 0.20  # 20%\n",
    "    max_theta_gap = 15.0\n",
    "    do_subdivide = True\n",
    "\n",
    "    valid_groups, df = process_spiral_arms_polar_intermediate(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        point_distance_tolerance=point_distance_tolerance,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        eps_polar=eps_polar,\n",
    "        min_samples_polar=min_samples_polar,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        do_subdivide=do_subdivide\n",
    "    )\n",
    "\n",
    "    print(\"Total de grupos finales:\", len(valid_groups))\n",
    "\n",
    "    # Gráfica en (theta vs r)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['theta'], df['r'], s=1, color='silver', alpha=0.5, label='Datos Originales')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_vals = pts['theta']\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(theta_vals, r_vals, s=10, alpha=0.8, color=color,\n",
    "                    label=f\"Grupo {idx+1}, PA={pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(theta_vals.min(), theta_vals.max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Ajustes intermedios en (r, θ) - Halo {id_halo}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfica polar\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(10, 8))\n",
    "    theta_all_rad = np.radians(df['theta'])\n",
    "    ax.scatter(theta_all_rad, df['r'], s=1, color='gray', alpha=0.5, label='Datos Originales')\n",
    "\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_rad = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_rad, r_vals, s=10, alpha=0.8, color=color,\n",
    "                   label=f\"Grupo {idx+1} PA: {pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "\n",
    "    ax.set_title(f\"Ajustes intermedios en proyección polar - Halo {id_halo}\", fontsize=14, pad=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbaf1b39-6e0f-4dd6-af3e-4df7b25732d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "215f51e4-34f8-40cc-9fb6-e852c12ff5f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9924f930-bb2c-4a81-95f7-51762e7e3c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.path import Path\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica un estado en el plano (theta, r).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'darkblue']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "        # Dibujar el convex hull\n",
    "        if len(pts) >= 3:\n",
    "            pts_arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(pts_arr)\n",
    "                hull_pts = pts_arr[hull.vertices]\n",
    "                hull_pts = np.concatenate([hull_pts, hull_pts[:1]], axis=0)  # cerrar polígono\n",
    "                plt.plot(hull_pts[:,0], hull_pts[:,1], '-', color=color, lw=2)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def merge_all_groups(groups, theta_merge_threshold, r_merge_threshold):\n",
    "    \"\"\"\n",
    "    Fusiona grupos si sus límites están muy próximos. (Opcional)\n",
    "    \"\"\"\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                g1 = groups[i]\n",
    "                g2 = groups[j]\n",
    "                # Límites de g1\n",
    "                g1_th_min = g1['theta_min']\n",
    "                g1_th_max = g1['theta_max']\n",
    "                g1_r_min  = g1['r_min']\n",
    "                g1_r_max  = g1['r_max']\n",
    "                # Límites de g2\n",
    "                g2_th_min = g2['theta_min']\n",
    "                g2_th_max = g2['theta_max']\n",
    "                g2_r_min  = g2['r_min']\n",
    "                g2_r_max  = g2['r_max']\n",
    "                \n",
    "                cond_theta = (abs(g1_th_min - g2_th_max) < theta_merge_threshold and\n",
    "                              abs(g1_th_max - g2_th_min) < theta_merge_threshold)\n",
    "                cond_r = (abs(g1_r_min - g2_r_max) < r_merge_threshold and\n",
    "                          abs(g1_r_max - g2_r_min) < r_merge_threshold)\n",
    "                \n",
    "                if cond_theta and cond_r:\n",
    "                    g1['points'] = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    # Recalcular límites con bounding box (o hull)\n",
    "                    update_group_limits(g1)\n",
    "                    groups.pop(j)\n",
    "                    merged_flag = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "\n",
    "def update_group_limits(group):\n",
    "    \"\"\"\n",
    "    Recalcula los límites (theta_min, etc.) usando el convex hull si hay 3 o más puntos.\n",
    "    De lo contrario, usa bounding box.\n",
    "    \"\"\"\n",
    "    pts = group['points']\n",
    "    if len(pts) >= 3:\n",
    "        arr = pts[['theta','r']].values\n",
    "        try:\n",
    "            hull = ConvexHull(arr)\n",
    "            hull_pts = arr[hull.vertices]\n",
    "            group['theta_min'] = hull_pts[:,0].min()\n",
    "            group['theta_max'] = hull_pts[:,0].max()\n",
    "            group['r_min'] = hull_pts[:,1].min()\n",
    "            group['r_max'] = hull_pts[:,1].max()\n",
    "            return\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Si falla o hay menos de 3 puntos\n",
    "    group['theta_min'] = pts['theta'].min()\n",
    "    group['theta_max'] = pts['theta'].max()\n",
    "    group['r_min'] = pts['r'].min()\n",
    "    group['r_max'] = pts['r'].max()\n",
    "\n",
    "def point_in_hull(point, hull_points):\n",
    "    \"\"\"\n",
    "    Verifica si un punto (theta_pt, r_pt) está estrictamente dentro de un polígono (hull_points).\n",
    "    hull_points es un array Nx2 (theta, r).\n",
    "    \"\"\"\n",
    "    poly = Path(hull_points, closed=True)\n",
    "    return poly.contains_point(point, radius=-1e-9)  # radio negativo => más estricto\n",
    "\n",
    "def add_point_strict_convex(point, groups, theta_strict_window, r_strict_window, r_gap=2.0):\n",
    "    \"\"\"\n",
    "    Agrega 'point' a un grupo existente de forma estricta:\n",
    "      - Si el grupo tiene >=3 puntos, se calcula su convex hull y se verifica si el punto está DENTRO.\n",
    "        (sin margen, más estricto)\n",
    "      - No se añade si está fuera del hull.\n",
    "      - Si el grupo tiene <3 puntos, se usa bounding box reducida (la mitad de theta_strict_window y r_strict_window).\n",
    "      - Se verifica que no haya un salto > r_gap en la coordenada r.\n",
    "    \"\"\"\n",
    "    theta_pt = point['theta']\n",
    "    r_pt = point['r']\n",
    "    \n",
    "    for g in groups:\n",
    "        # Comprobar gap en r\n",
    "        if r_pt < g['r_min'] - r_gap or r_pt > g['r_max'] + r_gap:\n",
    "            continue\n",
    "        \n",
    "        pts = g['points']\n",
    "        if len(pts) >= 3:\n",
    "            arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(arr)\n",
    "                hull_pts = arr[hull.vertices]\n",
    "                # Verificar si el punto está ESTRICTAMENTE dentro\n",
    "                inside = point_in_hull([theta_pt, r_pt], hull_pts)\n",
    "                if inside:\n",
    "                    # Agregar y recalcular límites\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    update_group_limits(g)\n",
    "                    return True\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Si no está dentro, no se agrega.\n",
    "        else:\n",
    "            # Para grupos con <3 puntos, bounding box muy estricta (la mitad)\n",
    "            half_theta = theta_strict_window / 2.0\n",
    "            half_r = r_strict_window / 2.0\n",
    "            t_min = pts['theta'].min()\n",
    "            t_max = pts['theta'].max()\n",
    "            rr_min = pts['r'].min()\n",
    "            rr_max = pts['r'].max()\n",
    "            \n",
    "            if (theta_pt >= t_min - half_theta/2 and theta_pt <= t_max + half_theta/2 and\n",
    "                r_pt >= rr_min - half_r/2 and r_pt <= rr_max + half_r/2):\n",
    "                g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                update_group_limits(g)\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def process_spiral_arms_by_proximity(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    # Ventanas \"estrictas\" para agregar puntos\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    # Umbrales para fusionar grupos\n",
    "    theta_merge_threshold=7.0,\n",
    "    r_merge_threshold=7.0,\n",
    "    # Salto máximo en r para no fusionar\n",
    "    r_gap=2.0,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa datos y agrupa puntos de forma más estricta con convex hull:\n",
    "      1) Se filtran por percentil y por theta (50-250).\n",
    "      2) Se recorre de izquierda a derecha (orden ascending de theta).\n",
    "      3) Para cada punto, se intenta agregar a un grupo:\n",
    "         - Si el grupo tiene >=3 puntos, se exige que el punto esté ESTRICTAMENTE dentro del hull (sin margen).\n",
    "         - Si el grupo tiene <3 puntos, se usa bounding box reducida (la mitad de la ventana).\n",
    "         - También se verifica que no haya un salto en r mayor a r_gap.\n",
    "      4) Si no se agrega, se crea un grupo nuevo.\n",
    "      5) Al final se fusionan grupos con bounding boxes cercanas (opcional).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='theta', inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    groups = []\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    df_local = df_filtered.copy()\n",
    "    \n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        added = add_point_strict_convex(point, groups, theta_fit_window, r_fit_window, r_gap=r_gap)\n",
    "        if not added:\n",
    "            new_group = {\n",
    "                'points': point.to_frame().T.copy()\n",
    "            }\n",
    "            # Inicializar límites\n",
    "            new_group['theta_min'] = point['theta']\n",
    "            new_group['theta_max'] = point['theta']\n",
    "            new_group['r_min'] = point['r']\n",
    "            new_group['r_max'] = point['r']\n",
    "            groups.append(new_group)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # Fusión final (opcional). Coméntalo si no deseas fusión.\n",
    "    merge_all_groups(groups, theta_merge_threshold, r_merge_threshold)\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(groups),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, groups, df_filtered\n",
    "\n",
    "# Ejemplo de ejecución (coméntalo si pegas en un notebook):\n",
    "if __name__ == \"__main__\":\n",
    "    states, final_groups, df_filtered = process_spiral_arms_by_proximity(\n",
    "        id_halo=\"17\",\n",
    "        quartile_threshold=0.155,\n",
    "        theta_fit_window=30.0,\n",
    "        r_fit_window=3.0,\n",
    "        theta_merge_threshold=10.0,\n",
    "        r_merge_threshold=2.0,\n",
    "        r_gap=2.0,\n",
    "        snapshot_interval=10\n",
    "    )\n",
    "    print(f\"Se formaron {len(final_groups)} grupos.\")\n",
    "    plot_intermediate_state_cartesian(states[-1], df_filtered)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5773ede3-49a5-4ffe-9a53-694654e26702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a4977e-ea51-477a-b9b8-809293e0ad35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e508c26-19bf-44aa-b48d-9155c6448eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# ==============================\n",
    "# Unión–Encuentra (Union–Find)\n",
    "# ==============================\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, a):\n",
    "        while self.parent[a] != a:\n",
    "            self.parent[a] = self.parent[self.parent[a]]\n",
    "            a = self.parent[a]\n",
    "        return a\n",
    "    def union(self, a, b):\n",
    "        ra = self.find(a)\n",
    "        rb = self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "# ==============================\n",
    "# Función de graficación\n",
    "# ==============================\n",
    "def plot_groups(groups, df_all, title=\"Grupos finales\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, g in enumerate(groups):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ==============================\n",
    "# Función de fusión final de grupos\n",
    "# ==============================\n",
    "def merge_groups(groups, theta_merge_threshold, r_merge_threshold):\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                g1 = groups[i]\n",
    "                g2 = groups[j]\n",
    "                tmin1, tmax1, rmin1, rmax1 = g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max']\n",
    "                tmin2, tmax2, rmin2, rmax2 = g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max']\n",
    "                cond_theta = (abs(tmin1 - tmax2) < theta_merge_threshold and abs(tmax1 - tmin2) < theta_merge_threshold)\n",
    "                cond_r = (abs(rmin1 - rmax2) < r_merge_threshold and abs(rmax1 - rmin2) < r_merge_threshold)\n",
    "                if cond_theta and cond_r:\n",
    "                    g1['points'] = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    g1['theta_min'] = g1['points']['theta'].min()\n",
    "                    g1['theta_max'] = g1['points']['theta'].max()\n",
    "                    g1['r_min'] = g1['points']['r'].min()\n",
    "                    g1['r_max'] = g1['points']['r'].max()\n",
    "                    groups.pop(j)\n",
    "                    merged_flag = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "\n",
    "# ==============================\n",
    "# Función principal optimizada\n",
    "# ==============================\n",
    "def process_spiral_arms_by_proximity_optimized(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    # Parámetros para la vecindad: dos puntos se consideran vecinos si:\n",
    "    # |theta_i - theta_j| <= theta_diff y |r_i - r_j| <= r_gap.\n",
    "    theta_diff=10.0,\n",
    "    r_gap=2.0,\n",
    "    # Umbrales para fusión final de grupos (bounding boxes)\n",
    "    theta_merge_threshold=7.0,\n",
    "    r_merge_threshold=7.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Agrupa puntos en (theta, r) usando KDTree y Union-Find.\n",
    "    1) Se filtran los datos (theta en [theta_min, theta_max] y percentil en 'rho_resta_final_exp').\n",
    "    2) Se ordenan por theta (de izquierda a derecha).\n",
    "    3) Se construye un KDTree en un espacio escalado:\n",
    "         X_scaled = [theta/theta_diff, r/r_gap]\n",
    "       de modo que dos puntos se consideren vecinos si su Chebyshev distance <= 1.\n",
    "    4) Se utiliza Union-Find para agrupar los puntos conectados.\n",
    "    5) Para cada grupo se calcula la bounding box.\n",
    "    6) (Opcional) Se fusionan grupos cuyas bounding boxes estén muy próximas.\n",
    "    Retorna:\n",
    "      - groups: lista de grupos (cada grupo es un diccionario con 'points', 'theta_min', 'theta_max', 'r_min', 'r_max').\n",
    "      - df_filtered: DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    # 1. Cargar y filtrar datos\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    thresh = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > thresh].copy()\n",
    "    df_filtered.sort_values(by='theta', inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    n = len(df_filtered)\n",
    "    if n == 0:\n",
    "        return [], df_filtered\n",
    "    \n",
    "    # 2. Construir KDTree en espacio escalado\n",
    "    # Escalamos: en la primera dimensión dividimos por theta_diff y en la segunda por r_gap.\n",
    "    X = df_filtered[['theta', 'r']].values\n",
    "    X_scaled = np.column_stack([X[:,0] / theta_diff, X[:,1] / r_gap])\n",
    "    \n",
    "    tree = KDTree(X_scaled, metric='chebyshev')\n",
    "    # Query: vecinos con Chebyshev distance <= 1 => |theta_i - theta_j| <= theta_diff and |r_i - r_j| <= r_gap\n",
    "    indices_list = tree.query_radius(X_scaled, r=1.0)\n",
    "    \n",
    "    # 3. Agrupar con Union-Find\n",
    "    uf = UnionFind(n)\n",
    "    for i, neighbors in enumerate(indices_list):\n",
    "        for j in neighbors:\n",
    "            if i != j:\n",
    "                uf.union(i, j)\n",
    "    \n",
    "    # 4. Extraer componentes conexas y calcular bounding box\n",
    "    groups_dict = {}\n",
    "    for i in range(n):\n",
    "        root = uf.find(i)\n",
    "        if root not in groups_dict:\n",
    "            groups_dict[root] = []\n",
    "        groups_dict[root].append(i)\n",
    "    \n",
    "    groups = []\n",
    "    for comp in groups_dict.values():\n",
    "        comp_df = df_filtered.iloc[comp].copy()\n",
    "        comp_df.reset_index(drop=True, inplace=True)\n",
    "        group = {\n",
    "            'points': comp_df,\n",
    "            'theta_min': comp_df['theta'].min(),\n",
    "            'theta_max': comp_df['theta'].max(),\n",
    "            'r_min': comp_df['r'].min(),\n",
    "            'r_max': comp_df['r'].max()\n",
    "        }\n",
    "        groups.append(group)\n",
    "    \n",
    "    # 5. Fusión final de grupos (opcional)\n",
    "    merge_groups(groups, theta_merge_threshold, r_merge_threshold)\n",
    "    \n",
    "    return groups, df_filtered\n",
    "\n",
    "# ==============================\n",
    "# Ejecución y visualización\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    # Ajusta los parámetros según tus necesidades.\n",
    "    groups, df_filtered = process_spiral_arms_by_proximity_optimized(\n",
    "        id_halo=\"17\",\n",
    "        quartile_threshold=0.155,\n",
    "        theta_min=50.0,\n",
    "        theta_max=250.0,\n",
    "        theta_diff=10.0,      # Vecindad en theta: 10 grados\n",
    "        r_gap=2.50,            # Vecindad en r: 2 unidades (separación vertical)\n",
    "        theta_merge_threshold=5.0,\n",
    "        r_merge_threshold=4.0\n",
    "    )\n",
    "    print(f\"Se formaron {len(groups)} grupos.\")\n",
    "    plot_groups(groups, df_filtered, title=\"Grupos finales (Optimizado)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21f6f1d9-2774-4962-846d-ad92ebbbcc27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "def plot_groups(groups, df_all, title=\"Grupos finales\"):\n",
    "    \"\"\"\n",
    "    Grafica los grupos resultantes en (theta, r), con df_all en gris de fondo.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    \n",
    "    for idx, g in enumerate(groups):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "    \n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff, r_gap=None):\n",
    "    \"\"\"\n",
    "    Construye un grafo (lista de adyacencia) con vecindad rectangular:\n",
    "      - |theta_i - theta_j| <= theta_diff\n",
    "      - |r_i - r_j| <= r_diff\n",
    "      - (opcional) |r_i - r_j| <= r_gap para excluir saltos grandes en r\n",
    "    \n",
    "    df_points debe estar ordenado por theta ascendente (para optimizar).\n",
    "    \"\"\"\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    \n",
    "    # Extraemos arrays\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    \n",
    "    # Para optimizar, buscaremos vecinos solo en un rango de theta\n",
    "    # (por ejemplo, usando índices incrementales)\n",
    "    j_start = 0\n",
    "    for i in range(n):\n",
    "        # Avanzamos j_start si theta_j < theta_i - theta_diff\n",
    "        while j_start < n and theta_arr[j_start] < theta_arr[i] - theta_diff:\n",
    "            j_start += 1\n",
    "        \n",
    "        # Explorar desde j_start hacia adelante mientras theta_j <= theta_i + theta_diff\n",
    "        j = j_start\n",
    "        while j < n and theta_arr[j] <= theta_arr[i] + theta_diff:\n",
    "            if i != j:\n",
    "                dtheta = abs(theta_arr[i] - theta_arr[j])\n",
    "                dr = abs(r_arr[i] - r_arr[j])\n",
    "                if dtheta <= theta_diff and dr <= r_diff:\n",
    "                    if r_gap is None or dr <= r_gap:\n",
    "                        graph[i].append(j)\n",
    "                        graph[j].append(i)\n",
    "            j += 1\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas mediante BFS en el grafo dado.\n",
    "    Retorna lista de grupos, donde cada grupo = {'points': DataFrame, ... bounding box ...}.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    groups = []\n",
    "    \n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            \n",
    "            comp_df = df_points.iloc[comp_indices].copy()\n",
    "            comp_df.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            g = {\n",
    "                'points': comp_df,\n",
    "                'theta_min': comp_df['theta'].min(),\n",
    "                'theta_max': comp_df['theta'].max(),\n",
    "                'r_min': comp_df['r'].min(),\n",
    "                'r_max': comp_df['r'].max()\n",
    "            }\n",
    "            groups.append(g)\n",
    "    return groups\n",
    "\n",
    "def merge_groups_bounding_box(groups, theta_merge_threshold, r_merge_threshold):\n",
    "    \"\"\"\n",
    "    Fusiona grupos cuyas bounding boxes se superponen dentro de thresholds fijos en theta y r.\n",
    "    \"\"\"\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                g1 = groups[i]\n",
    "                g2 = groups[j]\n",
    "                # Verificar superposición\n",
    "                cond_theta = (abs(g1['theta_min'] - g2['theta_max']) < theta_merge_threshold and\n",
    "                              abs(g1['theta_max'] - g2['theta_min']) < theta_merge_threshold)\n",
    "                cond_r = (abs(g1['r_min'] - g2['r_max']) < r_merge_threshold and\n",
    "                          abs(g1['r_max'] - g2['r_min']) < r_merge_threshold)\n",
    "                \n",
    "                if cond_theta and cond_r:\n",
    "                    # Fusionar\n",
    "                    g1['points'] = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    # Recalcular bounding box\n",
    "                    g1['theta_min'] = g1['points']['theta'].min()\n",
    "                    g1['theta_max'] = g1['points']['theta'].max()\n",
    "                    g1['r_min'] = g1['points']['r'].min()\n",
    "                    g1['r_max'] = g1['points']['r'].max()\n",
    "                    groups.pop(j)\n",
    "                    merged_flag = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "\n",
    "def process_spiral_arms_unified(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    # Parámetros para la vecindad rectangular\n",
    "    theta_diff=10.0,\n",
    "    r_diff=2.0,\n",
    "    r_gap=2.0,\n",
    "    # Umbrales para fusionar grupos\n",
    "    theta_merge_threshold=7.0,\n",
    "    r_merge_threshold=7.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Enfoque unificado:\n",
    "      1) Se cargan y filtran los datos (theta en [theta_min, theta_max], percentil).\n",
    "      2) Se ordenan por theta (izquierda a derecha).\n",
    "      3) Se construye un grafo con vecindad rectangular: \n",
    "         |theta_i - theta_j| <= theta_diff, |r_i - r_j| <= r_diff, y (opcional) dr <= r_gap.\n",
    "      4) Se extraen los grupos como componentes conexas (BFS).\n",
    "      5) Fusión final de bounding boxes con thresholds fijos (theta_merge_threshold, r_merge_threshold).\n",
    "    Retorna:\n",
    "      - groups: lista de grupos (cada uno con 'points', 'theta_min', etc.)\n",
    "      - df_filtered: DataFrame filtrado\n",
    "    \"\"\"\n",
    "    # 1) Carga y filtrado\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    # 2) Ordenar por theta\n",
    "    df_filtered.sort_values(by='theta', inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3) Construir grafo\n",
    "    graph = build_graph_rectangular(df_filtered, theta_diff, r_diff, r_gap=r_gap)\n",
    "    \n",
    "    # 4) BFS para componentes conexas\n",
    "    groups = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    # 5) Fusión final de bounding boxes\n",
    "    merge_groups_bounding_box(groups, theta_merge_threshold, r_merge_threshold)\n",
    "    \n",
    "    return groups, df_filtered\n",
    "\n",
    "# Ejemplo de uso en un script:\n",
    "if __name__ == \"__main__\":\n",
    "    groups, df_filtered = process_spiral_arms_unified(\n",
    "        id_halo=\"17\",\n",
    "        quartile_threshold=0.155,\n",
    "        theta_min=50,\n",
    "        theta_max=250,\n",
    "        theta_diff=10.0,       # Cambia para ser más estricto o más laxo en theta\n",
    "        r_diff=2.0,            # Cambia para ser más estricto o más laxo en r\n",
    "        r_gap=2.0,             # Evita que se unan saltos en r > 2\n",
    "        theta_merge_threshold=7.0,\n",
    "        r_merge_threshold=7.0\n",
    "    )\n",
    "    print(f\"Se formaron {len(groups)} grupos.\")\n",
    "    plot_groups(groups, df_filtered, title=\"Resultado final con BFS + bounding box merge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "519f34e9-3d0e-45f2-8620-73b709de90c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29bb9533-6cd1-43fb-8fb2-e030f3e742af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## version combinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60d01a6-82e2-4bbc-b775-bef3453f11df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import copy\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.path import Path\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica un estado en el plano (theta, r).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'darkblue']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "        # Dibujar el convex hull si hay suficientes puntos\n",
    "        if len(pts) >= 3:\n",
    "            pts_arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(pts_arr)\n",
    "                hull_pts = pts_arr[hull.vertices]\n",
    "                hull_pts = np.concatenate([hull_pts, hull_pts[:1]], axis=0)  # Cerrar polígono\n",
    "                plt.plot(hull_pts[:,0], hull_pts[:,1], '-', color=color, lw=2)\n",
    "            except Exception:\n",
    "                pass\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def update_group_limits(group):\n",
    "    \"\"\"\n",
    "    Recalcula los límites (theta_min, theta_max, r_min, r_max) usando el convex hull si hay 3 o más puntos;\n",
    "    de lo contrario, usa la bounding box.\n",
    "    \"\"\"\n",
    "    pts = group['points']\n",
    "    if len(pts) >= 3:\n",
    "        arr = pts[['theta','r']].values\n",
    "        try:\n",
    "            hull = ConvexHull(arr)\n",
    "            hull_pts = arr[hull.vertices]\n",
    "            group['theta_min'] = hull_pts[:,0].min()\n",
    "            group['theta_max'] = hull_pts[:,0].max()\n",
    "            group['r_min'] = hull_pts[:,1].min()\n",
    "            group['r_max'] = hull_pts[:,1].max()\n",
    "            return\n",
    "        except Exception:\n",
    "            pass\n",
    "    group['theta_min'] = pts['theta'].min()\n",
    "    group['theta_max'] = pts['theta'].max()\n",
    "    group['r_min'] = pts['r'].min()\n",
    "    group['r_max'] = pts['r'].max()\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def merge_all_groups(groups, theta_merge_threshold, r_merge_threshold):\n",
    "    \"\"\"\n",
    "    Fusiona grupos si, al extender sus bounding boxes en un 10%, se superponen y la variación\n",
    "    en la pendiente (del ajuste lineal de cada grupo) es ≤ 15%.\n",
    "    La fusión se realiza actualizando el ajuste lineal sobre el grupo combinado.\n",
    "    \"\"\"\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                g1 = groups[i]\n",
    "                g2 = groups[j]\n",
    "                # Límites originales de g1 y g2\n",
    "                th_min1, th_max1, r_min1, r_max1 = g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max']\n",
    "                th_min2, th_max2, r_min2, r_max2 = g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max']\n",
    "                \n",
    "                # Calcular rangos y extender en un 10%\n",
    "                range_th1 = th_max1 - th_min1 if th_max1 - th_min1 != 0 else 1e-3\n",
    "                range_r1 = r_max1 - r_min1 if r_max1 - r_min1 != 0 else 1e-3\n",
    "                ext_th_min1 = th_min1 - 0.1 * range_th1\n",
    "                ext_th_max1 = th_max1 + 0.1 * range_th1\n",
    "                ext_r_min1  = r_min1 - 0.1 * range_r1\n",
    "                ext_r_max1  = r_max1 + 0.1 * range_r1\n",
    "                \n",
    "                range_th2 = th_max2 - th_min2 if th_max2 - th_min2 != 0 else 1e-3\n",
    "                range_r2 = r_max2 - r_min2 if r_max2 - r_min2 != 0 else 1e-3\n",
    "                ext_th_min2 = th_min2 - 0.1 * range_th2\n",
    "                ext_th_max2 = th_max2 + 0.1 * range_th2\n",
    "                ext_r_min2  = r_min2 - 0.1 * range_r2\n",
    "                ext_r_max2  = r_max2 + 0.1 * range_r2\n",
    "                \n",
    "                # Comprobar si las cajas extendidas se superponen:\n",
    "                overlap_theta = not (ext_th_max1 < ext_th_min2 or ext_th_min1 > ext_th_max2)\n",
    "                overlap_r = not (ext_r_max1 < ext_r_min2 or ext_r_min1 > ext_r_max2)\n",
    "                \n",
    "                # Comprobar variación relativa de la pendiente:\n",
    "                slope1 = g1['slope']\n",
    "                slope2 = g2['slope']\n",
    "                if slope1 is None or slope2 is None:\n",
    "                    rel_variation = 0  # Si alguno no tiene ajuste, se permite fusionar\n",
    "                else:\n",
    "                    rel_variation = abs(slope1 - slope2) / (abs(slope1) + 1e-12)\n",
    "                \n",
    "                if overlap_theta and overlap_r and rel_variation <= 0.15:\n",
    "                    # Fusionar g2 en g1:\n",
    "                    merged_points = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    # Recalcular el ajuste lineal para el grupo combinado:\n",
    "                    model = LinearRegression()\n",
    "                    X_new = merged_points[['theta']].values\n",
    "                    y_new = merged_points['r'].values\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    new_intercept = model.intercept_\n",
    "                    g1['points'] = merged_points\n",
    "                    g1['slope'] = new_slope\n",
    "                    g1['intercept'] = new_intercept\n",
    "                    g1['pa'] = calculate_pa(new_slope, new_intercept)\n",
    "                    update_group_limits(g1)\n",
    "                    groups.pop(j)\n",
    "                    merged_flag = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "\n",
    "def add_point_strict_convex(point, groups, theta_strict_window, r_strict_window, r_gap=2.0):\n",
    "    \"\"\"\n",
    "    Agrega 'point' a un grupo existente de forma estricta:\n",
    "      - Si el grupo tiene ≥3 puntos, se calcula su convex hull y se verifica si el punto está DENTRO\n",
    "        (sin margen).\n",
    "      - Para grupos con <3 puntos, se usa una bounding box reducida (la mitad de la ventana).\n",
    "      - Se verifica que el nuevo punto no exceda r_gap en la coordenada r.\n",
    "    \"\"\"\n",
    "    theta_pt = point['theta']\n",
    "    r_pt = point['r']\n",
    "    \n",
    "    for g in groups:\n",
    "        if r_pt < g['r_min'] - r_gap or r_pt > g['r_max'] + r_gap:\n",
    "            continue\n",
    "        \n",
    "        pts = g['points']\n",
    "        if len(pts) >= 3:\n",
    "            arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(arr)\n",
    "                hull_pts = arr[hull.vertices]\n",
    "                poly = Path(hull_pts, closed=True)\n",
    "                inside = poly.contains_point([theta_pt, r_pt], radius=-1e-9)\n",
    "                if inside:\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    update_group_limits(g)\n",
    "                    return True\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Si falla, no se agrega.\n",
    "        else:\n",
    "            half_theta = theta_strict_window / 2.0\n",
    "            half_r = r_strict_window / 2.0\n",
    "            t_min = pts['theta'].min()\n",
    "            t_max = pts['theta'].max()\n",
    "            rr_min = pts['r'].min()\n",
    "            rr_max = pts['r'].max()\n",
    "            if (theta_pt >= t_min - half_theta and theta_pt <= t_max + half_theta and\n",
    "                r_pt >= rr_min - half_r and r_pt <= rr_max + half_r):\n",
    "                g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                update_group_limits(g)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def process_spiral_arms_by_proximity(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    theta_merge_threshold=7.0,\n",
    "    r_merge_threshold=7.0,\n",
    "    r_gap=2.0,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa datos y agrupa puntos de forma estricta con convex hull:\n",
    "      1) Filtra por percentil y por theta (50-250).\n",
    "      2) Recorre de izquierda a derecha (orden ascendente por theta).\n",
    "      3) Para cada punto, se intenta agregar a un grupo:\n",
    "         - Si el grupo tiene ≥3 puntos, se exige que el punto esté estrictamente dentro del hull.\n",
    "         - Para grupos con <3 puntos, se usa una bounding box reducida (la mitad de la ventana).\n",
    "         - Se verifica que no haya un salto en r mayor a r_gap.\n",
    "      4) Si no se agrega, se crea un grupo nuevo.\n",
    "      5) Al final se fusionan grupos: se extienden sus bounding boxes en 10% y se fusionan si\n",
    "         hay superposición y la variación en la pendiente es ≤15%.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='theta', inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Inicializar grupos a partir de clusters de DBSCAN\n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "\n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "    \n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "    \n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']].values\n",
    "            y_ = pts['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'cluster_id': cid,\n",
    "            'points': pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Asignación iterativa de puntos\n",
    "    df_local = df_filtered.copy()\n",
    "    discarded_points = []\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            # Condición extra: el punto debe estar dentro del rango horizontal del grupo extendido ±theta_margin\n",
    "            group_theta_min = g['points']['theta'].min()\n",
    "            group_theta_max = g['points']['theta'].max()\n",
    "            theta_margin = 0.10 * (group_theta_max - group_theta_min)  # 10% de extensión\n",
    "            if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                continue\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                # Verificar que la distancia de todos los puntos al ajuste sea menor a point_distance_tolerance\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    # Fusionar solo si la variación en pendiente es ≤15%\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= 0.15:\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "    \n",
    "    # Reasignar descartados\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                group_theta_min = g['points']['theta'].min()\n",
    "                group_theta_max = g['points']['theta'].max()\n",
    "                theta_margin = 0.10 * (group_theta_max - group_theta_min)\n",
    "                if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance and g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= 0.15:\n",
    "                        g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(new_slope, g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'cluster_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "\n",
    "    # Subdivisión opcional si hay un gap grande en theta\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "\n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "\n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "\n",
    "    # Filtrar grupos según PA\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "\n",
    "    return valid_groups, df\n",
    "\n",
    "###############################################################################\n",
    "# EJEMPLO DE EJECUCIÓN Y GRAFICACIÓN\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.155\n",
    "    radius_tolerance = 0.45\n",
    "    point_distance_tolerance = 0.59\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    eps_polar = 0.7\n",
    "    min_samples_polar = 18\n",
    "    slope_variation_threshold = 0.20  # 20%\n",
    "    max_theta_gap = 15.0\n",
    "    do_subdivide = True\n",
    "\n",
    "    valid_groups, df = process_spiral_arms_polar_intermediate(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        point_distance_tolerance=point_distance_tolerance,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        eps_polar=eps_polar,\n",
    "        min_samples_polar=min_samples_polar,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        do_subdivide=do_subdivide\n",
    "    )\n",
    "\n",
    "    print(\"Total de grupos finales:\", len(valid_groups))\n",
    "\n",
    "    # Gráfica en (theta vs r)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['theta'], df['r'], s=1, color='silver', alpha=0.5, label='Datos Originales')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_vals = pts['theta']\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(theta_vals, r_vals, s=10, alpha=0.8, color=color,\n",
    "                    label=f\"Grupo {idx+1}, PA={pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(theta_vals.min(), theta_vals.max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Ajustes intermedios en (r, θ) - Halo {id_halo}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfica polar\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(10, 8))\n",
    "    theta_all_rad = np.radians(df['theta'])\n",
    "    ax.scatter(theta_all_rad, df['r'], s=1, color='gray', alpha=0.5, label='Datos Originales')\n",
    "\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_rad = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_rad, r_vals, s=10, alpha=0.8, color=color,\n",
    "                   label=f\"Grupo {idx+1} PA: {pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "\n",
    "    ax.set_title(f\"Ajustes intermedios en proyección polar - Halo {id_halo}\", fontsize=14, pad=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b4d7cb9-11b9-48ab-9e2c-b9b6781d8036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69dd24bc-8b2d-4041-a024-a0223536969c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8daba0-03f2-446f-826b-3eaab7545e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.path import Path\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica un estado en el plano (θ, r).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'darkblue']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if len(pts) >= 3:\n",
    "            pts_arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(pts_arr)\n",
    "                hull_pts = pts_arr[hull.vertices]\n",
    "                hull_pts = np.concatenate([hull_pts, hull_pts[:1]], axis=0)  # Cerrar el polígono\n",
    "                plt.plot(hull_pts[:,0], hull_pts[:,1], '-', color=color, lw=2)\n",
    "            except Exception:\n",
    "                pass\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def update_group_limits(group):\n",
    "    \"\"\"\n",
    "    Recalcula los límites (θ_min, θ_max, r_min, r_max) usando el convex hull si hay ≥3 puntos;\n",
    "    en caso contrario, usa la bounding box simple.\n",
    "    \"\"\"\n",
    "    pts = group['points']\n",
    "    if len(pts) >= 3:\n",
    "        arr = pts[['theta','r']].values\n",
    "        try:\n",
    "            hull = ConvexHull(arr)\n",
    "            hull_pts = arr[hull.vertices]\n",
    "            group['theta_min'] = hull_pts[:,0].min()\n",
    "            group['theta_max'] = hull_pts[:,0].max()\n",
    "            group['r_min'] = hull_pts[:,1].min()\n",
    "            group['r_max'] = hull_pts[:,1].max()\n",
    "            return\n",
    "        except Exception:\n",
    "            pass\n",
    "    group['theta_min'] = pts['theta'].min()\n",
    "    group['theta_max'] = pts['theta'].max()\n",
    "    group['r_min'] = pts['r'].min()\n",
    "    group['r_max'] = pts['r'].max()\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def point_in_hull(point, hull_points):\n",
    "    \"\"\"\n",
    "    Verifica si un punto [θ, r] está estrictamente dentro del polígono definido por hull_points.\n",
    "    \"\"\"\n",
    "    poly = Path(hull_points, closed=True)\n",
    "    return poly.contains_point(point, radius=-1e-9)  # Usamos un radio negativo para mayor estrictidad\n",
    "\n",
    "def merge_all_groups(groups, theta_merge_threshold, r_merge_threshold):\n",
    "    \"\"\"\n",
    "    Fusiona grupos si, al extender sus límites en un 10%, se superponen y la variación\n",
    "    relativa en la pendiente es ≤15%.\n",
    "    \n",
    "    Para cada grupo se calculan los límites extendidos:\n",
    "      ext_θ_min = θ_min - 0.1*(θ_max-θ_min)\n",
    "      ext_θ_max = θ_max + 0.1*(θ_max-θ_min)\n",
    "      (similar para r)\n",
    "    \n",
    "    Se fusionan dos grupos si sus cajas extendidas se superponen en ambas dimensiones\n",
    "    y la variación en pendiente es ≤ 0.15.\n",
    "    \"\"\"\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                g1 = groups[i]\n",
    "                g2 = groups[j]\n",
    "                # Límites originales\n",
    "                th_min1, th_max1, r_min1, r_max1 = g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max']\n",
    "                th_min2, th_max2, r_min2, r_max2 = g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max']\n",
    "                \n",
    "                # Calcular rangos y extender en 10%\n",
    "                range_th1 = th_max1 - th_min1 if (th_max1 - th_min1) != 0 else 1e-3\n",
    "                range_r1  = r_max1 - r_min1 if (r_max1 - r_min1) != 0 else 1e-3\n",
    "                ext_th_min1 = th_min1 - 0.1 * range_th1\n",
    "                ext_th_max1 = th_max1 + 0.1 * range_th1\n",
    "                ext_r_min1  = r_min1 - 0.1 * range_r1\n",
    "                ext_r_max1  = r_max1 + 0.1 * range_r1\n",
    "                \n",
    "                range_th2 = th_max2 - th_min2 if (th_max2 - th_min2) != 0 else 1e-3\n",
    "                range_r2  = r_max2 - r_min2 if (r_max2 - r_min2) != 0 else 1e-3\n",
    "                ext_th_min2 = th_min2 - 0.1 * range_th2\n",
    "                ext_th_max2 = th_max2 + 0.1 * range_th2\n",
    "                ext_r_min2  = r_min2 - 0.1 * range_r2\n",
    "                ext_r_max2  = r_max2 + 0.1 * range_r2\n",
    "                \n",
    "                # Superposición de las cajas extendidas\n",
    "                overlap_theta = not (ext_th_max1 < ext_th_min2 or ext_th_min1 > ext_th_max2)\n",
    "                overlap_r = not (ext_r_max1 < ext_r_min2 or ext_r_min1 > ext_r_max2)\n",
    "                \n",
    "                # Variación relativa en pendiente\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    rel_variation = 0\n",
    "                else:\n",
    "                    rel_variation = abs(g1['slope'] - g2['slope']) / (abs(g1['slope']) + 1e-12)\n",
    "                \n",
    "                if overlap_theta and overlap_r and rel_variation <= 0.15:\n",
    "                    # Fusionar: unir puntos y recalcular ajuste\n",
    "                    merged_points = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    model = LinearRegression()\n",
    "                    X_new = merged_points[['theta']].values\n",
    "                    y_new = merged_points['r'].values\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    new_intercept = model.intercept_\n",
    "                    g1['points'] = merged_points\n",
    "                    g1['slope'] = new_slope\n",
    "                    g1['intercept'] = new_intercept\n",
    "                    g1['pa'] = calculate_pa(new_slope, new_intercept)\n",
    "                    update_group_limits(g1)\n",
    "                    groups.pop(j)\n",
    "                    merged_flag = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "\n",
    "def add_point_strict_convex(point, groups, theta_strict_window, r_strict_window, r_gap=2.0):\n",
    "    \"\"\"\n",
    "    Intenta agregar 'point' a un grupo existente de forma estricta:\n",
    "      - Para grupos con ≥3 puntos se calcula el convex hull y se exige que el punto esté estrictamente dentro.\n",
    "      - Para grupos con <3 puntos se usa una bounding box reducida (usando la mitad de theta_strict_window y r_strict_window).\n",
    "      - Se verifica que el nuevo punto no exceda r_gap en r.\n",
    "    \"\"\"\n",
    "    theta_pt = point['theta']\n",
    "    r_pt = point['r']\n",
    "    \n",
    "    for g in groups:\n",
    "        if r_pt < g['r_min'] - r_gap or r_pt > g['r_max'] + r_gap:\n",
    "            continue\n",
    "        pts = g['points']\n",
    "        if len(pts) >= 3:\n",
    "            arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(arr)\n",
    "                hull_pts = arr[hull.vertices]\n",
    "                poly = Path(hull_pts, closed=True)\n",
    "                inside = poly.contains_point([theta_pt, r_pt], radius=-1e-9)\n",
    "                if inside:\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    update_group_limits(g)\n",
    "                    return True\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            half_theta = theta_strict_window / 2.0\n",
    "            half_r = r_strict_window / 2.0\n",
    "            t_min = pts['theta'].min()\n",
    "            t_max = pts['theta'].max()\n",
    "            rr_min = pts['r'].min()\n",
    "            rr_max = pts['r'].max()\n",
    "            if (theta_pt >= t_min - half_theta and theta_pt <= t_max + half_theta and\n",
    "                r_pt >= rr_min - half_r and r_pt <= rr_max + half_r):\n",
    "                g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                update_group_limits(g)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def process_spiral_arms_by_proximity(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    theta_merge_threshold=7.0,\n",
    "    r_merge_threshold=7.0,\n",
    "    r_gap=2.0,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa datos y agrupa puntos de forma estricta usando convex hull, recorriendo de izquierda a derecha.\n",
    "    1) Se filtra por percentil y por θ (50-250).\n",
    "    2) Se ordena el DataFrame por θ (izquierda a derecha).\n",
    "    3) Para cada punto se intenta agregar a un grupo:\n",
    "         - Para grupos con ≥3 puntos se exige que el punto esté estrictamente dentro del convex hull.\n",
    "         - Para grupos con <3 puntos se usa una bounding box reducida (la mitad de la ventana).\n",
    "         - Se verifica que no haya un salto en r mayor a r_gap.\n",
    "         - Además, el punto debe estar dentro del rango horizontal actual extendido en ±10% del rango.\n",
    "    4) Si no se agrega, se crea un grupo nuevo.\n",
    "    5) Finalmente, se fusionan grupos extendiendo sus límites en un 10% y fusionando solo si la variación en pendiente es ≤15%.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='theta', inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X_polar = df_filtered[['r', 'theta']].values\n",
    "    dbscan = DBSCAN(eps=eps_polar, min_samples=min_samples_polar)\n",
    "    df_filtered['cluster'] = dbscan.fit_predict(X_polar)\n",
    "\n",
    "    \n",
    "    # Inicializar grupos a partir de clusters de DBSCAN fusionados\n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "\n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "    \n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "    \n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']].values\n",
    "            y_ = pts['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'cluster_id': cid,\n",
    "            'points': pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Recorrido de izquierda a derecha: se ordena por θ\n",
    "    df_local = df_filtered.copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    df_local.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    discarded_points = []\n",
    "    iteration = 0\n",
    "    while len(df_local) > 0:\n",
    "        # Siempre se toma el primer punto (el de menor θ)\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            group_theta_min = g['points']['theta'].min()\n",
    "            group_theta_max = g['points']['theta'].max()\n",
    "            theta_range = group_theta_max - group_theta_min if group_theta_max - group_theta_min > 0 else 1e-3\n",
    "            theta_margin = 0.10 * theta_range  # extensión del 10%\n",
    "            if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                continue\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= 0.15:\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        iteration += 1\n",
    "\n",
    "    # Reasignar descartados\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                group_theta_min = g['points']['theta'].min()\n",
    "                group_theta_max = g['points']['theta'].max()\n",
    "                theta_range = group_theta_max - group_theta_min if group_theta_max - group_theta_min > 0 else 1e-3\n",
    "                theta_margin = 0.10 * theta_range\n",
    "                if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance and g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= 0.15:\n",
    "                        g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(new_slope, g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'cluster_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "\n",
    "    # Subdivisión opcional en caso de gap grande en θ\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "\n",
    "    return valid_groups, df\n",
    "\n",
    "###############################################################################\n",
    "# EJEMPLO DE EJECUCIÓN Y GRAFICACIÓN\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.155\n",
    "    radius_tolerance = 0.45\n",
    "    point_distance_tolerance = 0.59\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    eps_polar = 0.7\n",
    "    min_samples_polar = 18\n",
    "    slope_variation_threshold = 0.20  # 20%\n",
    "    max_theta_gap = 15.0\n",
    "    do_subdivide = True\n",
    "\n",
    "    valid_groups, df = process_spiral_arms_by_proximity(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_fit_window=30.0,\n",
    "        r_fit_window=3.0,\n",
    "        theta_merge_threshold=10.0,\n",
    "        r_merge_threshold=2.0,\n",
    "        r_gap=2.0,\n",
    "        snapshot_interval=10\n",
    "    )\n",
    "    print(\"Total de grupos finales:\", len(valid_groups))\n",
    "    \n",
    "    # Gráfica en (θ vs r)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['theta'], df['r'], s=1, color='silver', alpha=0.5, label='Datos Originales')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_vals = pts['theta']\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(theta_vals, r_vals, s=10, alpha=0.8, color=color,\n",
    "                    label=f\"Grupo {idx+1}, PA={pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(theta_vals.min(), theta_vals.max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Ajustes intermedios en (r, θ) - Halo {id_halo}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfica en proyección polar\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(10, 8))\n",
    "    theta_all_rad = np.radians(df['theta'])\n",
    "    ax.scatter(theta_all_rad, df['r'], s=1, color='gray', alpha=0.5, label='Datos Originales')\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_rad = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_rad, r_vals, s=10, alpha=0.8, color=color,\n",
    "                   label=f\"Grupo {idx+1} PA: {pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    ax.set_title(f\"Ajustes intermedios en proyección polar - Halo {id_halo}\", fontsize=14, pad=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2200f0d6-bdd6-4680-953f-1c20e5566154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "# Preparar la matriz de características en el espacio polar:\n",
    "X_polar = df_filtered[['r', 'theta']].values\n",
    "\n",
    "# Configurar Birch: \n",
    "# - 'threshold' controla la distancia máxima entre puntos en un subcluster.\n",
    "# - 'n_clusters=None' hace que el modelo forme tantos subclusters como sea necesario.\n",
    "# - 'branching_factor' se puede usar para ajustar la estructura del árbol (opcional).\n",
    "birch = Birch(threshold=eps_polar, n_clusters=None, branching_factor=min_samples_polar)\n",
    "df_filtered['cluster'] = birch.fit_predict(X_polar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e533bfe4-809b-4b14-a9b7-c9d94763aa10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.path import Path\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica un estado en el plano (θ, r).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'darkblue']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if len(pts) >= 3:\n",
    "            pts_arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(pts_arr)\n",
    "                hull_pts = pts_arr[hull.vertices]\n",
    "                hull_pts = np.concatenate([hull_pts, hull_pts[:1]], axis=0)\n",
    "                plt.plot(hull_pts[:,0], hull_pts[:,1], '-', color=color, lw=2)\n",
    "            except Exception:\n",
    "                pass\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def update_group_limits(group):\n",
    "    pts = group['points']\n",
    "    if len(pts) >= 3:\n",
    "        arr = pts[['theta','r']].values\n",
    "        try:\n",
    "            hull = ConvexHull(arr)\n",
    "            hull_pts = arr[hull.vertices]\n",
    "            group['theta_min'] = hull_pts[:,0].min()\n",
    "            group['theta_max'] = hull_pts[:,0].max()\n",
    "            group['r_min'] = hull_pts[:,1].min()\n",
    "            group['r_max'] = hull_pts[:,1].max()\n",
    "            return\n",
    "        except Exception:\n",
    "            pass\n",
    "    group['theta_min'] = pts['theta'].min()\n",
    "    group['theta_max'] = pts['theta'].max()\n",
    "    group['r_min'] = pts['r'].min()\n",
    "    group['r_max'] = pts['r'].max()\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def point_in_hull(point, hull_points):\n",
    "    poly = Path(hull_points, closed=True)\n",
    "    return poly.contains_point(point, radius=-1e-9)\n",
    "\n",
    "def merge_all_groups(groups, theta_merge_threshold, r_merge_threshold):\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                g1 = groups[i]\n",
    "                g2 = groups[j]\n",
    "                th_min1, th_max1, r_min1, r_max1 = g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max']\n",
    "                th_min2, th_max2, r_min2, r_max2 = g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max']\n",
    "                range_th1 = th_max1 - th_min1 if (th_max1 - th_min1) != 0 else 1e-3\n",
    "                range_r1  = r_max1 - r_min1 if (r_max1 - r_min1) != 0 else 1e-3\n",
    "                ext_th_min1 = th_min1 - 0.1 * range_th1\n",
    "                ext_th_max1 = th_max1 + 0.1 * range_th1\n",
    "                ext_r_min1  = r_min1 - 0.1 * range_r1\n",
    "                ext_r_max1  = r_max1 + 0.1 * range_r1\n",
    "                range_th2 = th_max2 - th_min2 if (th_max2 - th_min2) != 0 else 1e-3\n",
    "                range_r2  = r_max2 - r_min2 if (r_max2 - r_min2) != 0 else 1e-3\n",
    "                ext_th_min2 = th_min2 - 0.1 * range_th2\n",
    "                ext_th_max2 = th_max2 + 0.1 * range_th2\n",
    "                ext_r_min2  = r_min2 - 0.1 * range_r2\n",
    "                ext_r_max2  = r_max2 + 0.1 * range_r2\n",
    "                overlap_theta = not (ext_th_max1 < ext_th_min2 or ext_th_min1 > ext_th_max2)\n",
    "                overlap_r = not (ext_r_max1 < ext_r_min2 or ext_r_min1 > ext_r_max2)\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    rel_variation = 0\n",
    "                else:\n",
    "                    rel_variation = abs(g1['slope'] - g2['slope']) / (abs(g1['slope']) + 1e-12)\n",
    "                if overlap_theta and overlap_r and rel_variation <= 0.15:\n",
    "                    merged_points = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    model = LinearRegression()\n",
    "                    X_new = merged_points[['theta']].values\n",
    "                    y_new = merged_points['r'].values\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    new_intercept = model.intercept_\n",
    "                    g1['points'] = merged_points\n",
    "                    g1['slope'] = new_slope\n",
    "                    g1['intercept'] = new_intercept\n",
    "                    g1['pa'] = calculate_pa(new_slope, new_intercept)\n",
    "                    update_group_limits(g1)\n",
    "                    groups.pop(j)\n",
    "                    merged_flag = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "\n",
    "def add_point_strict_convex(point, groups, theta_strict_window, r_strict_window, r_gap=2.0):\n",
    "    theta_pt = point['theta']\n",
    "    r_pt = point['r']\n",
    "    for g in groups:\n",
    "        if r_pt < g['r_min'] - r_gap or r_pt > g['r_max'] + r_gap:\n",
    "            continue\n",
    "        pts = g['points']\n",
    "        if len(pts) >= 3:\n",
    "            arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(arr)\n",
    "                hull_pts = arr[hull.vertices]\n",
    "                poly = Path(hull_pts, closed=True)\n",
    "                inside = poly.contains_point([theta_pt, r_pt], radius=-1e-9)\n",
    "                if inside:\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    update_group_limits(g)\n",
    "                    return True\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            half_theta = theta_strict_window / 2.0\n",
    "            half_r = r_strict_window / 2.0\n",
    "            t_min = pts['theta'].min()\n",
    "            t_max = pts['theta'].max()\n",
    "            rr_min = pts['r'].min()\n",
    "            rr_max = pts['r'].max()\n",
    "            if (theta_pt >= t_min - half_theta and theta_pt <= t_max + half_theta and\n",
    "                r_pt >= rr_min - half_r and r_pt <= rr_max + half_r):\n",
    "                g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                update_group_limits(g)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def process_spiral_arms_by_proximity(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    theta_merge_threshold=7.0,\n",
    "    r_merge_threshold=7.0,\n",
    "    r_gap=2.0,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa datos y agrupa puntos de forma estricta con convex hull, recorriendo de izquierda a derecha.\n",
    "    1) Se filtra por percentil y por θ (50-250).\n",
    "    2) Se ordena el DataFrame por θ (recorrido de izquierda a derecha).\n",
    "    3) Para cada punto se intenta agregar a un grupo:\n",
    "         - Para grupos con ≥3 puntos se exige que el punto esté estrictamente dentro del convex hull.\n",
    "         - Para grupos con <3 puntos se usa una bounding box reducida (la mitad de la ventana).\n",
    "         - Se verifica que no haya un salto en r mayor a r_gap.\n",
    "         - Además, el punto debe estar dentro del rango horizontal actual extendido en ±10% del rango.\n",
    "    4) Si no se agrega, se crea un grupo nuevo.\n",
    "    5) Finalmente, se fusionan grupos extendiendo sus límites en un 10% y fusionando solo si la variación en pendiente es ≤15%.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='theta', inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Inicializar grupos a partir de clusters de BIRCH o DBSCAN (aquí usamos DBSCAN como base)\n",
    "    # En este ejemplo se asume que df_filtered tiene la columna 'cluster' ya asignada.\n",
    "    # Si no existe, puedes asignarla usando:\n",
    "    # from sklearn.cluster import Birch\n",
    "    # birch = Birch(threshold=eps_polar, n_clusters=None, branching_factor=min_samples_polar)\n",
    "    # df_filtered['cluster'] = birch.fit_predict(df_filtered[['r','theta']].values)\n",
    "    if 'cluster' not in df_filtered.columns:\n",
    "        from sklearn.cluster import Birch\n",
    "        birch = Birch(threshold=0.7, n_clusters=None, branching_factor=min_samples_polar)\n",
    "        df_filtered['cluster'] = birch.fit_predict(df_filtered[['r','theta']].values)\n",
    "    \n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "\n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "    \n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "    \n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']].values\n",
    "            y_ = pts['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'cluster_id': cid,\n",
    "            'points': pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Recorrido de izquierda a derecha\n",
    "    df_local = df_filtered.copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    df_local.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    discarded_points = []\n",
    "    iteration = 0\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            group_theta_min = g['points']['theta'].min()\n",
    "            group_theta_max = g['points']['theta'].max()\n",
    "            theta_range = group_theta_max - group_theta_min if group_theta_max - group_theta_min > 0 else 1e-3\n",
    "            theta_margin = 0.10 * theta_range\n",
    "            if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                continue\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= 0.15:\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        iteration += 1\n",
    "\n",
    "    # Reasignar descartados\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                group_theta_min = g['points']['theta'].min()\n",
    "                group_theta_max = g['points']['theta'].max()\n",
    "                theta_range = group_theta_max - group_theta_min if group_theta_max - group_theta_min > 0 else 1e-3\n",
    "                theta_margin = 0.10 * theta_range\n",
    "                if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance and g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= 0.15:\n",
    "                        g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(new_slope, g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'cluster_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "\n",
    "    # Subdivisión opcional en caso de gap grande en θ\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "\n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "\n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "\n",
    "    return valid_groups, df\n",
    "\n",
    "###############################################################################\n",
    "# EJEMPLO DE EJECUCIÓN Y GRAFICACIÓN\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.155\n",
    "    radius_tolerance = 0.45\n",
    "    point_distance_tolerance = 0.59\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    eps_polar = 0.7\n",
    "    min_samples_polar = 18\n",
    "    slope_variation_threshold = 0.20  # 20%\n",
    "    max_theta_gap = 15.0\n",
    "    do_subdivide = True\n",
    "\n",
    "    valid_groups, df = process_spiral_arms_by_proximity(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_fit_window=30.0,\n",
    "        r_fit_window=3.0,\n",
    "        theta_merge_threshold=10.0,\n",
    "        r_merge_threshold=2.0,\n",
    "        r_gap=2.0,\n",
    "        snapshot_interval=10\n",
    "    )\n",
    "    print(\"Total de grupos finales:\", len(valid_groups))\n",
    "\n",
    "    # Gráfica en (θ vs r)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['theta'], df['r'], s=1, color='silver', alpha=0.5, label='Datos Originales')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_vals = pts['theta']\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(theta_vals, r_vals, s=10, alpha=0.8, color=color,\n",
    "                    label=f\"Grupo {idx+1}, PA={pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(theta_vals.min(), theta_vals.max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Ajustes intermedios en (r, θ) - Halo {id_halo}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfica en proyección polar\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(10, 8))\n",
    "    theta_all_rad = np.radians(df['theta'])\n",
    "    ax.scatter(theta_all_rad, df['r'], s=1, color='gray', alpha=0.5, label='Datos Originales')\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_rad = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_rad, r_vals, s=10, alpha=0.8, color=color,\n",
    "                   label=f\"Grupo {idx+1} PA: {pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    ax.set_title(f\"Ajustes intermedios en proyección polar - Halo {id_halo}\", fontsize=14, pad=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41987f59-0c3a-4b1e-8d8f-b5c59c3a3767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f86d29-607f-40a7-83f2-78369bb7c24a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.path import Path\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##############################\n",
    "# Funciones de Visualización\n",
    "##############################\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica los grupos en el plano (θ, r) junto con los datos completos.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'darkblue']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=15, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if len(pts) >= 3:\n",
    "            arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(arr)\n",
    "                hull_pts = arr[hull.vertices]\n",
    "                hull_pts = np.concatenate([hull_pts, hull_pts[:1]], axis=0)\n",
    "                plt.plot(hull_pts[:,0], hull_pts[:,1], '-', color=color, lw=2)\n",
    "            except Exception:\n",
    "                pass\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "##############################\n",
    "# Funciones de Actualización y Fusión\n",
    "##############################\n",
    "def update_group_limits(group):\n",
    "    \"\"\"\n",
    "    Recalcula los límites del grupo (θ_min, θ_max, r_min, r_max) usando el convex hull\n",
    "    si el grupo tiene al menos 3 puntos; si no, usa la bounding box.\n",
    "    \"\"\"\n",
    "    pts = group['points']\n",
    "    if len(pts) >= 3:\n",
    "        arr = pts[['theta','r']].values\n",
    "        try:\n",
    "            hull = ConvexHull(arr)\n",
    "            hull_pts = arr[hull.vertices]\n",
    "            group['theta_min'] = hull_pts[:,0].min()\n",
    "            group['theta_max'] = hull_pts[:,0].max()\n",
    "            group['r_min'] = hull_pts[:,1].min()\n",
    "            group['r_max'] = hull_pts[:,1].max()\n",
    "            return\n",
    "        except Exception:\n",
    "            pass\n",
    "    group['theta_min'] = pts['theta'].min()\n",
    "    group['theta_max'] = pts['theta'].max()\n",
    "    group['r_min'] = pts['r'].min()\n",
    "    group['r_max'] = pts['r'].max()\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def point_in_hull(point, hull_points):\n",
    "    poly = Path(hull_points, closed=True)\n",
    "    return poly.contains_point(point, radius=-1e-9)\n",
    "\n",
    "def merge_all_groups(groups, theta_merge_threshold, r_merge_threshold):\n",
    "    \"\"\"\n",
    "    Fusiona grupos extendiendo sus límites en un 10% y fusiona\n",
    "    solo si las cajas extendidas se superponen y la variación relativa en pendiente es ≤15%.\n",
    "    \"\"\"\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                g1 = groups[i]\n",
    "                g2 = groups[j]\n",
    "                # Límites originales\n",
    "                th_min1, th_max1, r_min1, r_max1 = g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max']\n",
    "                th_min2, th_max2, r_min2, r_max2 = g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max']\n",
    "                # Extender límites en 10%\n",
    "                range_th1 = th_max1 - th_min1 if (th_max1 - th_min1)!=0 else 1e-3\n",
    "                range_r1 = r_max1 - r_min1 if (r_max1 - r_min1)!=0 else 1e-3\n",
    "                ext_th_min1 = th_min1 - 0.1 * range_th1\n",
    "                ext_th_max1 = th_max1 + 0.1 * range_th1\n",
    "                ext_r_min1  = r_min1 - 0.1 * range_r1\n",
    "                ext_r_max1  = r_max1 + 0.1 * range_r1\n",
    "                range_th2 = th_max2 - th_min2 if (th_max2 - th_min2)!=0 else 1e-3\n",
    "                range_r2 = r_max2 - r_min2 if (r_max2 - r_min2)!=0 else 1e-3\n",
    "                ext_th_min2 = th_min2 - 0.1 * range_th2\n",
    "                ext_th_max2 = th_max2 + 0.1 * range_th2\n",
    "                ext_r_min2  = r_min2 - 0.1 * range_r2\n",
    "                ext_r_max2  = r_max2 + 0.1 * range_r2\n",
    "                # Superposición\n",
    "                overlap_theta = not (ext_th_max1 < ext_th_min2 or ext_th_min1 > ext_th_max2)\n",
    "                overlap_r = not (ext_r_max1 < ext_r_min2 or ext_r_min1 > ext_r_max2)\n",
    "                # Variación en pendiente\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    rel_variation = 0\n",
    "                else:\n",
    "                    rel_variation = abs(g1['slope'] - g2['slope']) / (abs(g1['slope'])+1e-12)\n",
    "                if overlap_theta and overlap_r and rel_variation <= 0.15:\n",
    "                    # Fusionar g2 en g1\n",
    "                    merged_points = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    model = LinearRegression()\n",
    "                    X_new = merged_points[['theta']].values\n",
    "                    y_new = merged_points['r'].values\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    new_intercept = model.intercept_\n",
    "                    g1['points'] = merged_points\n",
    "                    g1['slope'] = new_slope\n",
    "                    g1['intercept'] = new_intercept\n",
    "                    g1['pa'] = calculate_pa(new_slope, new_intercept)\n",
    "                    update_group_limits(g1)\n",
    "                    groups.pop(j)\n",
    "                    merged_flag = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "\n",
    "##############################\n",
    "# Asignación de puntos con Convex Hull y recorrido de izquierda a derecha\n",
    "##############################\n",
    "def add_point_strict_convex(point, groups, theta_strict_window, r_strict_window, r_gap=2.0):\n",
    "    \"\"\"\n",
    "    Intenta agregar 'point' a un grupo existente de forma estricta:\n",
    "      - Si el grupo tiene ≥3 puntos, se calcula su convex hull y se exige que el punto esté estrictamente dentro.\n",
    "      - Para grupos con <3 puntos se usa una bounding box reducida (usando la mitad de theta_strict_window y r_strict_window).\n",
    "      - Se verifica que el nuevo punto no exceda r_gap en r.\n",
    "    \"\"\"\n",
    "    theta_pt = point['theta']\n",
    "    r_pt = point['r']\n",
    "    for g in groups:\n",
    "        if r_pt < g['r_min'] - r_gap or r_pt > g['r_max'] + r_gap:\n",
    "            continue\n",
    "        pts = g['points']\n",
    "        if len(pts) >= 3:\n",
    "            arr = pts[['theta','r']].values\n",
    "            try:\n",
    "                hull = ConvexHull(arr)\n",
    "                hull_pts = arr[hull.vertices]\n",
    "                poly = Path(hull_pts, closed=True)\n",
    "                inside = poly.contains_point([theta_pt, r_pt], radius=-1e-9)\n",
    "                if inside:\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    update_group_limits(g)\n",
    "                    return True\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            half_theta = theta_strict_window / 2.0\n",
    "            half_r = r_strict_window / 2.0\n",
    "            t_min = pts['theta'].min()\n",
    "            t_max = pts['theta'].max()\n",
    "            rr_min = pts['r'].min()\n",
    "            rr_max = pts['r'].max()\n",
    "            if (theta_pt >= t_min - half_theta and theta_pt <= t_max + half_theta and\n",
    "                r_pt >= rr_min - half_r and r_pt <= rr_max + half_r):\n",
    "                g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                update_group_limits(g)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "##############################\n",
    "# Función principal final\n",
    "##############################\n",
    "def process_spiral_arms_by_proximity(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    theta_merge_threshold=7.0,\n",
    "    r_merge_threshold=7.0,\n",
    "    r_gap=2.0,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa datos y agrupa puntos de forma estricta con convex hull, recorriendo de izquierda a derecha.\n",
    "    1) Filtra por percentil y por θ (50-250).\n",
    "    2) Ordena el DataFrame por θ.\n",
    "    3) Para cada punto se intenta agregar a un grupo:\n",
    "         - Para grupos con ≥3 puntos se exige que el punto esté estrictamente dentro del convex hull.\n",
    "         - Para grupos con <3 puntos se usa una bounding box reducida (la mitad de la ventana).\n",
    "         - Se verifica que el punto no exceda r_gap en r.\n",
    "         - Además, el punto debe estar dentro del rango horizontal actual extendido en ±10% del rango.\n",
    "    4) Si no se agrega, se crea un grupo nuevo.\n",
    "    5) Finalmente, se fusionan grupos extendiendo sus límites en un 10% y fusionando solo si la variación en pendiente es ≤15%.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='theta', inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Inicializar grupos a partir de clusters de BIRCH (más robusto y eficiente)\n",
    "    from sklearn.cluster import Birch\n",
    "    birch = Birch(threshold=0.7, n_clusters=None, branching_factor=20)\n",
    "    X_polar = df_filtered[['r','theta']].values\n",
    "    df_filtered['cluster'] = birch.fit_predict(X_polar)\n",
    "    \n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "\n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "    \n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "    \n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']].values\n",
    "            y_ = pts['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'cluster_id': cid,\n",
    "            'points': pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Recorrido de izquierda a derecha (orden creciente de θ)\n",
    "    df_local = df_filtered.copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    df_local.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    discarded_points = []\n",
    "    iteration = 0\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            group_theta_min = g['points']['theta'].min()\n",
    "            group_theta_max = g['points']['theta'].max()\n",
    "            theta_range = group_theta_max - group_theta_min if group_theta_max - group_theta_min > 0 else 1e-3\n",
    "            theta_margin = 0.10 * theta_range\n",
    "            if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                continue\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                # Verificar que la distancia de cada punto en el grupo al ajuste sea ≤ point_distance_tolerance\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= 0.15:\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(new_slope, g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        iteration += 1\n",
    "\n",
    "    # Reasignar puntos descartados\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                group_theta_min = g['points']['theta'].min()\n",
    "                group_theta_max = g['points']['theta'].max()\n",
    "                theta_range = group_theta_max - group_theta_min if group_theta_max - group_theta_min > 0 else 1e-3\n",
    "                theta_margin = 0.10 * theta_range\n",
    "                if not (group_theta_min - theta_margin <= theta_pt <= group_theta_max + theta_margin):\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance and g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) <= 0.15:\n",
    "                        g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(new_slope, g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'cluster_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "\n",
    "    # Subdivisión opcional en caso de gap grande en θ\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "\n",
    "    return valid_groups, df\n",
    "\n",
    "##############################\n",
    "# Ejecución y Visualización\n",
    "##############################\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros de ejemplo (ajústalos según tus necesidades)\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.155\n",
    "    radius_tolerance = 0.45\n",
    "    point_distance_tolerance = 0.59\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    eps_polar = 0.7\n",
    "    min_samples_polar = 18\n",
    "    slope_variation_threshold = 0.20  # 20%\n",
    "    max_theta_gap = 15.0\n",
    "    do_subdivide = True\n",
    "\n",
    "    valid_groups, df = process_spiral_arms_by_proximity(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_fit_window=30.0,\n",
    "        r_fit_window=3.0,\n",
    "        theta_merge_threshold=10.0,\n",
    "        r_merge_threshold=2.0,\n",
    "        r_gap=2.0,\n",
    "        snapshot_interval=10\n",
    "    )\n",
    "    print(\"Total de grupos finales:\", len(valid_groups))\n",
    "    \n",
    "    # Gráfica en (θ vs r)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['theta'], df['r'], s=1, color='silver', alpha=0.5, label='Datos Originales')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_vals = pts['theta']\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(theta_vals, r_vals, s=10, alpha=0.8, color=color,\n",
    "                    label=f\"Grupo {idx+1}, PA={pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(theta_vals.min(), theta_vals.max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Ajustes intermedios en (r, θ) - Halo {id_halo}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfica en proyección polar\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(10, 8))\n",
    "    theta_all_rad = np.radians(df['theta'])\n",
    "    ax.scatter(theta_all_rad, df['r'], s=1, color='gray', alpha=0.5, label='Datos Originales')\n",
    "    for idx, grp in enumerate(valid_groups):\n",
    "        pts = grp['points']\n",
    "        theta_rad = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        pa_val = grp['pa'] if grp['pa'] is not None else 0.0\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_rad, r_vals, s=10, alpha=0.8, color=color,\n",
    "                   label=f\"Grupo {idx+1} PA: {pa_val:.2f}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    ax.set_title(f\"Ajustes intermedios en proyección polar - Halo {id_halo}\", fontsize=14, pad=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "989a06fc-7923-4d76-8c31-426fe9c7dc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cdd9a98-8b03-41a7-b4ba-5504dcaf12f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(\"data_rho_17_filtered.csv\")\n",
    "\n",
    "# Calcular coordenadas polares\n",
    "df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "\n",
    "# Filtrar puntos en el rango de theta (por ejemplo, 50 a 250 grados)\n",
    "df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "\n",
    "# Filtrar por percentil en 'rho_resta_final_exp' (ajusta el valor según necesites)\n",
    "quartile_threshold = 0.55\n",
    "threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "\n",
    "# Crear matriz de características en el espacio polar\n",
    "X = df_filtered[['r', 'theta']].values\n",
    "\n",
    "# Aplicar clustering con BIRCH\n",
    "birch = Birch(threshold=0.7, n_clusters=10, branching_factor=50)\n",
    "df_filtered['cluster'] = birch.fit_predict(X)\n",
    "\n",
    "# Visualizar los clusters en un gráfico de θ vs r\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'darkblue']\n",
    "for cluster in np.unique(df_filtered['cluster']):\n",
    "    # Puedes filtrar el ruido (si se usa -1) o mostrarlos todos\n",
    "    subset = df_filtered[df_filtered['cluster'] == cluster]\n",
    "    plt.scatter(subset['theta'], subset['r'], s=10, color=colors[int(cluster) % len(colors)], label=f\"Cluster {cluster}\")\n",
    "plt.xlabel(\"θ (grados)\")\n",
    "plt.ylabel(\"r\")\n",
    "plt.title(\"Clustering con BIRCH en coordenadas polares\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3afc0f-5242-4eae-b5bf-ea2bdfa98f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv(\"data_rho_17_filtered.csv\")\n",
    "\n",
    "# 2. Calcular coordenadas polares\n",
    "df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "\n",
    "# 3. Filtrar por rango de θ\n",
    "df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "\n",
    "# 4. Filtrar por percentil\n",
    "quartile_threshold = 0.55\n",
    "threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "\n",
    "# 5. Escalar variables para equilibrar r y theta\n",
    "#    Ajusta los divisores según tus datos (prueba y error).\n",
    "df_filtered['r_scaled'] = df_filtered['r'] / 5.0      # por ejemplo\n",
    "df_filtered['theta_scaled'] = df_filtered['theta'] / 50.0\n",
    "\n",
    "X = df_filtered[['r_scaled', 'theta_scaled']].values\n",
    "\n",
    "# 6. Ajustar BIRCH con parámetros ajustados\n",
    "#    threshold más bajo => más subclusters\n",
    "birch = Birch(threshold=0.5, n_clusters=None, branching_factor=20)\n",
    "df_filtered['cluster'] = birch.fit_predict(X)\n",
    "\n",
    "# 7. Visualizar\n",
    "plt.figure(figsize=(10,6))\n",
    "colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "for cluster_id in np.unique(df_filtered['cluster']):\n",
    "    subset = df_filtered[df_filtered['cluster'] == cluster_id]\n",
    "    c = colors[int(cluster_id) % len(colors)]\n",
    "    plt.scatter(subset['theta'], subset['r'], s=10, color=c, label=f\"Cluster {cluster_id}\")\n",
    "plt.xlabel(\"θ (grados)\")\n",
    "plt.ylabel(\"r\")\n",
    "plt.title(\"Clustering con BIRCH (escalado) en (r, θ)\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de226d6-ca3a-46b7-8159-a8a35d86ee4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "# ============================\n",
    "# Funciones para subdividir clusters por gaps\n",
    "# ============================\n",
    "def subdivide_by_theta_gap(df_cluster, gap_threshold=2.5):\n",
    "    \"\"\"\n",
    "    Recibe un DataFrame con un cluster y lo ordena por θ.\n",
    "    Si existe un gap (diferencia entre puntos consecutivos) mayor a gap_threshold en θ,\n",
    "    subdivide el cluster en segmentos.\n",
    "    Retorna una lista de DataFrames.\n",
    "    \"\"\"\n",
    "    df_cluster = df_cluster.sort_values(by='theta').reset_index(drop=True)\n",
    "    theta_vals = df_cluster['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    if np.all(dtheta <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    groups = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(dtheta):\n",
    "        if diff > gap_threshold:\n",
    "            groups.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    groups.append(df_cluster.iloc[start:].copy())\n",
    "    return groups\n",
    "\n",
    "def subdivide_by_r_gap(df_cluster, gap_threshold=1.5):\n",
    "    \"\"\"\n",
    "    Recibe un DataFrame con un cluster y lo ordena por r.\n",
    "    Si existe un gap mayor a gap_threshold en r, subdivide el cluster.\n",
    "    Retorna una lista de DataFrames.\n",
    "    \"\"\"\n",
    "    df_cluster = df_cluster.sort_values(by='r').reset_index(drop=True)\n",
    "    r_vals = df_cluster['r'].values\n",
    "    dr = np.diff(r_vals)\n",
    "    if np.all(dr <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    groups = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(dr):\n",
    "        if diff > gap_threshold:\n",
    "            groups.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    groups.append(df_cluster.iloc[start:].copy())\n",
    "    return groups\n",
    "\n",
    "# ============================\n",
    "# Función para graficar clusters\n",
    "# ============================\n",
    "def plot_clusters(clusters, title=\"Clusters segmentados\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, df_cluster in enumerate(clusters):\n",
    "        c = colors[idx % len(colors)]\n",
    "        plt.scatter(df_cluster['theta'], df_cluster['r'], s=10, color=c, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ============================\n",
    "# Ejecución final: BIRCH + subdivisión por gaps\n",
    "# ============================\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv(\"data_rho_17_filtered.csv\")\n",
    "\n",
    "# 2. Calcular coordenadas polares\n",
    "df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "\n",
    "# 3. Filtrar por rango de θ (por ejemplo, 50 a 250 grados)\n",
    "df = df[(df['theta'] >= 20) & (df['theta'] <= 270)].copy()\n",
    "\n",
    "# 4. Filtrar por percentil en 'rho_resta_final_exp'\n",
    "quartile_threshold = 0.55\n",
    "threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "\n",
    "# 5. Escalar variables para equilibrar (opcional, ajusta según tus datos)\n",
    "# Aquí podemos escalar θ y r para que tengan una contribución similar.\n",
    "# Por ejemplo, si r suele estar en [0,20] y θ en [50,250], se puede dividir θ entre 50.\n",
    "df_filtered['r_scaled'] = df_filtered['r'] /5.0 # O puedes dividir por algún factor\n",
    "df_filtered['theta_scaled'] = df_filtered['theta'] / 50.0\n",
    "\n",
    "# 6. Aplicar BIRCH para clustering inicial\n",
    "X = df_filtered[['r_scaled', 'theta_scaled']].values\n",
    "birch = Birch(threshold=0.5, n_clusters=None, branching_factor=20)\n",
    "df_filtered['cluster'] = birch.fit_predict(X)\n",
    "\n",
    "# 7. Extraer clusters de BIRCH\n",
    "clusters = []\n",
    "for cluster_id in np.unique(df_filtered['cluster']):\n",
    "    subset = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "    subset.reset_index(drop=True, inplace=True)\n",
    "    clusters.append(subset)\n",
    "\n",
    "# 8. Subdividir cada cluster si existe un gap mayor a 2.5 en θ o en r\n",
    "final_clusters = []\n",
    "for cluster in clusters:\n",
    "    # Primero subdividimos por θ\n",
    "    theta_subclusters = subdivide_by_theta_gap(cluster, gap_threshold=2.5)\n",
    "    for subc in theta_subclusters:\n",
    "        # Luego, subdividimos cada resultado por r\n",
    "        r_subclusters = subdivide_by_r_gap(subc, gap_threshold=2.5)\n",
    "        final_clusters.extend(r_subclusters)\n",
    "\n",
    "# 9. Visualizar el resultado final\n",
    "plot_clusters(final_clusters, title=\"Segmentación final con BIRCH + subdivisión por gap (umbral 2.5)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab5d845d-f7c9-409a-a40e-49090de8ff63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ba18662-d264-4ac2-b623-518f8750f74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dtheta = abs(theta_arr[i] - theta_arr[j])\n",
    "            dr = abs(r_arr[i] - r_arr[j])\n",
    "            # Vecindad más estricta\n",
    "            if dtheta <= theta_diff and dr <= r_diff:\n",
    "                graph[i].append(j)\n",
    "                graph[j].append(i)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            cluster_df.reset_index(drop=True, inplace=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    if mode=='theta':\n",
    "        df_cluster = df_cluster.sort_values(by='theta').reset_index(drop=True)\n",
    "        arr = df_cluster['theta'].values\n",
    "    else:\n",
    "        df_cluster = df_cluster.sort_values(by='r').reset_index(drop=True)\n",
    "        arr = df_cluster['r'].values\n",
    "    \n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff>gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    # Parámetros de cercanía\n",
    "    theta_diff=5.0,   # Ajusta para ser más o menos estricto\n",
    "    r_diff=1.0,       # Ajusta para ser más o menos estricto\n",
    "    # Subdivisión\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta']<0, df['theta']+360, df['theta'])\n",
    "    df = df[(df['theta']>=theta_min)&(df['theta']<=theta_max)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp']>threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Contruir grafo rectangular\n",
    "    graph, n = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    # BFS\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        # Subdividir por θ\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            # Subdividir por r\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        c = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=c, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Ajusta los parámetros para forzar mayor segmentación\n",
    "    final_clusters = process_clusters_rectangular_gaps(\n",
    "        id_halo=\"17\",\n",
    "        quartile_threshold=0.55,\n",
    "        theta_min=50,\n",
    "        theta_max=250,\n",
    "        theta_diff=3.0,     # Vecindad horizontal\n",
    "        r_diff=0.68,         # Vecindad radial\n",
    "        gap_threshold_theta=1.9,\n",
    "        gap_threshold_r=1.23\n",
    "    )\n",
    "    print(f\"Se formaron {len(final_clusters)} grupos finales.\")\n",
    "    plot_clusters_rectangular_gaps(final_clusters, \"Clusters rectangulares + subdivisión por gap\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbc365a-991b-4c7c-bea9-3e6b55c4e04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Parte 1: Segmentación inicial con vecinos rectangulares\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dtheta = abs(theta_arr[i] - theta_arr[j])\n",
    "            dr = abs(r_arr[i] - r_arr[j])\n",
    "            # Vecindad más estricta\n",
    "            if dtheta <= theta_diff and dr <= r_diff:\n",
    "                graph[i].append(j)\n",
    "                graph[j].append(i)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            cluster_df.reset_index(drop=True, inplace=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    if mode=='theta':\n",
    "        df_cluster = df_cluster.sort_values(by='theta').reset_index(drop=True)\n",
    "        arr = df_cluster['theta'].values\n",
    "    else:\n",
    "        df_cluster = df_cluster.sort_values(by='r').reset_index(drop=True)\n",
    "        arr = df_cluster['r'].values\n",
    "    \n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    # Parámetros de cercanía\n",
    "    theta_diff=5.0,   # Ajusta para ser más o menos estricto\n",
    "    r_diff=1.0,       # Ajusta para ser más o menos estricto\n",
    "    # Subdivisión\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Construir grafo rectangular y obtener clusters (grupos semilla)\n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        # Subdividir por theta\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            # Subdividir por r\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        c = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=c, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Parte 2: Proceso iterativo (similar a process_spiral_arms_interactive)\n",
    "# utilizando los grupos semilla de la segmentación rectangular\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_with_seeds(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    # Parámetros para la segmentación rectangular\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,     # Vecindad horizontal para seeds\n",
    "    r_diff=0.68,        # Vecindad radial para seeds\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    # Parámetros para la asignación iterativa\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    # 1. Cargar datos y calcular coordenadas polares\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # 2. Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Obtener grupos semilla mediante segmentación rectangular\n",
    "    seeds_all, df_seed_source = process_clusters_rectangular_gaps(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    \n",
    "    # Filtrar solo los grupos semilla con más de 60 puntos\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) > 60]\n",
    "    print(f\"Grupos semilla con más de 60 puntos: {len(seed_clusters)}\")\n",
    "    \n",
    "    # Para asegurarnos de que usamos los mismos puntos que en df_filtered,\n",
    "    # marcamos un identificador en cada punto que pertenezca a un seed\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        # Se asume que el índice de df_filtered es el mismo que el de df_seed_source\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 4. Inicializar grupos usando los seeds (regresión lineal)\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "    \n",
    "    def calculate_pa(slope, intercept):\n",
    "        if intercept != 0:\n",
    "            return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "        return np.nan\n",
    "    \n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "    \n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        # Solo se aplica regresión si hay suficientes puntos\n",
    "        if len(seed) >= 3:\n",
    "            X_ = seed[['theta']].values\n",
    "            y_ = seed['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'seed_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # 5. Preparar puntos restantes para asignación iterativa:\n",
    "    # Se consideran los puntos de df_filtered que NO fueron asignados a seeds.\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    discarded_points = []\n",
    "    states = []  # estados intermedios para visualización\n",
    "    iteration = 0\n",
    "    \n",
    "    # 6. Asignación iterativa de puntos\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "        \n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                # Se verifica que todos los puntos del grupo estén cercanos\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) <= slope_variation_threshold:\n",
    "                        # Actualizar grupo con el nuevo punto\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            groups_copy = copy.deepcopy(groups)\n",
    "            discarded_copy = copy.deepcopy(discarded_points)\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': groups_copy,\n",
    "                'discarded': discarded_copy,\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 7. Reasignar puntos descartados (etapa final)\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                        X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                        y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                        model.fit(X_new, y_new)\n",
    "                        new_slope = model.coef_[0]\n",
    "                        old_slope = g['slope']\n",
    "                        if abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) <= slope_variation_threshold:\n",
    "                            g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                            g['slope'] = new_slope\n",
    "                            g['intercept'] = model.intercept_\n",
    "                            g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                            assigned = True\n",
    "                            break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'seed_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "    \n",
    "    # 8. (Opcional) Subdividir grupos si existe un gran gap en theta\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'seed_id': group['seed_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'seed_id': group['seed_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    # 9. Filtrar grupos por PA válido\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    # Agregar estado final\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "def plot_intermediate_state(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica el estado intermedio en el plano polar:\n",
    "      - Fondo: todos los datos en gris pálido.\n",
    "      - Grupos asignados: cada grupo con un color y su línea de ajuste (si se ha calculado).\n",
    "      - Puntos descartados: marcados con una 'x' negra.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Fondo: todos los datos\n",
    "    theta_all = np.radians(df_all['theta'])\n",
    "    r_all = df_all['r']\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        theta_vals = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    \n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        theta_desc = np.radians(df_desc['theta'])\n",
    "        r_desc = df_desc['r']\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Ejecución interactiva e impresión de resultados\n",
    "# ========================\n",
    "\n",
    "states, final_groups, df_filtered = process_spiral_arms_with_seeds(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.68,\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    ")\n",
    "\n",
    "print(\"Total de estados guardados:\", len(states))\n",
    "print(\"Total de grupos finales:\", len(final_groups))\n",
    "\n",
    "# Widget interactivo para ver el proceso iterativo\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afbd5774-3661-4495-aa5e-3acf69c18862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Parte 1: Segmentación inicial con vecinos rectangulares\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dtheta = abs(theta_arr[i] - theta_arr[j])\n",
    "            dr = abs(r_arr[i] - r_arr[j])\n",
    "            # Vecindad más estricta\n",
    "            if dtheta <= theta_diff and dr <= r_diff:\n",
    "                graph[i].append(j)\n",
    "                graph[j].append(i)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            cluster_df.reset_index(drop=True, inplace=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    if mode=='theta':\n",
    "        df_cluster = df_cluster.sort_values(by='theta').reset_index(drop=True)\n",
    "        arr = df_cluster['theta'].values\n",
    "    else:\n",
    "        df_cluster = df_cluster.sort_values(by='r').reset_index(drop=True)\n",
    "        arr = df_cluster['r'].values\n",
    "    \n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    # Parámetros de cercanía\n",
    "    theta_diff=5.0,   # Ajusta para ser más o menos estricto\n",
    "    r_diff=1.0,       # Ajusta para ser más o menos estricto\n",
    "    # Subdivisión\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Construir grafo rectangular y obtener clusters (grupos semilla)\n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        # Subdividir por theta\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            # Subdividir por r\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        c = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=c, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Parte 2: Proceso iterativo (similar a process_spiral_arms_interactive)\n",
    "# utilizando los grupos semilla de la segmentación rectangular\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_with_seeds(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    # Parámetros para la segmentación rectangular\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,     # Vecindad horizontal para seeds\n",
    "    r_diff=0.68,        # Vecindad radial para seeds\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    # Parámetros para la asignación iterativa\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    # 1. Cargar datos y calcular coordenadas polares\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # 2. Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Obtener grupos semilla mediante segmentación rectangular\n",
    "    seeds_all, df_seed_source = process_clusters_rectangular_gaps(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    \n",
    "    # Filtrar solo los grupos semilla con más de 60 puntos\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) > 60]\n",
    "    print(f\"Grupos semilla con más de 60 puntos: {len(seed_clusters)}\")\n",
    "    \n",
    "    # Para asegurarnos de que usamos los mismos puntos que en df_filtered,\n",
    "    # marcamos un identificador en cada punto que pertenezca a un seed\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        # Se asume que el índice de df_filtered es el mismo que el de df_seed_source\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 4. Inicializar grupos usando los seeds (regresión lineal)\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "    \n",
    "    def calculate_pa(slope, intercept):\n",
    "        if intercept != 0:\n",
    "            return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "        return np.nan\n",
    "    \n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "    \n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        # Solo se aplica regresión si hay suficientes puntos\n",
    "        if len(seed) >= 3:\n",
    "            X_ = seed[['theta']].values\n",
    "            y_ = seed['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'seed_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # 5. Preparar puntos restantes para asignación iterativa:\n",
    "    # Se consideran los puntos de df_filtered que NO fueron asignados a seeds.\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    discarded_points = []\n",
    "    states = []  # estados intermedios para visualización\n",
    "    iteration = 0\n",
    "    \n",
    "    # 6. Asignación iterativa de puntos\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "        \n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                # Se verifica que todos los puntos del grupo estén cercanos\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) <= slope_variation_threshold:\n",
    "                        # Actualizar grupo con el nuevo punto\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            groups_copy = copy.deepcopy(groups)\n",
    "            discarded_copy = copy.deepcopy(discarded_points)\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': groups_copy,\n",
    "                'discarded': discarded_copy,\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 7. Reasignar puntos descartados (etapa final)\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                        X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                        y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                        model.fit(X_new, y_new)\n",
    "                        new_slope = model.coef_[0]\n",
    "                        old_slope = g['slope']\n",
    "                        if abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) <= slope_variation_threshold:\n",
    "                            g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                            g['slope'] = new_slope\n",
    "                            g['intercept'] = model.intercept_\n",
    "                            g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                            assigned = True\n",
    "                            break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'seed_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "    \n",
    "    # 8. (Opcional) Subdividir grupos si existe un gran gap en theta\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'seed_id': group['seed_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'seed_id': group['seed_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    # 9. Filtrar grupos por PA válido\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    # Agregar estado final\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "def plot_intermediate_state(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica el estado intermedio en el plano polar:\n",
    "      - Fondo: todos los datos en gris pálido.\n",
    "      - Grupos asignados: cada grupo con un color y su línea de ajuste (si se ha calculado).\n",
    "      - Puntos descartados: marcados con una 'x' negra.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Fondo: todos los datos\n",
    "    theta_all = np.radians(df_all['theta'])\n",
    "    r_all = df_all['r']\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        theta_vals = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    \n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        theta_desc = np.radians(df_desc['theta'])\n",
    "        r_desc = df_desc['r']\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Ejecución interactiva e impresión de resultados\n",
    "# ========================\n",
    "\n",
    "states, final_groups, df_filtered = process_spiral_arms_with_seeds(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.68,\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 40.0),\n",
    "    slope_variation_threshold=0.18,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    ")\n",
    "\n",
    "print(\"Total de estados guardados:\", len(states))\n",
    "print(\"Total de grupos finales:\", len(final_groups))\n",
    "\n",
    "# Widget interactivo para ver el proceso iterativo\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42a10ae-f73b-49fb-b025-5aec29f0e966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Parte 1: Segmentación inicial con vecinos rectangulares\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dtheta = abs(theta_arr[i] - theta_arr[j])\n",
    "            dr = abs(r_arr[i] - r_arr[j])\n",
    "            # Se conecta si las diferencias son menores o iguales a los umbrales\n",
    "            if dtheta <= theta_diff and dr <= r_diff:\n",
    "                graph[i].append(j)\n",
    "                graph[j].append(i)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            cluster_df.reset_index(drop=True, inplace=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    # Ordena según el valor de theta o r y subdivide si hay un salto mayor al gap_threshold\n",
    "    if mode == 'theta':\n",
    "        df_cluster = df_cluster.sort_values(by='theta').reset_index(drop=True)\n",
    "        arr = df_cluster['theta'].values\n",
    "    else:\n",
    "        df_cluster = df_cluster.sort_values(by='r').reset_index(drop=True)\n",
    "        arr = df_cluster['r'].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Construcción del grafo y obtención de clusters (grupos semilla)\n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        # Subdividir primero por theta y luego por r\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        c = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=c, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Parte 2: Proceso iterativo utilizando los grupos semilla\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_with_seeds(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,     # Vecindad horizontal para seeds\n",
    "    r_diff=0.68,        # Vecindad radial para seeds\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    # 1. Cargar datos y convertir a coordenadas polares\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # 2. Filtrar según el percentil de 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Obtener grupos semilla con segmentación rectangular\n",
    "    seeds_all, df_seed_source = process_clusters_rectangular_gaps(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    # Se filtran aquellos grupos con más de 60 puntos\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) > 60]\n",
    "    print(f\"Grupos semilla con más de 60 puntos: {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a un seed\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 4. Inicialización de grupos mediante regresión lineal\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "    \n",
    "    def calculate_pa(slope, intercept):\n",
    "        if intercept != 0:\n",
    "            return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "        return np.nan\n",
    "    \n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "    \n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 3:\n",
    "            X_ = seed[['theta']].values\n",
    "            y_ = seed['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'seed_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # 5. Preparar puntos restantes (los que no están en los seeds) para asignación iterativa\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    discarded_points = []\n",
    "    states = []  # Para guardar estados intermedios\n",
    "    iteration = 0\n",
    "    \n",
    "    # 6. Asignación iterativa de puntos\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "        \n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) <= slope_variation_threshold:\n",
    "                        # Actualiza el grupo con el nuevo punto\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            groups_copy = copy.deepcopy(groups)\n",
    "            discarded_copy = copy.deepcopy(discarded_points)\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': groups_copy,\n",
    "                'discarded': discarded_copy,\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 7. Reasignar los puntos descartados en una etapa final\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                        X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                        y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                        model.fit(X_new, y_new)\n",
    "                        new_slope = model.coef_[0]\n",
    "                        old_slope = g['slope']\n",
    "                        if abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) <= slope_variation_threshold:\n",
    "                            g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                            g['slope'] = new_slope\n",
    "                            g['intercept'] = model.intercept_\n",
    "                            g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                            assigned = True\n",
    "                            break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'seed_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "    \n",
    "    # 8. (Opcional) Subdividir grupos si existe un gap grande en theta\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'seed_id': group['seed_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'seed_id': group['seed_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    # 9. Filtrar grupos que cumplan la condición de PA válida\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and is_valid_pa(pa_val, flexible_pa_range):\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "def plot_intermediate_state(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica el estado intermedio en el plano polar:\n",
    "      - Fondo con todos los datos en gris pálido.\n",
    "      - Los grupos asignados se muestran en colores y, si es posible, se traza la línea de ajuste.\n",
    "      - Los puntos descartados se marcan con una 'x' negra.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    theta_all = np.radians(df_all['theta'])\n",
    "    r_all = df_all['r']\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        theta_vals = np.radians(pts['theta'])\n",
    "        r_vals = pts['r']\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    \n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        theta_desc = np.radians(df_desc['theta'])\n",
    "        r_desc = df_desc['r']\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Ejecución interactiva e impresión de resultados\n",
    "# ========================\n",
    "\n",
    "states, final_groups, df_filtered = process_spiral_arms_with_seeds(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.68,\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 40.0),\n",
    "    slope_variation_threshold=0.18,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    ")\n",
    "\n",
    "print(\"Total de estados guardados:\", len(states))\n",
    "print(\"Total de grupos finales:\", len(final_groups))\n",
    "\n",
    "# Widget interactivo para ver el proceso iterativo\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7916e327-eff3-4814-9130-c83aff551419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Parte 1: Segmentación inicial con vecinos rectangulares\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dtheta = abs(theta_arr[i] - theta_arr[j])\n",
    "            dr = abs(r_arr[i] - r_arr[j])\n",
    "            # Conecta si las diferencias son menores o iguales a los umbrales\n",
    "            if dtheta <= theta_diff and dr <= r_diff:\n",
    "                graph[i].append(j)\n",
    "                graph[j].append(i)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            cluster_df.reset_index(drop=True, inplace=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    # Ordena según el valor de theta o r y subdivide si hay un gap mayor al umbral\n",
    "    if mode == 'theta':\n",
    "        df_cluster = df_cluster.sort_values(by='theta').reset_index(drop=True)\n",
    "        arr = df_cluster['theta'].values\n",
    "    else:\n",
    "        df_cluster = df_cluster.sort_values(by='r').reset_index(drop=True)\n",
    "        arr = df_cluster['r'].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Construir el grafo y obtener clusters (grupos semilla)\n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        # Subdividir por theta y luego por r\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        c = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=c, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Función para subdividir un grupo si hay gap en θ o en r\n",
    "# ========================\n",
    "\n",
    "def subdivide_if_gap(group, max_gap_theta, max_gap_r):\n",
    "    \"\"\"\n",
    "    Subdivide el grupo si existe un gap en θ o en r.\n",
    "    Primero se ordena por θ y se subdivide si hay un gap mayor a max_gap_theta.\n",
    "    Luego, para cada subgrupo se ordena por r y se subdivide si hay un gap mayor a max_gap_r.\n",
    "    Se reestima la línea en cada subgrupo (si hay al menos 2 puntos).\n",
    "    \"\"\"\n",
    "    # Subdivisión por theta\n",
    "    pts_theta = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    dtheta = np.diff(pts_theta['theta'].values)\n",
    "    if not np.any(dtheta > max_gap_theta):\n",
    "        subgroups_theta = [pts_theta]\n",
    "    else:\n",
    "        subgroups_theta = subdivide_by_gap(pts_theta, gap_threshold=max_gap_theta, mode='theta')\n",
    "    \n",
    "    final_subgroups = []\n",
    "    # Para cada subgrupo obtenido, subdividir por r\n",
    "    for sub in subgroups_theta:\n",
    "        sub_sorted_r = sub.sort_values(by='r').reset_index(drop=True)\n",
    "        dr = np.diff(sub_sorted_r['r'].values)\n",
    "        if not np.any(dr > max_gap_r):\n",
    "            subgroups_r = [sub_sorted_r]\n",
    "        else:\n",
    "            subgroups_r = subdivide_by_gap(sub_sorted_r, gap_threshold=max_gap_r, mode='r')\n",
    "        # Recalcular el ajuste lineal para cada subdivisión\n",
    "        for sg in subgroups_r:\n",
    "            if len(sg) >= 2:\n",
    "                X_ = sg[['theta']].values\n",
    "                y_ = sg['r'].values\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_, y_)\n",
    "                s_ = model.coef_[0]\n",
    "                i_ = model.intercept_\n",
    "                pa_ = np.degrees(np.arctan((s_ * (180/np.pi)) / i_)) if i_ != 0 else np.nan\n",
    "            else:\n",
    "                s_, i_, pa_ = None, None, None\n",
    "            new_group = {\n",
    "                'seed_id': group.get('seed_id', None),\n",
    "                'points': sg,\n",
    "                'slope': s_,\n",
    "                'intercept': i_,\n",
    "                'pa': pa_\n",
    "            }\n",
    "            final_subgroups.append(new_group)\n",
    "    return final_subgroups\n",
    "\n",
    "# ========================\n",
    "# Parte 2: Proceso iterativo utilizando los grupos semilla\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_with_seeds(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,     # Vecindad horizontal para seeds\n",
    "    r_diff=0.68,        # Vecindad radial para seeds\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    max_r_gap=5.0,    # Umbral para gap en r en la subdivisión\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    # 1. Cargar datos y convertir a coordenadas polares\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # 2. Filtrar según percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Obtener grupos semilla mediante segmentación rectangular\n",
    "    seeds_all, df_seed_source = process_clusters_rectangular_gaps(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    # Filtrar solo grupos con más de 60 puntos\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) > 60]\n",
    "    print(f\"Grupos semilla con más de 60 puntos: {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a un seed\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 4. Inicialización de grupos mediante regresión lineal\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    def calculate_distance(slope, intercept, theta, r):\n",
    "        pred_r = slope * theta + intercept\n",
    "        return abs(pred_r - r)\n",
    "    \n",
    "    def calculate_pa(slope, intercept):\n",
    "        if intercept != 0:\n",
    "            return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "        return np.nan\n",
    "    \n",
    "    def is_valid_pa(pa, valid_range):\n",
    "        return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "    \n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 3:\n",
    "            X_ = seed[['theta']].values\n",
    "            y_ = seed['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'seed_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # 5. Preparar puntos restantes (no asignados a seeds)\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    discarded_points = []\n",
    "    states = []  # Para estados intermedios\n",
    "    iteration = 0\n",
    "    \n",
    "    # 6. Asignación iterativa de puntos\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        assigned = False\n",
    "        \n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "            if dist <= radius_tolerance:\n",
    "                distances = g['points'].apply(\n",
    "                    lambda row: calculate_distance(g['slope'], g['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if all(d <= point_distance_tolerance for d in distances):\n",
    "                    X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                    y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                    model.fit(X_new, y_new)\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = g['slope']\n",
    "                    if abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) <= slope_variation_threshold:\n",
    "                        # Actualizar grupo con el nuevo punto\n",
    "                        g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                        g['slope'] = new_slope\n",
    "                        g['intercept'] = model.intercept_\n",
    "                        g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                        assigned = True\n",
    "                        break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            groups_copy = copy.deepcopy(groups)\n",
    "            discarded_copy = copy.deepcopy(discarded_points)\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': groups_copy,\n",
    "                'discarded': discarded_copy,\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 7. Reasignar puntos descartados (etapa final)\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            theta_pt = p['theta']\n",
    "            r_pt = p['r']\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                dist = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if g['pa'] is not None and is_valid_pa(g['pa'], flexible_pa_range):\n",
    "                        X_new = np.vstack([g['points'][['theta']].values, [[theta_pt]]])\n",
    "                        y_new = np.hstack([g['points']['r'].values, r_pt])\n",
    "                        model.fit(X_new, y_new)\n",
    "                        new_slope = model.coef_[0]\n",
    "                        old_slope = g['slope']\n",
    "                        if abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) <= slope_variation_threshold:\n",
    "                            g['points'] = pd.concat([g['points'], p.to_frame().T], ignore_index=True)\n",
    "                            g['slope'] = new_slope\n",
    "                            g['intercept'] = model.intercept_\n",
    "                            g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                            assigned = True\n",
    "                            break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new, i_new)\n",
    "                if is_valid_pa(pa_new, flexible_pa_range):\n",
    "                    groups.append({\n",
    "                        'seed_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "    \n",
    "    # 8. Subdividir grupos si existe un gap grande en θ o en r\n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap_theta=max_theta_gap, max_gap_r=max_r_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    # 9. Filtrar grupos con PA válida\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "def plot_intermediate_state(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica el estado intermedio en el plano polar:\n",
    "      - Fondo: todos los datos en gris pálido.\n",
    "      - Grupos asignados: cada grupo con un color y su línea de ajuste (si se ha calculado).\n",
    "      - Puntos descartados: marcados con una 'x' negra.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    theta_all = np.radians(df_all['theta'].values)\n",
    "    r_all = df_all['r']\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        theta_vals = np.radians(pts['theta'].values)\n",
    "        r_vals = pts['r']\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    \n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        theta_desc = np.radians(df_desc['theta'].values)\n",
    "        r_desc = df_desc['r']\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Ejecución interactiva e impresión de resultados\n",
    "# ========================\n",
    "\n",
    "states, final_groups, df_filtered = process_spiral_arms_with_seeds(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.68,\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 40.0),\n",
    "    slope_variation_threshold=0.18,\n",
    "    max_theta_gap=15.0,\n",
    "    max_r_gap=5.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    ")\n",
    "\n",
    "print(\"Total de estados guardados:\", len(states))\n",
    "print(\"Total de grupos finales:\", len(final_groups))\n",
    "\n",
    "# Widget interactivo para ver el proceso iterativo\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c14ca2f8-3a63-4e6b-b125-5a09ba2bbe32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "411541fc-01f2-42f3-8320-77a8a0782b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "**Resumen del Proceso y Código Integrado:**\n",
    "\n",
    "1. **Segmentación Inicial con Vecinos Rectangulares:**  \n",
    "   - Se construyó un grafo en el que cada punto se conecta a otros si la diferencia en θ y r es menor o igual a unos umbrales predefinidos.  \n",
    "   - Se aplicó una búsqueda en anchura (BFS) para extraer clusters de puntos (los grupos semilla).  \n",
    "   - Se subdividieron estos clusters usando una función `subdivide_by_gap` que detecta “gaps” en los valores de θ o r.  \n",
    "   - Finalmente, se filtran los grupos semilla para conservar solo aquellos con más de 60 puntos.\n",
    "\n",
    "2. **Proceso Iterativo de Asignación y Validación:**  \n",
    "   - Se cargaron los datos y se calcularon las coordenadas polares, filtrando los puntos por percentil en la columna `rho_resta_final_exp`.  \n",
    "   - Se inicializaron los grupos semilla aplicando una regresión lineal para obtener los parámetros de ajuste (pendiente, intersección y un parámetro adicional denominado *pa*).  \n",
    "   - Se asignaron de forma iterativa los puntos restantes a los grupos existentes, validando que la actualización de la pendiente no varíe demasiado (usando un umbral para la variación).  \n",
    "   - Los puntos que no cumplían los criterios se almacenaron como \"descartados\" y se reevalúan al final para asignarlos (o crear nuevos grupos).\n",
    "\n",
    "3. **Subdivisión Adicional en Caso de Gaps en θ o r:**  \n",
    "   - Se implementó una función `subdivide_if_gap` que primero ordena los puntos por θ y subdivide si se detecta un gap mayor a un umbral (`max_theta_gap`).\n",
    "   - Luego, cada subgrupo se ordena por r y se subdivide si hay un gap mayor a otro umbral (`max_r_gap`).\n",
    "   - En cada subgrupo resultante se recalcula el ajuste lineal para actualizar los parámetros.\n",
    "\n",
    "4. **Visualización con Widgets Interactivos:**  \n",
    "   - Se utiliza la biblioteca `ipywidgets` y la función `interact` para crear un slider que permite recorrer los diferentes estados intermedios del proceso de asignación y visualizarlos.  \n",
    "   - Se corrigió el error en la función de graficación forzando la conversión de las columnas de θ a arrays de NumPy (usando `.values`) para evitar que se intente aplicar `np.radians` sobre un valor escalar.\n",
    "\n",
    "---\n",
    "\n",
    "**Funcionamiento General:**\n",
    "\n",
    "- El código integra el proceso de segmentación inicial para generar grupos semilla y luego utiliza esos grupos para iniciar un proceso iterativo en el que se asignan y validan puntos.  \n",
    "- Durante la asignación se verifica que los puntos se encuentren dentro de una tolerancia respecto a la línea de ajuste, manteniendo la consistencia de la pendiente.  \n",
    "- Finalmente, se permite una subdivisión adicional de los grupos en caso de que se detecten gaps grandes en las variables θ o r, y se visualizan los resultados interactivamente con un widget.\n",
    "\n",
    "Este resumen puede servirte como referencia para estructurar futuros prompts o para recordar el flujo general y las modificaciones realizadas en el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d224f71-a856-44b0-8d61-141e850d0418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones de Preprocesamiento\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula coordenadas polares y filtra puntos según el rango de θ.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    return df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "\n",
    "def calculate_pa(slope):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) correspondiente a la pendiente usando la función arctan.\n",
    "    Si la pendiente es None, retorna None.\n",
    "    \"\"\"\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Parte 1: Segmentación inicial con vecinos rectangulares\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo donde cada punto se conecta a otros si las diferencias en θ y r\n",
    "    son menores o iguales a los umbrales especificados. Se utiliza una aproximación\n",
    "    vectorizada para cada punto.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Excluir el mismo índice\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda en anchura (BFS) sobre el grafo para extraer componentes conexas.\n",
    "    Cada componente se retorna como un DataFrame de puntos.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide en subclusters si se detecta un gap mayor\n",
    "    que el umbral especificado.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Carga y filtra los datos, construye el grafo, extrae componentes (clusters) y los subdivide\n",
    "    en función de gaps en θ y r.\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    \"\"\"\n",
    "    Genera una visualización de los clusters obtenidos.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Función para subdividir un grupo si hay gap en θ o en r\n",
    "# ========================\n",
    "\n",
    "def subdivide_if_gap(group, max_gap_theta, max_gap_r):\n",
    "    \"\"\"\n",
    "    Subdivide el grupo si existe un gap en θ o en r.\n",
    "    Primero se ordena por θ y se subdivide si hay un gap mayor a max_gap_theta.\n",
    "    Luego, para cada subgrupo se ordena por r y se subdivide si hay un gap mayor a max_gap_r.\n",
    "    Se reestima el ajuste lineal para cada subdivisión (si hay al menos 2 puntos).\n",
    "    \"\"\"\n",
    "    # Verificar que 'points' sea un DataFrame\n",
    "    if not isinstance(group.get('points', None), pd.DataFrame):\n",
    "        print(\"Error en subdivide_if_gap: group['points'] no es un DataFrame.\")\n",
    "        print(\"Valor recibido:\", group.get('points'))\n",
    "        raise ValueError(f\"Se esperaba un DataFrame en 'points', pero se obtuvo {type(group.get('points', None))}\")\n",
    "    \n",
    "    pts_theta = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    dtheta = np.diff(pts_theta['theta'].values)\n",
    "    if not np.any(dtheta > max_gap_theta):\n",
    "        subgroups_theta = [pts_theta]\n",
    "    else:\n",
    "        subgroups_theta = subdivide_by_gap(pts_theta, gap_threshold=max_gap_theta, mode='theta')\n",
    "    \n",
    "    final_subgroups = []\n",
    "    for sub in subgroups_theta:\n",
    "        sub_sorted_r = sub.sort_values(by='r').reset_index(drop=True)\n",
    "        dr = np.diff(sub_sorted_r['r'].values)\n",
    "        if not np.any(dr > max_gap_r):\n",
    "            subgroups_r = [sub_sorted_r]\n",
    "        else:\n",
    "            subgroups_r = subdivide_by_gap(sub_sorted_r, gap_threshold=max_gap_r, mode='r')\n",
    "        for sg in subgroups_r:\n",
    "            if len(sg) >= 2:\n",
    "                X_ = sg[['theta']].values\n",
    "                y_ = sg['r'].values\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_, y_)\n",
    "                s_ = model.coef_[0]\n",
    "                i_ = model.intercept_\n",
    "                pa_ = calculate_pa(s_)\n",
    "            else:\n",
    "                s_, i_, pa_ = None, None, None\n",
    "            new_group = {\n",
    "                'seed_id': group.get('seed_id', None),\n",
    "                'points': sg,\n",
    "                'slope': s_,\n",
    "                'intercept': i_,\n",
    "                'pa': pa_\n",
    "            }\n",
    "            final_subgroups.append(new_group)\n",
    "    return final_subgroups\n",
    "\n",
    "# ========================\n",
    "# Función auxiliar para asignar un punto a un grupo (utilizada en el proceso iterativo)\n",
    "# ========================\n",
    "\n",
    "def try_assign_point(point, group, model, radius_tolerance, point_distance_tolerance, slope_variation_threshold):\n",
    "    \"\"\"\n",
    "    Intenta asignar 'point' al 'group' si cumple con la tolerancia de distancia al ajuste actual.\n",
    "    Actualiza la línea de ajuste y retorna True si la asignación es exitosa.\n",
    "    \"\"\"\n",
    "    theta_pt, r_pt = point['theta'], point['r']\n",
    "    pred_r = group['slope'] * theta_pt + group['intercept']\n",
    "    dist = abs(pred_r - r_pt)\n",
    "    if dist > radius_tolerance:\n",
    "        return False\n",
    "    \n",
    "    distances = group['points'].apply(\n",
    "        lambda row: abs(group['slope'] * row['theta'] + group['intercept'] - row['r']),\n",
    "        axis=1\n",
    "    )\n",
    "    if not all(d <= point_distance_tolerance for d in distances):\n",
    "        return False\n",
    "    \n",
    "    X_new = np.vstack([group['points'][['theta']].values, [[theta_pt]]])\n",
    "    y_new = np.hstack([group['points']['r'].values, r_pt])\n",
    "    model.fit(X_new, y_new)\n",
    "    new_slope = model.coef_[0]\n",
    "    if abs(new_slope - group['slope']) / (abs(group['slope']) + 1e-12) > slope_variation_threshold:\n",
    "        return False\n",
    "    \n",
    "    group['points'] = pd.concat([group['points'], point.to_frame().T], ignore_index=True)\n",
    "    group['slope'] = new_slope\n",
    "    group['intercept'] = model.intercept_\n",
    "    group['pa'] = calculate_pa(new_slope)\n",
    "    return True\n",
    "\n",
    "# ========================\n",
    "# Parte 2: Proceso iterativo utilizando los grupos semilla\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_with_seeds(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,     # Vecindad horizontal para seeds\n",
    "    r_diff=0.68,        # Vecindad radial para seeds\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    max_r_gap=5.0,    # Umbral para gap en r en la subdivisión\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso iterativo de asignación de puntos a grupos (espirales) usando seeds iniciales.\n",
    "    Incluye reasignación de puntos descartados y subdivisión adicional si se detectan gaps.\n",
    "    Retorna los estados intermedios, los grupos finales válidos y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    # 1. Cargar y filtrar datos\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    \n",
    "    # 2. Filtrar según percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Obtener grupos semilla mediante segmentación rectangular\n",
    "    seeds_all, _ = process_clusters_rectangular_gaps(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    # Filtrar solo seeds con más de 60 puntos\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) > 60]\n",
    "    print(f\"Grupos semilla con más de 60 puntos: {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a un seed\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 4. Inicialización de grupos mediante regresión lineal\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 3:\n",
    "            X_ = seed[['theta']].values\n",
    "            y_ = seed['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'seed_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # 5. Preparar puntos restantes (no asignados a seeds)\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    discarded_points = []\n",
    "    states = []  # Para guardar estados intermedios\n",
    "    iteration = 0\n",
    "    \n",
    "    # 6. Asignación iterativa de puntos\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        assigned = False\n",
    "        \n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            if try_assign_point(point, g, model, radius_tolerance, point_distance_tolerance, slope_variation_threshold):\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'discarded': copy.deepcopy(discarded_points),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 7. Reasignar puntos descartados (etapa final)\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                if try_assign_point(p, g, model, radius_tolerance, point_distance_tolerance, slope_variation_threshold):\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new)\n",
    "                if flexible_pa_range[0] <= abs(pa_new) <= flexible_pa_range[1]:\n",
    "                    groups.append({\n",
    "                        'seed_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "    \n",
    "    # 8. Subdividir grupos si existe un gap grande en θ o en r\n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap_theta=max_theta_gap, max_gap_r=max_r_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    # 9. Filtrar grupos con PA válida\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "    \n",
    "def plot_intermediate_state(state, df_all):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Fondo de todos los datos\n",
    "    df_all['theta'] = pd.to_numeric(df_all['theta'], errors='coerce')\n",
    "    theta_all = np.radians(df_all['theta'].values)\n",
    "    r_all = df_all['r'].values\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points'].copy()\n",
    "\n",
    "        # Verificar que la columna 'theta' sea numérica\n",
    "        pts['theta'] = pd.to_numeric(pts['theta'], errors='coerce')\n",
    "        # Si hay nulos, imprimirlos\n",
    "        if pts['theta'].isna().any():\n",
    "            print(f\"Hay valores NaN en 'theta' para el grupo {idx}. Filas:\")\n",
    "            print(pts[pts['theta'].isna()])\n",
    "\n",
    "        theta_vals = np.radians(pts['theta'].values)\n",
    "        r_vals = pts['r'].values\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "\n",
    "    # Puntos descartados\n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        df_desc['theta'] = pd.to_numeric(df_desc['theta'], errors='coerce')\n",
    "        theta_desc = np.radians(df_desc['theta'].values)\n",
    "        r_desc = df_desc['r'].values\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Ejecución interactiva e impresión de resultados\n",
    "# ========================\n",
    "\n",
    "states, final_groups, df_filtered = process_spiral_arms_with_seeds(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.68,\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 40.0),\n",
    "    slope_variation_threshold=0.18,\n",
    "    max_theta_gap=15.0,\n",
    "    max_r_gap=5.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    ")\n",
    "\n",
    "print(f\"Total de estados guardados: {len(states)}\")\n",
    "print(f\"Total de grupos finales: {len(final_groups)}\")\n",
    "\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965f40ca-d062-42b7-bfe8-87acc62595c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones de Preprocesamiento\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula coordenadas polares y filtra puntos según el rango de θ.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    return df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "\n",
    "def calculate_pa(slope):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) correspondiente a la pendiente usando la función arctan.\n",
    "    Si la pendiente es None, retorna None.\n",
    "    \"\"\"\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Parte 1: Segmentación inicial con vecinos rectangulares\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo donde cada punto se conecta a otros si las diferencias en θ y r\n",
    "    son menores o iguales a los umbrales especificados. Se utiliza una aproximación\n",
    "    vectorizada para cada punto.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Excluir el mismo índice\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda en anchura (BFS) sobre el grafo para extraer componentes conexas.\n",
    "    Cada componente se retorna como un DataFrame de puntos.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide en subclusters si se detecta un gap mayor\n",
    "    que el umbral especificado.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Carga y filtra los datos, construye el grafo, extrae componentes (clusters) y los subdivide\n",
    "    en función de gaps en θ y r.\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    \"\"\"\n",
    "    Genera una visualización de los clusters obtenidos.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Función para subdividir un grupo si hay gap en θ o en r\n",
    "# ========================\n",
    "\n",
    "def subdivide_if_gap(group, max_gap_theta, max_gap_r):\n",
    "    \"\"\"\n",
    "    Subdivide el grupo si existe un gap en θ o en r.\n",
    "    Primero se ordena por θ y se subdivide si hay un gap mayor a max_gap_theta.\n",
    "    Luego, para cada subgrupo se ordena por r y se subdivide si hay un gap mayor a max_gap_r.\n",
    "    Se reestima el ajuste lineal para cada subdivisión (si hay al menos 2 puntos).\n",
    "    \"\"\"\n",
    "    # Verificar que 'points' sea un DataFrame\n",
    "    if not isinstance(group.get('points', None), pd.DataFrame):\n",
    "        print(\"Error en subdivide_if_gap: group['points'] no es un DataFrame.\")\n",
    "        print(\"Valor recibido:\", group.get('points'))\n",
    "        raise ValueError(f\"Se esperaba un DataFrame en 'points', pero se obtuvo {type(group.get('points', None))}\")\n",
    "    \n",
    "    pts_theta = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    dtheta = np.diff(pts_theta['theta'].values)\n",
    "    if not np.any(dtheta > max_gap_theta):\n",
    "        subgroups_theta = [pts_theta]\n",
    "    else:\n",
    "        subgroups_theta = subdivide_by_gap(pts_theta, gap_threshold=max_gap_theta, mode='theta')\n",
    "    \n",
    "    final_subgroups = []\n",
    "    for sub in subgroups_theta:\n",
    "        sub_sorted_r = sub.sort_values(by='r').reset_index(drop=True)\n",
    "        dr = np.diff(sub_sorted_r['r'].values)\n",
    "        if not np.any(dr > max_gap_r):\n",
    "            subgroups_r = [sub_sorted_r]\n",
    "        else:\n",
    "            subgroups_r = subdivide_by_gap(sub_sorted_r, gap_threshold=max_gap_r, mode='r')\n",
    "        for sg in subgroups_r:\n",
    "            if len(sg) >= 2:\n",
    "                X_ = sg[['theta']].values\n",
    "                y_ = sg['r'].values\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_, y_)\n",
    "                s_ = model.coef_[0]\n",
    "                i_ = model.intercept_\n",
    "                pa_ = calculate_pa(s_)\n",
    "            else:\n",
    "                s_, i_, pa_ = None, None, None\n",
    "            new_group = {\n",
    "                'seed_id': group.get('seed_id', None),\n",
    "                'points': sg,\n",
    "                'slope': s_,\n",
    "                'intercept': i_,\n",
    "                'pa': pa_\n",
    "            }\n",
    "            final_subgroups.append(new_group)\n",
    "    return final_subgroups\n",
    "\n",
    "# ========================\n",
    "# Función auxiliar para asignar un punto a un grupo (utilizada en el proceso iterativo)\n",
    "# ========================\n",
    "\n",
    "def try_assign_point(point, group, model, radius_tolerance, point_distance_tolerance, slope_variation_threshold):\n",
    "    \"\"\"\n",
    "    Intenta asignar 'point' al 'group' si cumple con la tolerancia de distancia al ajuste actual.\n",
    "    Actualiza la línea de ajuste y retorna True si la asignación es exitosa.\n",
    "    \"\"\"\n",
    "    theta_pt, r_pt = point['theta'], point['r']\n",
    "    pred_r = group['slope'] * theta_pt + group['intercept']\n",
    "    dist = abs(pred_r - r_pt)\n",
    "    if dist > radius_tolerance:\n",
    "        return False\n",
    "    \n",
    "    distances = group['points'].apply(\n",
    "        lambda row: abs(group['slope'] * row['theta'] + group['intercept'] - row['r']),\n",
    "        axis=1\n",
    "    )\n",
    "    if not all(d <= point_distance_tolerance for d in distances):\n",
    "        return False\n",
    "    \n",
    "    X_new = np.vstack([group['points'][['theta']].values, [[theta_pt]]])\n",
    "    y_new = np.hstack([group['points']['r'].values, r_pt])\n",
    "    model.fit(X_new, y_new)\n",
    "    new_slope = model.coef_[0]\n",
    "    if abs(new_slope - group['slope']) / (abs(group['slope']) + 1e-12) > slope_variation_threshold:\n",
    "        return False\n",
    "    \n",
    "    group['points'] = pd.concat([group['points'], point.to_frame().T], ignore_index=True)\n",
    "    group['slope'] = new_slope\n",
    "    group['intercept'] = model.intercept_\n",
    "    group['pa'] = calculate_pa(new_slope)\n",
    "    return True\n",
    "\n",
    "# ========================\n",
    "# Parte 2: Proceso iterativo utilizando los grupos semilla\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_with_seeds(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,     # Vecindad horizontal para seeds\n",
    "    r_diff=0.68,        # Vecindad radial para seeds\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    max_r_gap=5.0,    # Umbral para gap en r en la subdivisión\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso iterativo de asignación de puntos a grupos (espirales) usando seeds iniciales.\n",
    "    Incluye reasignación de puntos descartados y subdivisión adicional si se detectan gaps.\n",
    "    Retorna los estados intermedios, los grupos finales válidos y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    # 1. Cargar y filtrar datos\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    \n",
    "    # 2. Filtrar según percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Obtener grupos semilla mediante segmentación rectangular\n",
    "    seeds_all, _ = process_clusters_rectangular_gaps(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    # Filtrar solo seeds con más de 60 puntos\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) > 60]\n",
    "    print(f\"Grupos semilla con más de 60 puntos: {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a un seed\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 4. Inicialización de grupos mediante regresión lineal\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 3:\n",
    "            X_ = seed[['theta']].values\n",
    "            y_ = seed['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'seed_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # 5. Preparar puntos restantes (no asignados a seeds)\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    # Importante: recorrer de izquierda a derecha (ordenar por θ)\n",
    "    df_local = df_local.sort_values(by='theta').reset_index(drop=True)\n",
    "    \n",
    "    discarded_points = []\n",
    "    states = []  # Para guardar estados intermedios\n",
    "    iteration = 0\n",
    "    \n",
    "    # 6. Asignación iterativa de puntos\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        assigned = False\n",
    "        \n",
    "        for g in groups:\n",
    "            if g['slope'] is None or g['intercept'] is None:\n",
    "                continue\n",
    "            if try_assign_point(point, g, model, radius_tolerance, point_distance_tolerance, slope_variation_threshold):\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'discarded': copy.deepcopy(discarded_points),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 7. Reasignar puntos descartados (etapa final)\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if g['slope'] is None or g['intercept'] is None:\n",
    "                    continue\n",
    "                if try_assign_point(p, g, model, radius_tolerance, point_distance_tolerance, slope_variation_threshold):\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:\n",
    "                new_grp = pd.DataFrame([p])\n",
    "                model.fit(new_grp[['theta']], new_grp['r'])\n",
    "                s_new = model.coef_[0]\n",
    "                i_new = model.intercept_\n",
    "                pa_new = calculate_pa(s_new)\n",
    "                if flexible_pa_range[0] <= abs(pa_new) <= flexible_pa_range[1]:\n",
    "                    groups.append({\n",
    "                        'seed_id': None,\n",
    "                        'points': new_grp,\n",
    "                        'slope': s_new,\n",
    "                        'intercept': i_new,\n",
    "                        'pa': pa_new\n",
    "                    })\n",
    "    \n",
    "    # 8. Subdividir grupos si existe un gap grande en θ o en r\n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap_theta=max_theta_gap, max_gap_r=max_r_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    # 9. Filtrar grupos con PA válida\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "    \n",
    "def plot_intermediate_state(state, df_all):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Fondo de todos los datos (asegurando conversión a numérico)\n",
    "    df_all['theta'] = pd.to_numeric(df_all['theta'], errors='coerce')\n",
    "    theta_all = np.radians(df_all['theta'].values)\n",
    "    r_all = df_all['r'].values\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points'].copy()\n",
    "        pts['theta'] = pd.to_numeric(pts['theta'], errors='coerce')\n",
    "        if pts['theta'].isna().any():\n",
    "            print(f\"Hay valores NaN en 'theta' para el grupo {idx}. Filas:\")\n",
    "            print(pts[pts['theta'].isna()])\n",
    "\n",
    "        theta_vals = np.radians(pts['theta'].values)\n",
    "        r_vals = pts['r'].values\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "\n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        df_desc['theta'] = pd.to_numeric(df_desc['theta'], errors='coerce')\n",
    "        theta_desc = np.radians(df_desc['theta'].values)\n",
    "        r_desc = df_desc['r'].values\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Ejecución interactiva e impresión de resultados\n",
    "# ========================\n",
    "\n",
    "states, final_groups, df_filtered = process_spiral_arms_with_seeds(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.68,\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 40.0),\n",
    "    slope_variation_threshold=0.18,\n",
    "    max_theta_gap=15.0,\n",
    "    max_r_gap=5.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10\n",
    ")\n",
    "\n",
    "print(f\"Total de estados guardados: {len(states)}\")\n",
    "print(f\"Total de grupos finales: {len(final_groups)}\")\n",
    "\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c024eee9-25d3-4ab9-a8db-016590f25518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones de Preprocesamiento\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula coordenadas polares y filtra puntos según el rango de θ.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    return df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "\n",
    "def calculate_pa(slope):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) correspondiente a la pendiente usando la función arctan.\n",
    "    Si la pendiente es None, retorna None.\n",
    "    \"\"\"\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Parte 1: Segmentación inicial con vecinos rectangulares y subdivisión por gaps\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo donde cada punto se conecta a otros si las diferencias en θ y r\n",
    "    son menores o iguales a los umbrales especificados.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Excluir el mismo índice\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda en anchura (BFS) sobre el grafo para extraer componentes conexas.\n",
    "    Cada componente se retorna como un DataFrame de puntos.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide en subclusters si se detecta un gap mayor\n",
    "    que el umbral especificado.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Carga y filtra los datos, construye el grafo, extrae componentes (clusters) y los subdivide\n",
    "    en función de gaps en θ y r.\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    \"\"\"\n",
    "    Genera una visualización de los clusters obtenidos.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Asignación de puntos a grupos sin ajuste (solo agrupación por rango)\n",
    "# ========================\n",
    "\n",
    "def try_assign_point_to_group(point, group, theta_tol=2.0, r_tol=0.5):\n",
    "    \"\"\"\n",
    "    Asigna 'point' al 'group' si su valor de θ y r está dentro de un rango definido\n",
    "    por el mínimo y máximo del grupo, con una tolerancia.\n",
    "    \"\"\"\n",
    "    # Aseguramos que el punto sea una Series\n",
    "    if isinstance(point, dict):\n",
    "        point = pd.DataFrame([point]).iloc[0]\n",
    "    theta_pt, r_pt = point['theta'], point['r']\n",
    "    group_theta_min = group['points']['theta'].min()\n",
    "    group_theta_max = group['points']['theta'].max()\n",
    "    group_r_min = group['points']['r'].min()\n",
    "    group_r_max = group['points']['r'].max()\n",
    "    if (theta_pt >= group_theta_min - theta_tol) and (theta_pt <= group_theta_max + theta_tol) and \\\n",
    "       (r_pt >= group_r_min - r_tol) and (r_pt <= group_r_max + r_tol):\n",
    "        new_row = point.to_frame().T\n",
    "        group['points'] = pd.concat([group['points'], new_row], ignore_index=True)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ========================\n",
    "# Finalización recursiva de segmentos: aplicar ajuste solo si el grupo tiene >=80 puntos y sin huecos.\n",
    "# Si hay huecos, subdividir y repetir.\n",
    "# ========================\n",
    "\n",
    "def check_no_gap(df_group, gap_threshold_theta, gap_threshold_r):\n",
    "    \"\"\"\n",
    "    Verifica que en el grupo no existan diferencias mayores que los umbrales en θ ni en r.\n",
    "    \"\"\"\n",
    "    sorted_theta = df_group.sort_values(by='theta')['theta'].values\n",
    "    sorted_r = df_group.sort_values(by='r')['r'].values\n",
    "    if len(sorted_theta) > 1 and np.max(np.diff(sorted_theta)) > gap_threshold_theta:\n",
    "        return False\n",
    "    if len(sorted_r) > 1 and np.max(np.diff(sorted_r)) > gap_threshold_r:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def process_group(pts, min_points, gap_threshold_theta, gap_threshold_r, seed_id=None):\n",
    "    \"\"\"\n",
    "    Procesa recursivamente un grupo de puntos:\n",
    "      - Si el grupo tiene menos de min_points, se retorna sin ajuste.\n",
    "      - Si no hay huecos y tiene >= min_points, se aplica la regresión.\n",
    "      - Si hay huecos, se subdivide primero por θ y luego por r, y se procesa cada subgrupo.\n",
    "    \"\"\"\n",
    "    pts = pts.copy().reset_index(drop=True)\n",
    "    if len(pts) < min_points:\n",
    "        return [{'seed_id': seed_id, 'points': pts, 'slope': None, 'intercept': None, 'pa': None}]\n",
    "    \n",
    "    if check_no_gap(pts, gap_threshold_theta, gap_threshold_r):\n",
    "        model = LinearRegression()\n",
    "        X = pts[['theta']].values\n",
    "        y = pts['r'].values\n",
    "        model.fit(X, y)\n",
    "        slope = model.coef_[0]\n",
    "        intercept = model.intercept_\n",
    "        pa = calculate_pa(slope)\n",
    "        return [{'seed_id': seed_id, 'points': pts, 'slope': slope, 'intercept': intercept, 'pa': pa}]\n",
    "    else:\n",
    "        segments = []\n",
    "        sub_theta = subdivide_by_gap(pts, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for sub in sub_theta:\n",
    "            sub_r = subdivide_by_gap(sub, gap_threshold=gap_threshold_r, mode='r')\n",
    "            for sg in sub_r:\n",
    "                segments.extend(process_group(sg, min_points, gap_threshold_theta, gap_threshold_r, seed_id=seed_id))\n",
    "        return segments\n",
    "\n",
    "def finalize_segments(groups, min_points=80, gap_threshold_theta=2.5, gap_threshold_r=2.5):\n",
    "    \"\"\"\n",
    "    Para cada grupo se aplica el proceso recursivo:\n",
    "      - Si el grupo es semilla (tiene seed_id) y cumple la condición (>= min_points y sin huecos),\n",
    "        se mantiene como grupo base actualizando su ajuste.\n",
    "      - En caso contrario, se subdivide recursivamente.\n",
    "    Retorna la lista de segmentos finales.\n",
    "    \"\"\"\n",
    "    final_segments = []\n",
    "    for g in groups:\n",
    "        pts = g['points']\n",
    "        seed_id = g.get('seed_id')\n",
    "        # Si es un grupo semilla y cumple la condición, se actualiza y se deja como está\n",
    "        if seed_id is not None and len(pts) >= min_points and check_no_gap(pts, gap_threshold_theta, gap_threshold_r):\n",
    "            model = LinearRegression()\n",
    "            X = pts[['theta']].values\n",
    "            y = pts['r'].values\n",
    "            model.fit(X, y)\n",
    "            g['slope'] = model.coef_[0]\n",
    "            g['intercept'] = model.intercept_\n",
    "            g['pa'] = calculate_pa(g['slope'])\n",
    "            final_segments.append(g)\n",
    "        else:\n",
    "            # Para grupos que no cumplen la condición (o no son semilla), se procesa recursivamente\n",
    "            segments = process_group(pts, min_points, gap_threshold_theta, gap_threshold_r, seed_id=seed_id)\n",
    "            final_segments.extend(segments)\n",
    "    return final_segments\n",
    "\n",
    "# ========================\n",
    "# Parte 2: Proceso iterativo utilizando los grupos semilla (sin ajuste inicial)\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_with_seeds(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,     # Vecindad horizontal para seeds\n",
    "    r_diff=0.68,        # Vecindad radial para seeds\n",
    "    gap_threshold_theta=1.9,\n",
    "    gap_threshold_r=1.23,\n",
    "    snapshot_interval=10,\n",
    "    theta_assignment_tol=2.0,\n",
    "    r_assignment_tol=0.5,\n",
    "    min_points_final=80  # Mínimo de puntos para aplicar el ajuste final\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso iterativo de asignación de puntos a grupos usando los grupos semilla como base.\n",
    "    Primero se agrupan los puntos (sin ajuste), luego se asignan los puntos restantes según su rango.\n",
    "    Finalmente, se finalizan los segmentos: si un grupo (preferiblemente una semilla) tiene al menos\n",
    "    min_points_final y no presenta huecos, se realiza la regresión; de lo contrario, se subdivide recursivamente.\n",
    "    Retorna los estados intermedios, los segmentos finales y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    # 1. Cargar y filtrar datos\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    \n",
    "    # 2. Filtrar según percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Obtener grupos semilla mediante segmentación rectangular\n",
    "    seeds_all, _ = process_clusters_rectangular_gaps(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    # Filtrar solo seeds con más de 60 puntos\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) > 60]\n",
    "    print(f\"Grupos semilla con más de 60 puntos: {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a un seed\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 4. Inicialización de grupos a partir de seeds SIN aplicar ajuste (sólo agrupación)\n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        groups.append({\n",
    "            'seed_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': None,\n",
    "            'intercept': None,\n",
    "            'pa': None\n",
    "        })\n",
    "    \n",
    "    # 5. Preparar puntos restantes (los que no pertenecen a ningún seed)\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    df_local = df_local.sort_values(by='theta').reset_index(drop=True)\n",
    "    \n",
    "    discarded_points = []  # Puntos que no se pudieron asignar inicialmente\n",
    "    states = []            # Para guardar estados intermedios\n",
    "    iteration = 0\n",
    "    \n",
    "    # 6. Asignación iterativa de puntos a grupos usando criterio basado en rango\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        assigned = False\n",
    "        for g in groups:\n",
    "            if try_assign_point_to_group(point, g, theta_tol=theta_assignment_tol, r_tol=r_assignment_tol):\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'discarded': copy.deepcopy(discarded_points),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 7. Reasignar puntos descartados\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            for g in groups:\n",
    "                if try_assign_point_to_group(p, g, theta_tol=theta_assignment_tol, r_tol=r_assignment_tol):\n",
    "                    break\n",
    "    \n",
    "    # 8. Finalización de segmentos: se aplica el proceso recursivo para evaluar gaps y, si corresponde, aplicar ajuste\n",
    "    final_segments = finalize_segments(groups, min_points=min_points_final, gap_threshold_theta=gap_threshold_theta, gap_threshold_r=gap_threshold_r)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(final_segments),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, final_segments, df_filtered\n",
    "\n",
    "def plot_intermediate_state(state, df_all):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Fondo de todos los datos (asegurando conversión a numérico)\n",
    "    df_all['theta'] = pd.to_numeric(df_all['theta'], errors='coerce')\n",
    "    theta_all = np.radians(df_all['theta'].values)\n",
    "    r_all = df_all['r'].values\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points'].copy()\n",
    "        pts['theta'] = pd.to_numeric(pts['theta'], errors='coerce')\n",
    "        theta_vals = np.radians(pts['theta'].values)\n",
    "        r_vals = pts['r'].values\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    \n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        df_desc['theta'] = pd.to_numeric(df_desc['theta'], errors='coerce')\n",
    "        theta_desc = np.radians(df_desc['theta'].values)\n",
    "        r_desc = df_desc['r'].values\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Ejecución interactiva e impresión de resultados\n",
    "# ========================\n",
    "\n",
    "states, final_segments, df_filtered = process_spiral_arms_with_seeds(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.155,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=2.0,\n",
    "    r_diff=0.468,\n",
    "    gap_threshold_theta=1.89,\n",
    "    gap_threshold_r=0.523,\n",
    "    snapshot_interval=10,\n",
    "    theta_assignment_tol=2.0,\n",
    "    r_assignment_tol=0.5,\n",
    "    min_points_final=80\n",
    ")\n",
    "\n",
    "print(f\"Total de estados guardados: {len(states)}\")\n",
    "print(f\"Total de segmentos finales: {len(final_segments)}\")\n",
    "\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda7c4db-1d44-4497-baaa-15b2f74ec6cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones de Preprocesamiento\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula coordenadas polares y filtra puntos según el rango de θ.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    return df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "\n",
    "def calculate_pa(slope):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) correspondiente a la pendiente usando la función arctan.\n",
    "    Si la pendiente es None, retorna None.\n",
    "    \"\"\"\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Parte 1: Segmentación inicial con vecinos rectangulares y subdivisión por gaps\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo donde cada punto se conecta a otros si las diferencias en θ y r\n",
    "    son menores o iguales a los umbrales especificados.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Excluir el mismo índice\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda en anchura (BFS) sobre el grafo para extraer componentes conexas.\n",
    "    Cada componente se retorna como un DataFrame de puntos.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide en subclusters si se detecta un gap mayor\n",
    "    que el umbral especificado.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Carga y filtra los datos, construye el grafo, extrae componentes (clusters) y los subdivide\n",
    "    en función de gaps en θ y r.\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    \"\"\"\n",
    "    Genera una visualización de los clusters obtenidos.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Asignación de puntos a grupos (sin ajuste inicial) basada en rango\n",
    "# ========================\n",
    "\n",
    "def try_assign_point_to_group(point, group, theta_tol=2.0, r_tol=0.5):\n",
    "    \"\"\"\n",
    "    Asigna 'point' al 'group' si su valor de θ y r está dentro de un rango definido\n",
    "    por el mínimo y máximo del grupo, con una tolerancia.\n",
    "    \"\"\"\n",
    "    if isinstance(point, dict):\n",
    "        point = pd.DataFrame([point]).iloc[0]\n",
    "    theta_pt, r_pt = point['theta'], point['r']\n",
    "    group_theta_min = group['points']['theta'].min()\n",
    "    group_theta_max = group['points']['theta'].max()\n",
    "    group_r_min = group['points']['r'].min()\n",
    "    group_r_max = group['points']['r'].max()\n",
    "    if (theta_pt >= group_theta_min - theta_tol) and (theta_pt <= group_theta_max + theta_tol) and \\\n",
    "       (r_pt >= group_r_min - r_tol) and (r_pt <= group_r_max + r_tol):\n",
    "        new_row = point.to_frame().T\n",
    "        group['points'] = pd.concat([group['points'], new_row], ignore_index=True)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ========================\n",
    "# Función para ajustar hasta dos rectas a cada grupo\n",
    "# ========================\n",
    "\n",
    "def adjust_two_lines(pts, min_segment_points=10, improvement_ratio_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Dado un DataFrame de puntos (ordenado por 'theta'), intenta ajustar dos rectas.\n",
    "    \n",
    "    - Si el número de puntos es insuficiente para dividir (menor que 2*min_segment_points),\n",
    "      se ajusta solo una recta.\n",
    "    - Se ajusta una recta única a todos los puntos y se calcula su error (suma de residuos al cuadrado).\n",
    "    - Se prueban posibles puntos de quiebre (breakpoints) para dividir en dos segmentos y se\n",
    "      calcula el error total (suma de errores de ambos segmentos).\n",
    "    - Si la mejora es significativa (error_total < improvement_ratio_threshold * error_unico),\n",
    "      se retornan dos segmentos; en caso contrario, se retorna un único segmento.\n",
    "    \"\"\"\n",
    "    pts = pts.copy().reset_index(drop=True)\n",
    "    n = len(pts)\n",
    "    if n < 2 * min_segment_points:\n",
    "        # No es posible dividir, se ajusta una recta\n",
    "        model = LinearRegression()\n",
    "        X = pts[['theta']].values\n",
    "        y = pts['r'].values\n",
    "        model.fit(X, y)\n",
    "        seg = {'points': pts, 'slope': model.coef_[0],\n",
    "               'intercept': model.intercept_, 'pa': calculate_pa(model.coef_[0])}\n",
    "        return [seg]\n",
    "    \n",
    "    # Ajuste con una recta única\n",
    "    model_all = LinearRegression()\n",
    "    X_all = pts[['theta']].values\n",
    "    y_all = pts['r'].values\n",
    "    model_all.fit(X_all, y_all)\n",
    "    error_all = np.sum((model_all.predict(X_all) - y_all)**2)\n",
    "    \n",
    "    best_error = np.inf\n",
    "    best_split = None\n",
    "    # Se prueban puntos de quiebre, asegurando que cada segmento tenga al menos min_segment_points\n",
    "    for i in range(min_segment_points, n - min_segment_points + 1):\n",
    "        pts_left = pts.iloc[:i]\n",
    "        pts_right = pts.iloc[i:]\n",
    "        model_left = LinearRegression()\n",
    "        model_right = LinearRegression()\n",
    "        X_left = pts_left[['theta']].values\n",
    "        y_left = pts_left['r'].values\n",
    "        X_right = pts_right[['theta']].values\n",
    "        y_right = pts_right['r'].values\n",
    "        model_left.fit(X_left, y_left)\n",
    "        model_right.fit(X_right, y_right)\n",
    "        error_left = np.sum((model_left.predict(X_left) - y_left)**2)\n",
    "        error_right = np.sum((model_right.predict(X_right) - y_right)**2)\n",
    "        total_error = error_left + error_right\n",
    "        if total_error < best_error:\n",
    "            best_error = total_error\n",
    "            best_split = i\n",
    "\n",
    "    if best_split is None:\n",
    "        seg = {'points': pts, 'slope': model_all.coef_[0],\n",
    "               'intercept': model_all.intercept_, 'pa': calculate_pa(model_all.coef_[0])}\n",
    "        return [seg]\n",
    "\n",
    "    if best_error < improvement_ratio_threshold * error_all:\n",
    "        # Se opta por dos segmentos\n",
    "        pts_left = pts.iloc[:best_split]\n",
    "        pts_right = pts.iloc[best_split:]\n",
    "        model_left = LinearRegression()\n",
    "        model_right = LinearRegression()\n",
    "        X_left = pts_left[['theta']].values\n",
    "        y_left = pts_left['r'].values\n",
    "        X_right = pts_right[['theta']].values\n",
    "        y_right = pts_right['r'].values\n",
    "        model_left.fit(X_left, y_left)\n",
    "        model_right.fit(X_right, y_right)\n",
    "        seg_left = {'points': pts_left, 'slope': model_left.coef_[0],\n",
    "                    'intercept': model_left.intercept_, 'pa': calculate_pa(model_left.coef_[0])}\n",
    "        seg_right = {'points': pts_right, 'slope': model_right.coef_[0],\n",
    "                     'intercept': model_right.intercept_, 'pa': calculate_pa(model_right.coef_[0])}\n",
    "        return [seg_left, seg_right]\n",
    "    else:\n",
    "        # No se obtiene una mejora significativa; se opta por un único segmento.\n",
    "        seg = {'points': pts, 'slope': model_all.coef_[0],\n",
    "               'intercept': model_all.intercept_, 'pa': calculate_pa(model_all.coef_[0])}\n",
    "        return [seg]\n",
    "\n",
    "# ========================\n",
    "# Finalización recursiva de segmentos usando ajuste de hasta dos rectas\n",
    "# ========================\n",
    "\n",
    "def finalize_segments(groups, min_points=80, gap_threshold_theta=2.05, gap_threshold_r=2.05,\n",
    "                      min_segment_points=10, improvement_ratio_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Para cada grupo (base, preferentemente semilla), se intenta ajustar hasta dos rectas.\n",
    "    Si el grupo es muy pequeño o uniforme se ajusta una recta; en caso contrario, se\n",
    "    busca dividir el grupo en dos segmentos. Si no se logra una división significativa,\n",
    "    se retorna un único segmento.\n",
    "    Retorna la lista de segmentos finales.\n",
    "    \"\"\"\n",
    "    final_segments = []\n",
    "    for g in groups:\n",
    "        pts = g['points']\n",
    "        if len(pts) < min_points:\n",
    "            seg = {'seed_id': g.get('seed_id'), 'points': pts,\n",
    "                   'slope': None, 'intercept': None, 'pa': None}\n",
    "            final_segments.append(seg)\n",
    "        else:\n",
    "            segs = adjust_two_lines(pts, min_segment_points, improvement_ratio_threshold)\n",
    "            # Aquí se podrían aplicar criterios adicionales; por ejemplo, si se obtuvo\n",
    "            # un único segmento pero el error es alto, se podría desagregar y re-procesar.\n",
    "            final_segments.extend(segs)\n",
    "    return final_segments\n",
    "\n",
    "# ========================\n",
    "# Parte 2: Proceso iterativo utilizando los grupos semilla (sin ajuste inicial)\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_with_seeds(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,     # Vecindad horizontal para seeds\n",
    "    r_diff=0.468,        # Vecindad radial para seeds\n",
    "    gap_threshold_theta=1.59,\n",
    "    gap_threshold_r=1.023,\n",
    "    snapshot_interval=10,\n",
    "    theta_assignment_tol=2.0,\n",
    "    r_assignment_tol=0.5,\n",
    "    min_points_final=80  # Mínimo de puntos para aplicar el ajuste final\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso iterativo de asignación de puntos a grupos usando los grupos semilla como base.\n",
    "    Primero se agrupan los puntos (sin ajuste), luego se asignan los puntos restantes según su rango.\n",
    "    Finalmente, se finalizan los segmentos aplicando el ajuste de hasta dos rectas para cada grupo.\n",
    "    Retorna los estados intermedios, los segmentos finales y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    # 1. Cargar y filtrar datos\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    \n",
    "    # 2. Filtrar según percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Obtener grupos semilla mediante segmentación rectangular\n",
    "    seeds_all, _ = process_clusters_rectangular_gaps(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    # Filtrar solo seeds con más de 60 puntos\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) > 60]\n",
    "    print(f\"Grupos semilla con más de 60 puntos: {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a un seed\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 4. Inicialización de grupos a partir de seeds SIN aplicar ajuste (solo agrupación)\n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        groups.append({\n",
    "            'seed_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': None,\n",
    "            'intercept': None,\n",
    "            'pa': None\n",
    "        })\n",
    "    \n",
    "    # 5. Preparar puntos restantes (los que no pertenecen a ningún seed)\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    df_local = df_local.sort_values(by='theta').reset_index(drop=True)\n",
    "    \n",
    "    discarded_points = []  # Puntos que no se pudieron asignar inicialmente\n",
    "    states = []            # Para guardar estados intermedios\n",
    "    iteration = 0\n",
    "    \n",
    "    # 6. Asignación iterativa de puntos a grupos usando criterio basado en rango\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        assigned = False\n",
    "        for g in groups:\n",
    "            if try_assign_point_to_group(point, g, theta_tol=theta_assignment_tol, r_tol=r_assignment_tol):\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'discarded': copy.deepcopy(discarded_points),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 7. Reasignar puntos descartados\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            for g in groups:\n",
    "                if try_assign_point_to_group(p, g, theta_tol=theta_assignment_tol, r_tol=r_assignment_tol):\n",
    "                    break\n",
    "    \n",
    "    # 8. Finalización de segmentos: aplicar el ajuste de hasta dos rectas a cada grupo\n",
    "    final_segments = finalize_segments(groups, min_points=min_points_final,\n",
    "                                       gap_threshold_theta=gap_threshold_theta,\n",
    "                                       gap_threshold_r=gap_threshold_r,\n",
    "                                       min_segment_points=10,\n",
    "                                       improvement_ratio_threshold=0.7)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(final_segments),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, final_segments, df_filtered\n",
    "\n",
    "def plot_intermediate_state(state, df_all):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Fondo de todos los datos (asegurando conversión a numérico)\n",
    "    df_all['theta'] = pd.to_numeric(df_all['theta'], errors='coerce')\n",
    "    theta_all = np.radians(df_all['theta'].values)\n",
    "    r_all = df_all['r'].values\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points'].copy()\n",
    "        pts['theta'] = pd.to_numeric(pts['theta'], errors='coerce')\n",
    "        theta_vals = np.radians(pts['theta'].values)\n",
    "        r_vals = pts['r'].values\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    \n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        df_desc['theta'] = pd.to_numeric(df_desc['theta'], errors='coerce')\n",
    "        theta_desc = np.radians(df_desc['theta'].values)\n",
    "        r_desc = df_desc['r'].values\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Ejecución interactiva e impresión de resultados\n",
    "# ========================\n",
    "\n",
    "states, final_segments, df_filtered = process_spiral_arms_with_seeds(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.155,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=2.0,\n",
    "    r_diff=0.468,\n",
    "    gap_threshold_theta=1.89,\n",
    "    gap_threshold_r=0.523,\n",
    "    snapshot_interval=10,\n",
    "    theta_assignment_tol=2.0,\n",
    "    r_assignment_tol=0.5,\n",
    "    min_points_final=80\n",
    ")\n",
    "\n",
    "print(f\"Total de estados guardados: {len(states)}\")\n",
    "print(f\"Total de segmentos finales: {len(final_segments)}\")\n",
    "\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2081884-7608-49d1-8a02-55ce5e4dd7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14be769e-6c17-4924-a547-da6b86c900bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "necesito que actúes como un experto en astrofísica computacional con experiencia en ingeniería de software y en algoritmos avanzados bueno qué va a pasar te voy a compartir dos códigos el primer código eh te voy a llamar lo voy a llamar código uno es el código que está tomando segmentos de recta y los requerimientos físicos para evaluar si un punto de de un conjunto dado de un submarino pertenece o no a a un grupo esto en términos de la pendiente en términos de teta y r de una ventana esa ventana digamos que permite que no haya una variación mayor de el 20% de esa pendiente digamos que con este con el código 1 hemos tenido muchos problemas porque no se ajusta de manera adecuacada uno de los puntos entonces tenemos una versión 2 te comparto la versión 2 el código 2 entonces el código 2 lo que te está tratando de hacer es de coger una un enfoque alternativo no utilizar de scan sino utilizar una técnica distinta para entonces eh ir evaluando qué puntos hacen parte de pues para coger esa semilla pero es una semilla que tiene muchos puntos entonces ese código 2 está iterando de manera reiterativa y no está teniendo en cuenta esa semilla de manera adecuada porque se está no se está teniendo pues no sé está preguntando si ese punto siguiente en realidad realmente hace parte de la semilla o es un punto distinto nuevo entonces la idea esexcluir los puntos que no son parte de la semilla antes de empezar a iterar para qué para que entonces evalúe si analice muy bien esos dos códigos y primero eh mira cómo se podrían integrar en uno solo de tal manera que yo tenga las bondades iniciales de la nueva técnica de las semillas pero que adicional también tenga los los las formas de validar que los puntos estén segmentados dentro de cada una de esas semillas dentro de cada uno de esos grupos de la manera en que el código uno lo hace me hago entender?\n",
    "código uno:\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# Función auxiliar para calcular distancia respecto a una línea (ajuste lineal)\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "# Función auxiliar para calcular PA\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "# Función para agregar un punto a un grupo temporal (si coincide en θ y en r)\n",
    "def add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window):\n",
    "    assigned = False\n",
    "    for tg in temp_groups:\n",
    "        tg_theta_min = tg['points']['theta'].min()\n",
    "        tg_theta_max = tg['points']['theta'].max()\n",
    "        tg_r_min = tg['points']['r'].min()\n",
    "        tg_r_max = tg['points']['r'].max()\n",
    "        if ((point['theta'] >= tg_theta_min - theta_fit_window/2) and \n",
    "            (point['theta'] <= tg_theta_max + theta_fit_window/2)) and \\\n",
    "           ((point['r'] >= tg_r_min - r_fit_window/2) and \n",
    "            (point['r'] <= tg_r_max + r_fit_window/2)):\n",
    "            tg['points'] = pd.concat([tg['points'], point.to_frame().T], ignore_index=True)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        new_tg = {'points': point.to_frame().T.copy()}\n",
    "        temp_groups.append(new_tg)\n",
    "\n",
    "# Función para revisar y fusionar (o promover) los grupos temporales\n",
    "def review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range):\n",
    "    new_temp_groups = []\n",
    "    for tg in temp_groups:\n",
    "        if len(tg['points']) < 3:\n",
    "            new_temp_groups.append(tg)\n",
    "            continue\n",
    "        X_tg = tg['points'][['theta']]\n",
    "        y_tg = tg['points']['r']\n",
    "        model.fit(X_tg, y_tg)\n",
    "        tg_slope = model.coef_[0]\n",
    "        tg_intercept = model.intercept_\n",
    "        tg_mean = tg['points']['theta'].mean()\n",
    "        merged = False\n",
    "        for mg in groups:\n",
    "            mg_min = mg['points']['theta'].min()\n",
    "            mg_max = mg['points']['theta'].max()\n",
    "            if mg_min <= tg_mean <= mg_max:\n",
    "                dist_values = tg['points'].apply(lambda row: calculate_distance(mg['slope'], mg['intercept'], row['theta'], row['r']), axis=1)\n",
    "                if dist_values.mean() < radius_tolerance:\n",
    "                    mg['points'] = pd.concat([mg['points'], tg['points']], ignore_index=True)\n",
    "                    model.fit(mg['points'][['theta']], mg['points']['r'])\n",
    "                    new_slope = model.coef_[0]\n",
    "                    if abs(new_slope - mg['slope'])/(abs(mg['slope'])+1e-12) < slope_variation_threshold:\n",
    "                        mg['slope'] = new_slope\n",
    "                        mg['intercept'] = model.intercept_\n",
    "                        mg['pa'] = calculate_pa(mg['slope'], mg['intercept'])\n",
    "                    merged = True\n",
    "                    break\n",
    "        if not merged:\n",
    "            new_group = {\n",
    "                'cluster_id': None,\n",
    "                'points': tg['points'],\n",
    "                'slope': tg_slope,\n",
    "                'intercept': tg_intercept,\n",
    "                'pa': calculate_pa(tg_slope, tg_intercept)\n",
    "            }\n",
    "            groups.append(new_group)\n",
    "    temp_groups.clear()\n",
    "    for tg in new_temp_groups:\n",
    "        temp_groups.append(tg)\n",
    "\n",
    "def process_spiral_arms_interactive(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    radius_tolerance=0.45,\n",
    "    point_distance_tolerance=0.59,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    eps_polar=0.7,\n",
    "    min_samples_polar=18,\n",
    "    slope_variation_threshold=0.20,\n",
    "    max_theta_gap=15.0,\n",
    "    do_subdivide=True,\n",
    "    snapshot_interval=10,     # Cada cuántas iteraciones se guarda un estado\n",
    "    theta_fit_window=30.0,    # Ventana en grados para ajustar la recta localmente\n",
    "    r_fit_window=10.0,        # Ventana fija para r\n",
    "    fusion_theta_threshold=7.0,  # Diferencia máxima fija en θ para fusionar\n",
    "    fusion_r_threshold=7.0       # Diferencia máxima fija en r para fusionar\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa los datos y retorna una lista de estados intermedios para visualización interactiva.\n",
    "    Se filtran los puntos con θ entre 50 y 250 grados.\n",
    "    \n",
    "    La fusión de grupos se realiza utilizando condiciones fijas:\n",
    "      - Se fusionan si la diferencia entre el límite inferior de un grupo y el límite superior de otro \n",
    "        (y viceversa) es menor que fusion_theta_threshold, y\n",
    "      - la diferencia en los límites de r es menor que fusion_r_threshold.\n",
    "    \"\"\"\n",
    "    # 1. Cargar datos\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    \n",
    "    # 2. Convertir a coordenadas polares\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= 50) & (df['theta'] <= 250)].copy()\n",
    "    \n",
    "    # 3. Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 4. DBSCAN en el plano (r, θ)\n",
    "    X_polar = df_filtered[['r', 'theta']].values\n",
    "    dbscan = DBSCAN(eps=eps_polar, min_samples=min_samples_polar)\n",
    "    df_filtered['cluster'] = dbscan.fit_predict(X_polar)\n",
    "    \n",
    "    # 5. Fusión de clusters cercanos\n",
    "    clusters_tolerance = 0.70\n",
    "    def merge_close_clusters(clusters, clusters_tolerance):\n",
    "        merged_clusters = {}\n",
    "        used_clusters = set()\n",
    "        for id1, points1 in clusters.items():\n",
    "            if id1 in used_clusters:\n",
    "                continue\n",
    "            merged_points = points1.copy()\n",
    "            for id2, points2 in clusters.items():\n",
    "                if id1 >= id2 or id2 in used_clusters:\n",
    "                    continue\n",
    "                min_distance = np.min(np.sqrt(\n",
    "                    (points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                    (points1['theta'].values[:, None] - points2['theta'].values)**2\n",
    "                ))\n",
    "                if min_distance < clusters_tolerance:\n",
    "                    merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                    used_clusters.add(id2)\n",
    "            merged_clusters[id1] = merged_points\n",
    "            used_clusters.add(id1)\n",
    "        return merged_clusters\n",
    "    \n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df_filtered['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df_filtered[df_filtered['cluster'] == cluster_id].copy()\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "    \n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "    \n",
    "    # 6. Inicializar grupos principales a partir de los clusters obtenidos\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for cid, pts in clusters_polar.items():\n",
    "        if len(pts) >= 3:\n",
    "            X_ = pts[['theta']].values\n",
    "            y_ = pts['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'cluster_id': cid,\n",
    "            'points': pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Preparar estados y grupos temporales\n",
    "    df_local = df_filtered.copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    states = []\n",
    "    temp_groups = []\n",
    "    iteration = 0\n",
    "    temp_review_interval = snapshot_interval * 15\n",
    "    \n",
    "    # 7. Asignación iterativa de puntos\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Buscar grupo candidato que contenga el punto\n",
    "        for g in groups:\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            if not (g_theta_min <= theta_pt <= g_theta_max and g_r_min <= r_pt <= g_r_max):\n",
    "                continue\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            if len(subset) == 0:\n",
    "                continue\n",
    "            r_min = subset['r'].min()\n",
    "            r_max = subset['r'].max()\n",
    "            if not (r_min <= r_pt <= r_max):\n",
    "                continue\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            if old_slope is None:\n",
    "                continue\n",
    "            if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) < 0.10:\n",
    "                continue\n",
    "            dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "            if dist > radius_tolerance:\n",
    "                continue\n",
    "            if best_distance is None or dist < best_distance:\n",
    "                best_distance = dist\n",
    "                best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = best_group['points'][(best_group['points']['theta'] >= window_min) & (best_group['points']['theta'] <= window_max)]\n",
    "            if len(subset) < 2:\n",
    "                subset = best_group['points']\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "            \n",
    "            # Fusión fija con grupos anteriores: se fusionan solo si las diferencias en ambos límites son menores que los umbrales fijos.\n",
    "            merged_flag = True\n",
    "            while merged_flag:\n",
    "                merged_flag = False\n",
    "                for other_group in groups:\n",
    "                    if other_group is best_group:\n",
    "                        continue\n",
    "                    best_theta_min = best_group['points']['theta'].min()\n",
    "                    best_theta_max = best_group['points']['theta'].max()\n",
    "                    best_r_min = best_group['points']['r'].min()\n",
    "                    best_r_max = best_group['points']['r'].max()\n",
    "                    \n",
    "                    other_theta_min = other_group['points']['theta'].min()\n",
    "                    other_theta_max = other_group['points']['theta'].max()\n",
    "                    other_r_min = other_group['points']['r'].min()\n",
    "                    other_r_max = other_group['points']['r'].max()\n",
    "                    \n",
    "                    if (abs(best_theta_min - other_theta_max) < fusion_theta_threshold and\n",
    "                        abs(best_theta_max - other_theta_min) < fusion_theta_threshold and\n",
    "                        abs(best_r_min - other_r_max) < fusion_r_threshold and\n",
    "                        abs(best_r_max - other_r_min) < fusion_r_threshold):\n",
    "                        merged_points = pd.concat([best_group['points'], other_group['points']], ignore_index=True)\n",
    "                        model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                        best_group['points'] = merged_points\n",
    "                        best_group['slope'] = model.coef_[0]\n",
    "                        best_group['intercept'] = model.intercept_\n",
    "                        best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "                        # Eliminar other_group por identidad\n",
    "                        for idx, grp in enumerate(groups):\n",
    "                            if grp is other_group:\n",
    "                                groups.pop(idx)\n",
    "                                break\n",
    "                        merged_flag = True\n",
    "                        break\n",
    "                        \n",
    "        else:\n",
    "            # Si no hay grupo candidato, intentar fusionar con el último grupo principal sin filtro de pendiente\n",
    "            merged_with_last = False\n",
    "            if len(groups) > 0:\n",
    "                last_g = groups[-1]\n",
    "                last_theta_min = last_g['points']['theta'].min()\n",
    "                last_theta_max = last_g['points']['theta'].max()\n",
    "                last_r_min = last_g['points']['r'].min()\n",
    "                last_r_max = last_g['points']['r'].max()\n",
    "                if (last_theta_min <= theta_pt <= last_theta_max and last_r_min <= r_pt <= last_r_max):\n",
    "                    window_min = theta_pt - theta_fit_window/2\n",
    "                    window_max = theta_pt + theta_fit_window/2\n",
    "                    subset_last = last_g['points'][(last_g['points']['theta'] >= window_min) & (last_g['points']['theta'] <= window_max)]\n",
    "                    if len(subset_last) > 0:\n",
    "                        r_min_l = subset_last['r'].min()\n",
    "                        r_max_l = subset_last['r'].max()\n",
    "                        if r_min_l <= r_pt <= r_max_l:\n",
    "                            subset_last = pd.concat([subset_last, point.to_frame().T], ignore_index=True)\n",
    "                            model.fit(subset_last[['theta']], subset_last['r'])\n",
    "                            new_slope_l = model.coef_[0]\n",
    "                            new_intercept_l = model.intercept_\n",
    "                            dist_l = calculate_distance(new_slope_l, new_intercept_l, theta_pt, r_pt)\n",
    "                            if dist_l < radius_tolerance:\n",
    "                                last_g['points'] = pd.concat([last_g['points'], point.to_frame().T], ignore_index=True)\n",
    "                                model.fit(last_g['points'][['theta']], last_g['points']['r'])\n",
    "                                last_g['slope'] = model.coef_[0]\n",
    "                                last_g['intercept'] = model.intercept_\n",
    "                                last_g['pa'] = calculate_pa(last_g['slope'], last_g['intercept'])\n",
    "                                merged_with_last = True\n",
    "            if not merged_with_last:\n",
    "                add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window)\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        if iteration % temp_review_interval == 0:\n",
    "            review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "        \n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_groups': copy.deepcopy(temp_groups),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    if len(temp_groups) > 0:\n",
    "        review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "    \n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    def subdivide_if_gap(group, max_gap=max_theta_gap):\n",
    "        pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group['cluster_id'],\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'temp_groups': copy.deepcopy(temp_groups),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if state['temp_groups']:\n",
    "        for idx, tg in enumerate(state['temp_groups']):\n",
    "            pts = tg['points']\n",
    "            plt.scatter(pts['theta'], pts['r'], s=10, color='black', marker='x', label=f\"Temp {idx+1}\")\n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "fin de código uno.\n",
    "\n",
    "código dos:\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones de Preprocesamiento\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula coordenadas polares y filtra puntos según el rango de θ.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    return df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "\n",
    "def calculate_pa(slope):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) correspondiente a la pendiente usando la función arctan.\n",
    "    Si la pendiente es None, retorna None.\n",
    "    \"\"\"\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Parte 1: Segmentación inicial con vecinos rectangulares y subdivisión por gaps\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo donde cada punto se conecta a otros si las diferencias en θ y r\n",
    "    son menores o iguales a los umbrales especificados.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Excluir el mismo índice\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda en anchura (BFS) sobre el grafo para extraer componentes conexas.\n",
    "    Cada componente se retorna como un DataFrame de puntos.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide en subclusters si se detecta un gap mayor\n",
    "    que el umbral especificado.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Carga y filtra los datos, construye el grafo, extrae componentes (clusters) y los subdivide\n",
    "    en función de gaps en θ y r.\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_clusters_rectangular_gaps(clusters, title=\"Clusters con BFS + Gaps\"):\n",
    "    \"\"\"\n",
    "    Genera una visualización de los clusters obtenidos.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','darkblue']\n",
    "    for idx, dfc in enumerate(clusters):\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(dfc['theta'], dfc['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Asignación de puntos a grupos (sin ajuste inicial) basada en rango\n",
    "# ========================\n",
    "\n",
    "def try_assign_point_to_group(point, group, theta_tol=2.0, r_tol=0.5):\n",
    "    \"\"\"\n",
    "    Asigna 'point' al 'group' si su valor de θ y r está dentro de un rango definido\n",
    "    por el mínimo y máximo del grupo, con una tolerancia.\n",
    "    \"\"\"\n",
    "    if isinstance(point, dict):\n",
    "        point = pd.DataFrame([point]).iloc[0]\n",
    "    theta_pt, r_pt = point['theta'], point['r']\n",
    "    group_theta_min = group['points']['theta'].min()\n",
    "    group_theta_max = group['points']['theta'].max()\n",
    "    group_r_min = group['points']['r'].min()\n",
    "    group_r_max = group['points']['r'].max()\n",
    "    if (theta_pt >= group_theta_min - theta_tol) and (theta_pt <= group_theta_max + theta_tol) and \\\n",
    "       (r_pt >= group_r_min - r_tol) and (r_pt <= group_r_max + r_tol):\n",
    "        new_row = point.to_frame().T\n",
    "        group['points'] = pd.concat([group['points'], new_row], ignore_index=True)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ========================\n",
    "# Función para ajustar hasta dos rectas a cada grupo\n",
    "# ========================\n",
    "\n",
    "def adjust_two_lines(pts, min_segment_points=10, improvement_ratio_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Dado un DataFrame de puntos (ordenado por 'theta'), intenta ajustar dos rectas.\n",
    "    \n",
    "    - Si el número de puntos es insuficiente para dividir (menor que 2*min_segment_points),\n",
    "      se ajusta solo una recta.\n",
    "    - Se ajusta una recta única a todos los puntos y se calcula su error (suma de residuos al cuadrado).\n",
    "    - Se prueban posibles puntos de quiebre (breakpoints) para dividir en dos segmentos y se\n",
    "      calcula el error total (suma de errores de ambos segmentos).\n",
    "    - Si la mejora es significativa (error_total < improvement_ratio_threshold * error_unico),\n",
    "      se retornan dos segmentos; en caso contrario, se retorna un único segmento.\n",
    "    \"\"\"\n",
    "    pts = pts.copy().reset_index(drop=True)\n",
    "    n = len(pts)\n",
    "    if n < 2 * min_segment_points:\n",
    "        # No es posible dividir, se ajusta una recta\n",
    "        model = LinearRegression()\n",
    "        X = pts[['theta']].values\n",
    "        y = pts['r'].values\n",
    "        model.fit(X, y)\n",
    "        seg = {'points': pts, 'slope': model.coef_[0],\n",
    "               'intercept': model.intercept_, 'pa': calculate_pa(model.coef_[0])}\n",
    "        return [seg]\n",
    "    \n",
    "    # Ajuste con una recta única\n",
    "    model_all = LinearRegression()\n",
    "    X_all = pts[['theta']].values\n",
    "    y_all = pts['r'].values\n",
    "    model_all.fit(X_all, y_all)\n",
    "    error_all = np.sum((model_all.predict(X_all) - y_all)**2)\n",
    "    \n",
    "    best_error = np.inf\n",
    "    best_split = None\n",
    "    # Se prueban puntos de quiebre, asegurando que cada segmento tenga al menos min_segment_points\n",
    "    for i in range(min_segment_points, n - min_segment_points + 1):\n",
    "        pts_left = pts.iloc[:i]\n",
    "        pts_right = pts.iloc[i:]\n",
    "        model_left = LinearRegression()\n",
    "        model_right = LinearRegression()\n",
    "        X_left = pts_left[['theta']].values\n",
    "        y_left = pts_left['r'].values\n",
    "        X_right = pts_right[['theta']].values\n",
    "        y_right = pts_right['r'].values\n",
    "        model_left.fit(X_left, y_left)\n",
    "        model_right.fit(X_right, y_right)\n",
    "        error_left = np.sum((model_left.predict(X_left) - y_left)**2)\n",
    "        error_right = np.sum((model_right.predict(X_right) - y_right)**2)\n",
    "        total_error = error_left + error_right\n",
    "        if total_error < best_error:\n",
    "            best_error = total_error\n",
    "            best_split = i\n",
    "\n",
    "    if best_split is None:\n",
    "        seg = {'points': pts, 'slope': model_all.coef_[0],\n",
    "               'intercept': model_all.intercept_, 'pa': calculate_pa(model_all.coef_[0])}\n",
    "        return [seg]\n",
    "\n",
    "    if best_error < improvement_ratio_threshold * error_all:\n",
    "        # Se opta por dos segmentos\n",
    "        pts_left = pts.iloc[:best_split]\n",
    "        pts_right = pts.iloc[best_split:]\n",
    "        model_left = LinearRegression()\n",
    "        model_right = LinearRegression()\n",
    "        X_left = pts_left[['theta']].values\n",
    "        y_left = pts_left['r'].values\n",
    "        X_right = pts_right[['theta']].values\n",
    "        y_right = pts_right['r'].values\n",
    "        model_left.fit(X_left, y_left)\n",
    "        model_right.fit(X_right, y_right)\n",
    "        seg_left = {'points': pts_left, 'slope': model_left.coef_[0],\n",
    "                    'intercept': model_left.intercept_, 'pa': calculate_pa(model_left.coef_[0])}\n",
    "        seg_right = {'points': pts_right, 'slope': model_right.coef_[0],\n",
    "                     'intercept': model_right.intercept_, 'pa': calculate_pa(model_right.coef_[0])}\n",
    "        return [seg_left, seg_right]\n",
    "    else:\n",
    "        # No se obtiene una mejora significativa; se opta por un único segmento.\n",
    "        seg = {'points': pts, 'slope': model_all.coef_[0],\n",
    "               'intercept': model_all.intercept_, 'pa': calculate_pa(model_all.coef_[0])}\n",
    "        return [seg]\n",
    "\n",
    "# ========================\n",
    "# Finalización recursiva de segmentos usando ajuste de hasta dos rectas\n",
    "# ========================\n",
    "\n",
    "def finalize_segments(groups, min_points=80, gap_threshold_theta=2.05, gap_threshold_r=2.05,\n",
    "                      min_segment_points=10, improvement_ratio_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Para cada grupo (base, preferentemente semilla), se intenta ajustar hasta dos rectas.\n",
    "    Si el grupo es muy pequeño o uniforme se ajusta una recta; en caso contrario, se\n",
    "    busca dividir el grupo en dos segmentos. Si no se logra una división significativa,\n",
    "    se retorna un único segmento.\n",
    "    Retorna la lista de segmentos finales.\n",
    "    \"\"\"\n",
    "    final_segments = []\n",
    "    for g in groups:\n",
    "        pts = g['points']\n",
    "        if len(pts) < min_points:\n",
    "            seg = {'seed_id': g.get('seed_id'), 'points': pts,\n",
    "                   'slope': None, 'intercept': None, 'pa': None}\n",
    "            final_segments.append(seg)\n",
    "        else:\n",
    "            segs = adjust_two_lines(pts, min_segment_points, improvement_ratio_threshold)\n",
    "            # Aquí se podrían aplicar criterios adicionales; por ejemplo, si se obtuvo\n",
    "            # un único segmento pero el error es alto, se podría desagregar y re-procesar.\n",
    "            final_segments.extend(segs)\n",
    "    return final_segments\n",
    "\n",
    "# ========================\n",
    "# Parte 2: Proceso iterativo utilizando los grupos semilla (sin ajuste inicial)\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_with_seeds(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=3.0,     # Vecindad horizontal para seeds\n",
    "    r_diff=0.468,        # Vecindad radial para seeds\n",
    "    gap_threshold_theta=1.59,\n",
    "    gap_threshold_r=1.023,\n",
    "    snapshot_interval=10,\n",
    "    theta_assignment_tol=2.0,\n",
    "    r_assignment_tol=0.5,\n",
    "    min_points_final=80  # Mínimo de puntos para aplicar el ajuste final\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso iterativo de asignación de puntos a grupos usando los grupos semilla como base.\n",
    "    Primero se agrupan los puntos (sin ajuste), luego se asignan los puntos restantes según su rango.\n",
    "    Finalmente, se finalizan los segmentos aplicando el ajuste de hasta dos rectas para cada grupo.\n",
    "    Retorna los estados intermedios, los segmentos finales y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    # 1. Cargar y filtrar datos\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    \n",
    "    # 2. Filtrar según percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 3. Obtener grupos semilla mediante segmentación rectangular\n",
    "    seeds_all, _ = process_clusters_rectangular_gaps(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    # Filtrar solo seeds con más de 60 puntos\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) > 60]\n",
    "    print(f\"Grupos semilla con más de 60 puntos: {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a un seed\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 4. Inicialización de grupos a partir de seeds SIN aplicar ajuste (solo agrupación)\n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        groups.append({\n",
    "            'seed_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': None,\n",
    "            'intercept': None,\n",
    "            'pa': None\n",
    "        })\n",
    "    \n",
    "    # 5. Preparar puntos restantes (los que no pertenecen a ningún seed)\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    df_local = df_local.sort_values(by='theta').reset_index(drop=True)\n",
    "    \n",
    "    discarded_points = []  # Puntos que no se pudieron asignar inicialmente\n",
    "    states = []            # Para guardar estados intermedios\n",
    "    iteration = 0\n",
    "    \n",
    "    # 6. Asignación iterativa de puntos a grupos usando criterio basado en rango\n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        assigned = False\n",
    "        for g in groups:\n",
    "            if try_assign_point_to_group(point, g, theta_tol=theta_assignment_tol, r_tol=r_assignment_tol):\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            discarded_points.append(point.to_dict())\n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'discarded': copy.deepcopy(discarded_points),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # 7. Reasignar puntos descartados\n",
    "    discarded_df = pd.DataFrame(discarded_points)\n",
    "    if not discarded_df.empty:\n",
    "        for _, p in discarded_df.iterrows():\n",
    "            for g in groups:\n",
    "                if try_assign_point_to_group(p, g, theta_tol=theta_assignment_tol, r_tol=r_assignment_tol):\n",
    "                    break\n",
    "    \n",
    "    # 8. Finalización de segmentos: aplicar el ajuste de hasta dos rectas a cada grupo\n",
    "    final_segments = finalize_segments(groups, min_points=min_points_final,\n",
    "                                       gap_threshold_theta=gap_threshold_theta,\n",
    "                                       gap_threshold_r=gap_threshold_r,\n",
    "                                       min_segment_points=10,\n",
    "                                       improvement_ratio_threshold=0.7)\n",
    "    \n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(final_segments),\n",
    "        'discarded': copy.deepcopy(discarded_points),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, final_segments, df_filtered\n",
    "\n",
    "def plot_intermediate_state(state, df_all):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Fondo de todos los datos (asegurando conversión a numérico)\n",
    "    df_all['theta'] = pd.to_numeric(df_all['theta'], errors='coerce')\n",
    "    theta_all = np.radians(df_all['theta'].values)\n",
    "    r_all = df_all['r'].values\n",
    "    ax.scatter(theta_all, r_all, s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    \n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points'].copy()\n",
    "        pts['theta'] = pd.to_numeric(pts['theta'], errors='coerce')\n",
    "        theta_vals = np.radians(pts['theta'].values)\n",
    "        r_vals = pts['r'].values\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.scatter(theta_vals, r_vals, s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            ax.plot(np.radians(theta_line), r_line, '--', color=color, linewidth=1.5)\n",
    "    \n",
    "    if len(state['discarded']) > 0:\n",
    "        df_desc = pd.DataFrame(state['discarded'])\n",
    "        df_desc['theta'] = pd.to_numeric(df_desc['theta'], errors='coerce')\n",
    "        theta_desc = np.radians(df_desc['theta'].values)\n",
    "        r_desc = df_desc['r'].values\n",
    "        ax.scatter(theta_desc, r_desc, s=10, color='black', marker='x', label='Descartados')\n",
    "    \n",
    "    ax.set_title(f\"Estado en iteración {state['iteration']}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Ejecución interactiva e impresión de resultados\n",
    "# ========================\n",
    "\n",
    "states, final_segments, df_filtered = process_spiral_arms_with_seeds(\n",
    "    id_halo=\"17\",\n",
    "    quartile_threshold=0.155,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    theta_diff=2.0,\n",
    "    r_diff=0.468,\n",
    "    gap_threshold_theta=1.89,\n",
    "    gap_threshold_r=0.523,\n",
    "    snapshot_interval=10,\n",
    "    theta_assignment_tol=2.0,\n",
    "    r_assignment_tol=0.5,\n",
    "    min_points_final=80\n",
    ")\n",
    "\n",
    "print(f\"Total de estados guardados: {len(states)}\")\n",
    "print(f\"Total de segmentos finales: {len(final_segments)}\")\n",
    "\n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0)).\n",
    "fin de codigo dos.----------\n",
    "**algunos ajustes para entender que hacer y la razón de cada uno**:\n",
    "\n",
    "1. **Semillas construidas con BFS pero sin ajustar pendiente**  \n",
    "   - **Motivo**: Iniciar con grupos (semillas) “en bruto” sin slope/intercept, lo cual evita un ajuste prematuro y permite que, en la iteración posterior, se calcule la pendiente únicamente cuando el grupo tenga suficientes puntos (≥ 2).\n",
    "\n",
    "2. **Exclusión de los puntos ya usados en las semillas en el bucle iterativo**  \n",
    "   - **Motivo**: Evitar redundancia y sobreasignación. Ya que las semillas están definidas, no volvemos a procesar esos puntos en la misma rutina; únicamente trabajamos con los que no fueron capturados por las semillas.\n",
    "\n",
    "3. **Asignación de cada nuevo punto a un grupo mediante un criterio rápido de “radio de tolerancia” (radius_tolerance)**  \n",
    "   - **Motivo**: Reducir la complejidad de la comparación y agilizar la asignación, chequeando si el punto está cerca (en el espacio θ–r) del grupo (sea euclídeamente o respecto a la recta si el grupo ya tiene slope).\n",
    "\n",
    "4. **Cálculo del slope cuando el grupo no lo tenía**  \n",
    "   - **Motivo**: Una vez que el grupo tiene al menos 2 puntos, se hace su primer ajuste lineal. Hasta ese momento, no se asigna pendiente.\n",
    "\n",
    "5. **Fusión “temprana” de grupos contiguos o cercanos**  \n",
    "   - **Motivo**: En cuanto un punto actualiza un grupo, se chequea si hay grupos vecinos superpuestos o muy próximos (por bounding box **o** por una distancia mínima en θ–r) para **fusionarlos** inmediatamente. Así se evitan grupos duplicados o separados artificialmente.\n",
    "\n",
    "6. **Extrapolación (20% extra) y variación de la pendiente hasta 30%** (solo en la versión donde se pedía “extrapolar” un 20% y permitir un 30% de variación)  \n",
    "   - **Motivo**: Permitir cierta flexibilidad en la asignación de puntos que están en el borde (extrapolación) y mayor tolerancia en la pendiente en esas zonas “extendidas”.\n",
    "\n",
    "7. **Criterio de cercanía más robusto** (bounding box + distancia mínima)  \n",
    "   - **Motivo**: En ciertos casos, grupos que lucen “muy próximos” no se fusionan porque sus bounding boxes no se superponen lo suficiente. Añadir la verificación de distancia mínima entre puntos de ambos grupos garantiza que sí se fusionen cuando existen subzonas verdaderamente cercanas.\n",
    "\n",
    "8. **Eliminar grupos por identidad y no por igualdad** (reemplazar `groups.remove(...)` con un `for ... pop(...)`)  \n",
    "   - **Motivo**: Evitar el `ValueError: Can only compare identically-labeled DataFrame objects`. Python intenta comparar DataFrames cuando se usa `.remove()`, lo que genera un conflicto. Con `if grp is other_group: pop(...)` se elimina el grupo exacto sin comparar los contenidos.\n",
    "\n",
    "9. **Forzar `astype(float)` en las columnas `theta` y `r`** para evitar errores de `ufunc`  \n",
    "   - **Motivo**: Asegurarse de que, al calcular distancias con NumPy, los valores sean numéricos válidos. Si alguna columna contiene objetos no convertibles a float, se suscita el error “loop of ufunc does not support argument 0 of type...”.\n",
    "\n",
    "**En conjunto**, todos estos ajustes:\n",
    "- **Optimizan** la construcción de grupos iniciales (semillas).  \n",
    "- **Agilizan** la asignación iterativa de puntos.  \n",
    "- **Permiten** un ajuste lineal más coherente (solo cuando un grupo lo necesita).  \n",
    "- **Garantizan** que grupos realmente cercanos se fusionen adecuadamente.  \n",
    "- **Evitan** errores de comparación de DataFrames y de tipos no numéricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f515cc3f-27da-411b-86fa-1586ae4278b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## combinación c+ódigos y engfoques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c3ecbb00-0a3e-41ae-9ad8-166cbf561fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import deque\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones Auxiliares\n",
    "# ========================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"Calcula la distancia vertical (en r) de un punto a la recta definida por slope e intercept.\"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) de la recta a partir de su pendiente.\n",
    "    Si se dispone de intercepto, se usa para un cálculo ajustado.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Carga y Preprocesamiento de Datos\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold):\n",
    "    \"\"\"\n",
    "    Carga el CSV, calcula coordenadas polares y filtra puntos:\n",
    "      - Por rango de θ (en grados).\n",
    "      - Por percentil en 'rho_resta_final_exp'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# ========================\n",
    "# Detección de Semillas utilizando Grafos y BFS\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo conectando puntos si la diferencia en θ y en r es menor o igual a los umbrales.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Se excluye el mismo punto\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae las componentes conexas (clusters) del grafo usando búsqueda en anchura (BFS).\n",
    "    Cada componente se retorna como un DataFrame.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide el cluster en subclusters \n",
    "    si existe un gap mayor que gap_threshold.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def detect_seeds(id_halo, quartile_threshold=0.55, theta_min=50.0, theta_max=250.0, \n",
    "                 theta_diff=5.0, r_diff=1.0, gap_threshold_theta=2.5, gap_threshold_r=2.5, min_seed_points=60):\n",
    "    \"\"\"\n",
    "    Detecta semillas robustas usando la construcción de grafo, BFS y subdivisión por gaps.\n",
    "    Retorna una lista de DataFrames (cada uno es una semilla) y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    df_filtered = load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold)\n",
    "    \n",
    "    # Construir grafo y extraer componentes conexas\n",
    "    graph = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    # Subdividir cada cluster según gaps en θ y luego en r\n",
    "    seeds_all = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = []\n",
    "            for group in subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r'):\n",
    "                sub_r.append(group)\n",
    "            seeds_all.extend(sub_r)\n",
    "    \n",
    "    # Filtrar semillas que tengan al menos min_seed_points\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) >= min_seed_points]\n",
    "    print(f\"Total de semillas robustas (>= {min_seed_points} puntos): {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a alguna semilla\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    return seed_clusters, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Asignación Iterativa de Puntos a Grupos con Validación Local\n",
    "# ========================\n",
    "\n",
    "def iterative_assignment(seed_clusters, df_filtered, theta_fit_window=30.0, r_fit_window=10.0, \n",
    "                          radius_tolerance=0.45, slope_variation_threshold=0.20, snapshot_interval=10):\n",
    "    \"\"\"\n",
    "    Usa las semillas como grupos iniciales (con ajuste lineal para obtener pendiente, intercepto y ángulo)\n",
    "    y asigna iterativamente los puntos restantes (no pertenecientes a semillas) a estos grupos, validando\n",
    "    que la incorporación de cada punto mantenga la coherencia geométrica.\n",
    "    Retorna estados intermedios y los grupos finales.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    # Inicializar grupos a partir de semillas\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 2:\n",
    "            X_seed = seed[['theta']].values\n",
    "            y_seed = seed['r'].values\n",
    "            model.fit(X_seed, y_seed)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'group_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Puntos restantes que no están en semillas\n",
    "    df_remaining = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    df_remaining = df_remaining.sort_values(by='theta').reset_index(drop=True)\n",
    "    \n",
    "    temp_groups = []  # Para almacenar puntos que inicialmente no se asignan\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    \n",
    "    # Bucle iterativo de asignación\n",
    "    while len(df_remaining) > 0:\n",
    "        point = df_remaining.iloc[0]\n",
    "        df_remaining = df_remaining.iloc[1:].reset_index(drop=True)\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Buscar grupo candidato en el que el punto caiga dentro de una ventana local\n",
    "        for g in groups:\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            if not (g_theta_min <= theta_pt <= g_theta_max and g_r_min <= r_pt <= g_r_max):\n",
    "                continue\n",
    "            # Definir ventana local para ajuste\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            # Agregar el punto y reajustar la recta\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            # Validar que la variación en la pendiente no exceda el umbral\n",
    "            if old_slope is not None and abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) > slope_variation_threshold:\n",
    "                dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if best_distance is None or dist < best_distance:\n",
    "                        best_distance = dist\n",
    "                        best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            # Asigna el punto al grupo y actualiza sus parámetros\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = best_group['points'][(best_group['points']['theta'] >= window_min) & (best_group['points']['theta'] <= window_max)]\n",
    "            if len(subset) < 2:\n",
    "                subset = best_group['points']\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "        else:\n",
    "            # Si no se pudo asignar, se agrega a un grupo temporal\n",
    "            temp_groups.append(point.to_frame().T)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_remaining) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_points': pd.concat(temp_groups, ignore_index=True) if temp_groups else pd.DataFrame(),\n",
    "                'remaining': df_remaining.copy()\n",
    "            })\n",
    "    \n",
    "    # Reintentar asignar puntos temporales a los grupos existentes\n",
    "    if temp_groups:\n",
    "        temp_df = pd.concat(temp_groups, ignore_index=True)\n",
    "        for idx, point in temp_df.iterrows():\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if (g['points']['theta'].min() - theta_fit_window/2 <= point['theta'] <= g['points']['theta'].max() + theta_fit_window/2 and\n",
    "                    g['points']['r'].min() - r_fit_window/2 <= point['r'] <= g['points']['r'].max() + r_fit_window/2):\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    model.fit(g['points'][['theta']], g['points']['r'])\n",
    "                    g['slope'] = model.coef_[0]\n",
    "                    g['intercept'] = model.intercept_\n",
    "                    g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:\n",
    "                # Si aun no se asigna, se puede dejar descartado o manejarlo aparte\n",
    "                pass\n",
    "    \n",
    "    return states, groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Fusión y Subdivisión Final de Grupos\n",
    "# ========================\n",
    "\n",
    "def subdivide_if_gap(group, max_theta_gap=15.0):\n",
    "    \"\"\"\n",
    "    Divide el grupo en dos segmentos si se detecta un gap significativo en la variable θ.\n",
    "    \"\"\"\n",
    "    pts = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_theta_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    model = LinearRegression()\n",
    "    if len(pts1) >= 2:\n",
    "        model.fit(pts1[['theta']], pts1['r'])\n",
    "        s1 = model.coef_[0]\n",
    "        i1 = model.intercept_\n",
    "        pa1 = calculate_pa(s1, i1)\n",
    "    else:\n",
    "        s1, i1, pa1 = None, None, None\n",
    "    if len(pts2) >= 2:\n",
    "        model.fit(pts2[['theta']], pts2['r'])\n",
    "        s2 = model.coef_[0]\n",
    "        i2 = model.intercept_\n",
    "        pa2 = calculate_pa(s2, i2)\n",
    "    else:\n",
    "        s2, i2, pa2 = None, None, None\n",
    "    seg1 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts1,\n",
    "        'slope': s1,\n",
    "        'intercept': i1,\n",
    "        'pa': pa1\n",
    "    }\n",
    "    seg2 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts2,\n",
    "        'slope': s2,\n",
    "        'intercept': i2,\n",
    "        'pa': pa2\n",
    "    }\n",
    "    return [seg1, seg2]\n",
    "\n",
    "def finalize_groups(groups, max_theta_gap=15.0, flexible_pa_range=(4.0, 30.0)):\n",
    "    \"\"\"\n",
    "    Aplica la subdivisión por gaps y filtra los grupos que cumplan con un rango flexible para pa.\n",
    "    \"\"\"\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap(g, max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    \n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    return valid_groups\n",
    "\n",
    "# ========================\n",
    "# Funciones de Visualización\n",
    "# ========================\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if 'temp_points' in state and not state['temp_points'].empty:\n",
    "        plt.scatter(state['temp_points']['theta'], state['temp_points']['r'], s=10, color='black', marker='x', label='Temp')\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_final_groups(final_groups, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, grp in enumerate(final_groups):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Segmento {idx+1}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Segmentos Finales (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Proceso Integrado\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_integrated(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5,\n",
    "    min_seed_points=60,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    snapshot_interval=10,\n",
    "    max_theta_gap=15.0,\n",
    "    flexible_pa_range=(4.0, 30.0)\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso integrado:\n",
    "      1. Detecta semillas robustas.\n",
    "      2. Asigna iterativamente los puntos restantes a grupos existentes.\n",
    "      3. Realiza una fusión/subdivisión final para obtener los segmentos finales.\n",
    "    \"\"\"\n",
    "    # 1. Detección de semillas\n",
    "    seed_clusters, df_filtered = detect_seeds(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points\n",
    "    )\n",
    "    \n",
    "    # 2. Asignación iterativa de puntos restantes\n",
    "    states, groups, df_filtered = iterative_assignment(\n",
    "        seed_clusters, df_filtered,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval\n",
    "    )\n",
    "    \n",
    "    # 3. Fusión y subdivisión final de grupos\n",
    "    final_groups = finalize_groups(groups, max_theta_gap=max_theta_gap, flexible_pa_range=flexible_pa_range)\n",
    "    \n",
    "    return states, final_groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Ejecución Interactiva\n",
    "# ========================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parámetros de ejemplo (ajusta según tus necesidades)\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.55\n",
    "    theta_min = 20.0\n",
    "    theta_max = 270.0\n",
    "    theta_diff = 4.0\n",
    "    r_diff = 1.0\n",
    "    gap_threshold_theta = 1.5\n",
    "    gap_threshold_r = 1.5\n",
    "    min_seed_points = 50\n",
    "    theta_fit_window = 20.0\n",
    "    r_fit_window = 2.0\n",
    "    radius_tolerance = 0.345\n",
    "    slope_variation_threshold = 0.18\n",
    "    snapshot_interval = 10\n",
    "    max_theta_gap = 8.0\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    \n",
    "    # Ejecutar el proceso integrado\n",
    "    states, final_groups, df_filtered = process_spiral_arms_integrated(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        flexible_pa_range=flexible_pa_range\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de estados intermedios: {len(states)}\")\n",
    "    print(f\"Total de segmentos finales: {len(final_groups)}\")\n",
    "    \n",
    "    # Función interactiva para visualizar estados intermedios\n",
    "    def view_state(state_index):\n",
    "        state = states[state_index]\n",
    "        plot_intermediate_state_cartesian(state, df_filtered)\n",
    "    \n",
    "    interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n",
    "    \n",
    "    # Visualizar los segmentos finales\n",
    "    plot_final_groups(final_groups, df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ff1be0-962e-4c28-a2f9-1264f72ad65e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import deque\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones Auxiliares\n",
    "# ========================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"Calcula la distancia vertical (en r) de un punto a la recta definida por slope e intercept.\"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) de la recta a partir de su pendiente.\n",
    "    Si se dispone de intercepto, se usa para un cálculo ajustado.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Carga y Preprocesamiento de Datos\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold):\n",
    "    \"\"\"\n",
    "    Carga el CSV, calcula coordenadas polares y filtra puntos:\n",
    "      - Por rango de θ (en grados).\n",
    "      - Por percentil en 'rho_resta_final_exp'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# ========================\n",
    "# Detección de Semillas utilizando Grafos y BFS\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo conectando puntos si la diferencia en θ y en r es menor o igual a los umbrales.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Se excluye el mismo punto\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae las componentes conexas (clusters) del grafo usando búsqueda en anchura (BFS).\n",
    "    Cada componente se retorna como un DataFrame.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide el cluster en subclusters \n",
    "    si existe un gap mayor que gap_threshold.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def detect_seeds(id_halo, quartile_threshold=0.55, theta_min=50.0, theta_max=250.0, \n",
    "                 theta_diff=5.0, r_diff=1.0, gap_threshold_theta=2.5, gap_threshold_r=2.5, min_seed_points=60):\n",
    "    \"\"\"\n",
    "    Detecta semillas robustas usando la construcción de grafo, BFS y subdivisión por gaps.\n",
    "    Retorna una lista de DataFrames (cada uno es una semilla) y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    df_filtered = load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold)\n",
    "    \n",
    "    # Construir grafo y extraer componentes conexas\n",
    "    graph = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    # Subdividir cada cluster según gaps en θ y luego en r\n",
    "    seeds_all = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = []\n",
    "            for group in subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r'):\n",
    "                sub_r.append(group)\n",
    "            seeds_all.extend(sub_r)\n",
    "    \n",
    "    # Filtrar semillas que tengan al menos min_seed_points\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) >= min_seed_points]\n",
    "    print(f\"Total de semillas robustas (>= {min_seed_points} puntos): {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a alguna semilla\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    return seed_clusters, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Asignación Iterativa de Puntos a Grupos con Validación Local\n",
    "# ========================\n",
    "\n",
    "def iterative_assignment(seed_clusters, df_filtered, theta_fit_window=30.0, r_fit_window=10.0, \n",
    "                          radius_tolerance=0.45, slope_variation_threshold=0.20, snapshot_interval=10):\n",
    "    \"\"\"\n",
    "    Usa las semillas como grupos iniciales (con ajuste lineal para obtener pendiente, intercepto y ángulo)\n",
    "    y asigna iterativamente los puntos restantes (no pertenecientes a semillas) a estos grupos, validando\n",
    "    que la incorporación de cada punto mantenga la coherencia geométrica.\n",
    "    Retorna estados intermedios y los grupos finales.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    # Inicializar grupos a partir de semillas\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 2:\n",
    "            X_seed = seed[['theta']].values\n",
    "            y_seed = seed['r'].values\n",
    "            model.fit(X_seed, y_seed)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'group_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Puntos restantes que no están en semillas\n",
    "    df_remaining = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    df_remaining = df_remaining.sort_values(by='theta').reset_index(drop=True)\n",
    "    \n",
    "    temp_groups = []  # Para almacenar puntos que inicialmente no se asignan\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    \n",
    "    # Bucle iterativo de asignación\n",
    "    while len(df_remaining) > 0:\n",
    "        point = df_remaining.iloc[0]\n",
    "        df_remaining = df_remaining.iloc[1:].reset_index(drop=True)\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Buscar grupo candidato en el que el punto caiga dentro de una ventana local\n",
    "        for g in groups:\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            if not (g_theta_min <= theta_pt <= g_theta_max and g_r_min <= r_pt <= g_r_max):\n",
    "                continue\n",
    "            # Definir ventana local para ajuste\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            # Agregar el punto y reajustar la recta\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            # Validar que la variación en la pendiente no exceda el umbral\n",
    "            if old_slope is not None and abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) > slope_variation_threshold:\n",
    "                dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if best_distance is None or dist < best_distance:\n",
    "                        best_distance = dist\n",
    "                        best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            # Asigna el punto al grupo y actualiza sus parámetros\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = best_group['points'][(best_group['points']['theta'] >= window_min) & (best_group['points']['theta'] <= window_max)]\n",
    "            if len(subset) < 2:\n",
    "                subset = best_group['points']\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "        else:\n",
    "            # Si no se pudo asignar, se agrega a un grupo temporal\n",
    "            temp_groups.append(point.to_frame().T)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_remaining) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_points': pd.concat(temp_groups, ignore_index=True) if temp_groups else pd.DataFrame(),\n",
    "                'remaining': df_remaining.copy()\n",
    "            })\n",
    "    \n",
    "    # Reintentar asignar puntos temporales a los grupos existentes\n",
    "    if temp_groups:\n",
    "        temp_df = pd.concat(temp_groups, ignore_index=True)\n",
    "        for idx, point in temp_df.iterrows():\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if (g['points']['theta'].min() - theta_fit_window/2 <= point['theta'] <= g['points']['theta'].max() + theta_fit_window/2 and\n",
    "                    g['points']['r'].min() - r_fit_window/2 <= point['r'] <= g['points']['r'].max() + r_fit_window/2):\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    model.fit(g['points'][['theta']], g['points']['r'])\n",
    "                    g['slope'] = model.coef_[0]\n",
    "                    g['intercept'] = model.intercept_\n",
    "                    g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:\n",
    "                # Si aun no se asigna, se puede dejar descartado o manejarlo aparte\n",
    "                pass\n",
    "    \n",
    "    return states, groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Validación de Gaps en θ y en r\n",
    "# ========================\n",
    "\n",
    "def validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r):\n",
    "    \"\"\"\n",
    "    Valida que el grupo no tenga gaps en θ ni en r.\n",
    "    Primero se subdivide por gaps en θ y luego, para cada subgrupo, se subdivide por gaps en r.\n",
    "    Retorna una lista de grupos validados.\n",
    "    \"\"\"\n",
    "    subgroups_theta = subdivide_by_gap(group['points'], gap_threshold_theta, mode='theta')\n",
    "    refined_groups = []\n",
    "    for sg in subgroups_theta:\n",
    "        subgroups_r = subdivide_by_gap(sg, gap_threshold_r, mode='r')\n",
    "        for sgr in subgroups_r:\n",
    "            if len(sgr) >= 2:\n",
    "                model = LinearRegression()\n",
    "                model.fit(sgr[['theta']], sgr['r'])\n",
    "                slope = model.coef_[0]\n",
    "                intercept = model.intercept_\n",
    "                pa = calculate_pa(slope, intercept)\n",
    "            else:\n",
    "                slope, intercept, pa = None, None, None\n",
    "            refined_groups.append({\n",
    "                'group_id': group.get('group_id'),\n",
    "                'points': sgr,\n",
    "                'slope': slope,\n",
    "                'intercept': intercept,\n",
    "                'pa': pa\n",
    "            })\n",
    "    return refined_groups\n",
    "\n",
    "# ========================\n",
    "# Cálculo de la Dispersión\n",
    "# ========================\n",
    "\n",
    "def compute_dispersion(group):\n",
    "    \"\"\"\n",
    "    Calcula la dispersión del grupo como la desviación estándar de las distancias de cada punto\n",
    "    a la recta ajustada (usando calculate_distance).\n",
    "    \"\"\"\n",
    "    if group['slope'] is None or group['points'].empty:\n",
    "        return 0\n",
    "    distances = group['points'].apply(lambda row: calculate_distance(group['slope'], group['intercept'], row['theta'], row['r']), axis=1)\n",
    "    return distances.std()\n",
    "\n",
    "# ========================\n",
    "# Refinamiento Recursivo de Grupos basándose en la Dispersión\n",
    "# ========================\n",
    "\n",
    "def refine_group(group, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    \"\"\"\n",
    "    Refina recursivamente un grupo:\n",
    "      - Primero valida que no tenga gaps en θ ni en r.\n",
    "      - Si la dispersión (std de las distancias) es mayor que dispersion_threshold, se divide el grupo en dos\n",
    "        (por la mediana de θ) y se refina cada subgrupo.\n",
    "    Retorna una lista de grupos refinados.\n",
    "    \"\"\"\n",
    "    # Validar gaps en el grupo\n",
    "    valid_groups = validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r)\n",
    "    refined = []\n",
    "    for vg in valid_groups:\n",
    "        if len(vg['points']) < min_points:\n",
    "            refined.append(vg)\n",
    "            continue\n",
    "        disp = compute_dispersion(vg)\n",
    "        if disp <= dispersion_threshold:\n",
    "            refined.append(vg)\n",
    "        else:\n",
    "            # Dividir el grupo usando la mediana de θ\n",
    "            pts = vg['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "            median_index = len(pts) // 2\n",
    "            group1_pts = pts.iloc[:median_index].copy()\n",
    "            group2_pts = pts.iloc[median_index:].copy()\n",
    "            model = LinearRegression()\n",
    "            if len(group1_pts) >= 2:\n",
    "                model.fit(group1_pts[['theta']], group1_pts['r'])\n",
    "                slope1 = model.coef_[0]\n",
    "                intercept1 = model.intercept_\n",
    "                pa1 = calculate_pa(slope1, intercept1)\n",
    "            else:\n",
    "                slope1, intercept1, pa1 = None, None, None\n",
    "            if len(group2_pts) >= 2:\n",
    "                model.fit(group2_pts[['theta']], group2_pts['r'])\n",
    "                slope2 = model.coef_[0]\n",
    "                intercept2 = model.intercept_\n",
    "                pa2 = calculate_pa(slope2, intercept2)\n",
    "            else:\n",
    "                slope2, intercept2, pa2 = None, None, None\n",
    "            sub_group1 = {\n",
    "                'group_id': vg.get('group_id'),\n",
    "                'points': group1_pts,\n",
    "                'slope': slope1,\n",
    "                'intercept': intercept1,\n",
    "                'pa': pa1\n",
    "            }\n",
    "            sub_group2 = {\n",
    "                'group_id': vg.get('group_id'),\n",
    "                'points': group2_pts,\n",
    "                'slope': slope2,\n",
    "                'intercept': intercept2,\n",
    "                'pa': pa2\n",
    "            }\n",
    "            # Refinar recursivamente cada subgrupo\n",
    "            refined.extend(refine_group(sub_group1, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "            refined.extend(refine_group(sub_group2, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined\n",
    "\n",
    "def refine_all_groups(groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    \"\"\"\n",
    "    Aplica el refinamiento recursivo a todos los grupos de la lista.\n",
    "    \"\"\"\n",
    "    refined_groups = []\n",
    "    for g in groups:\n",
    "        refined_groups.extend(refine_group(g, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined_groups\n",
    "\n",
    "# ========================\n",
    "# Fusión y Subdivisión Final de Grupos (por gap en θ, como paso final)\n",
    "# ========================\n",
    "\n",
    "def subdivide_if_gap_final(group, max_theta_gap=15.0):\n",
    "    \"\"\"\n",
    "    Divide el grupo en dos segmentos si se detecta un gap significativo en la variable θ.\n",
    "    \"\"\"\n",
    "    pts = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_theta_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    model = LinearRegression()\n",
    "    if len(pts1) >= 2:\n",
    "        model.fit(pts1[['theta']], pts1['r'])\n",
    "        s1 = model.coef_[0]\n",
    "        i1 = model.intercept_\n",
    "        pa1 = calculate_pa(s1, i1)\n",
    "    else:\n",
    "        s1, i1, pa1 = None, None, None\n",
    "    if len(pts2) >= 2:\n",
    "        model.fit(pts2[['theta']], pts2['r'])\n",
    "        s2 = model.coef_[0]\n",
    "        i2 = model.intercept_\n",
    "        pa2 = calculate_pa(s2, i2)\n",
    "    else:\n",
    "        s2, i2, pa2 = None, None, None\n",
    "    seg1 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts1,\n",
    "        'slope': s1,\n",
    "        'intercept': i1,\n",
    "        'pa': pa1\n",
    "    }\n",
    "    seg2 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts2,\n",
    "        'slope': s2,\n",
    "        'intercept': i2,\n",
    "        'pa': pa2\n",
    "    }\n",
    "    return [seg1, seg2]\n",
    "\n",
    "def finalize_groups(groups, max_theta_gap=15.0, flexible_pa_range=(4.0, 30.0)):\n",
    "    \"\"\"\n",
    "    Aplica una subdivisión final por gaps en θ y filtra los grupos que cumplan con un rango flexible para pa.\n",
    "    \"\"\"\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap_final(g, max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    \n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    return valid_groups\n",
    "\n",
    "# ========================\n",
    "# Funciones de Visualización\n",
    "# ========================\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if 'temp_points' in state and not state['temp_points'].empty:\n",
    "        plt.scatter(state['temp_points']['theta'], state['temp_points']['r'], s=10, color='black', marker='x', label='Temp')\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_final_groups(final_groups, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, grp in enumerate(final_groups):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Segmento {idx+1}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Segmentos Finales (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Proceso Integrado\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_integrated(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5,\n",
    "    min_seed_points=60,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    snapshot_interval=10,\n",
    "    max_theta_gap=15.0,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    dispersion_threshold=0.35  # Parámetro para decidir si un grupo es muy disperso\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso integrado:\n",
    "      1. Detecta semillas robustas.\n",
    "      2. Asigna iterativamente los puntos restantes a grupos existentes.\n",
    "      3. Aplica una fusión/subdivisión final.\n",
    "      4. Refina recursivamente los grupos que tengan alta dispersión, validando que no tengan gaps en r ni en θ.\n",
    "    \"\"\"\n",
    "    # 1. Detección de semillas\n",
    "    seed_clusters, df_filtered = detect_seeds(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points\n",
    "    )\n",
    "    \n",
    "    # 2. Asignación iterativa de puntos restantes\n",
    "    states, groups, df_filtered = iterative_assignment(\n",
    "        seed_clusters, df_filtered,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval\n",
    "    )\n",
    "    \n",
    "    # 3. Fusión y subdivisión final de grupos (por gaps en θ)\n",
    "    preliminary_groups = finalize_groups(groups, max_theta_gap=max_theta_gap, flexible_pa_range=flexible_pa_range)\n",
    "    \n",
    "    # 4. Refinamiento recursivo basado en la dispersión y validación de gaps en θ y r\n",
    "    refined_groups = refine_all_groups(preliminary_groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r)\n",
    "    \n",
    "    return states, refined_groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Ejecución Interactiva\n",
    "# ========================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parámetros de ejemplo (ajusta según tus necesidades)\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.55\n",
    "    theta_min = 20.0\n",
    "    theta_max = 270.0\n",
    "    theta_diff = 4.0\n",
    "    r_diff = 1.0\n",
    "    gap_threshold_theta = 1.5\n",
    "    gap_threshold_r = 1.5\n",
    "    min_seed_points = 50\n",
    "    theta_fit_window = 20.0\n",
    "    r_fit_window = 2.0\n",
    "    radius_tolerance = 0.345\n",
    "    slope_variation_threshold = 0.18\n",
    "    snapshot_interval = 10\n",
    "    max_theta_gap = 8.0\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    dispersion_threshold = 0.35  # Ajusta según la escala de las distancias\n",
    "    \n",
    "    # Ejecutar el proceso integrado\n",
    "    states, final_groups, df_filtered = process_spiral_arms_integrated(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        dispersion_threshold=dispersion_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de estados intermedios: {len(states)}\")\n",
    "    print(f\"Total de segmentos finales refinados: {len(final_groups)}\")\n",
    "    \n",
    "    # Función interactiva para visualizar estados intermedios\n",
    "    def view_state(state_index):\n",
    "        state = states[state_index]\n",
    "        plot_intermediate_state_cartesian(state, df_filtered)\n",
    "    \n",
    "    interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n",
    "    \n",
    "    # Visualizar los segmentos finales refinados\n",
    "    plot_final_groups(final_groups, df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "451a271f-583d-470f-9cba-3257d1233116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import deque\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones Auxiliares\n",
    "# ========================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"Calcula la distancia vertical (en r) de un punto a la recta definida por slope e intercept.\"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) de la recta a partir de su pendiente.\n",
    "    Si se dispone de intercepto, se usa para un cálculo ajustado.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Carga y Preprocesamiento de Datos\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold):\n",
    "    \"\"\"\n",
    "    Carga el CSV, calcula coordenadas polares y filtra puntos:\n",
    "      - Por rango de θ (en grados).\n",
    "      - Por percentil en 'rho_resta_final_exp'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# ========================\n",
    "# Detección de Semillas utilizando Grafos y BFS\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo conectando puntos si la diferencia en θ y en r es menor o igual a los umbrales.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Se excluye el mismo punto\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae las componentes conexas (clusters) del grafo usando búsqueda en anchura (BFS).\n",
    "    Cada componente se retorna como un DataFrame.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide el cluster en subclusters \n",
    "    si existe un gap mayor que gap_threshold.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def detect_seeds(id_halo, quartile_threshold=0.55, theta_min=50.0, theta_max=250.0, \n",
    "                 theta_diff=5.0, r_diff=1.0, gap_threshold_theta=2.5, gap_threshold_r=2.5, min_seed_points=60):\n",
    "    \"\"\"\n",
    "    Detecta semillas robustas usando la construcción de grafo, BFS y subdivisión por gaps.\n",
    "    Retorna una lista de DataFrames (cada uno es una semilla) y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    df_filtered = load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold)\n",
    "    \n",
    "    # Construir grafo y extraer componentes conexas\n",
    "    graph = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    # Subdividir cada cluster según gaps en θ y luego en r\n",
    "    seeds_all = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = []\n",
    "            for group in subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r'):\n",
    "                sub_r.append(group)\n",
    "            seeds_all.extend(sub_r)\n",
    "    \n",
    "    # Filtrar semillas que tengan al menos min_seed_points\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) >= min_seed_points]\n",
    "    print(f\"Total de semillas robustas (>= {min_seed_points} puntos): {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a alguna semilla\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    return seed_clusters, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Asignación Iterativa de Puntos a Grupos con Validación Local\n",
    "# ========================\n",
    "\n",
    "def iterative_assignment(seed_clusters, df_filtered, theta_fit_window=30.0, r_fit_window=10.0, \n",
    "                          radius_tolerance=0.45, slope_variation_threshold=0.20, snapshot_interval=10):\n",
    "    \"\"\"\n",
    "    Usa las semillas como grupos iniciales (con ajuste lineal para obtener pendiente, intercepto y ángulo)\n",
    "    y asigna iterativamente los puntos restantes (no pertenecientes a semillas) a estos grupos, validando\n",
    "    que la incorporación de cada punto mantenga la coherencia geométrica.\n",
    "    Retorna estados intermedios y los grupos finales.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    # Inicializar grupos a partir de semillas\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 2:\n",
    "            X_seed = seed[['theta']].values\n",
    "            y_seed = seed['r'].values\n",
    "            model.fit(X_seed, y_seed)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'group_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Puntos restantes que no están en semillas\n",
    "    df_remaining = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    df_remaining = df_remaining.sort_values(by='theta').reset_index(drop=True)\n",
    "    \n",
    "    temp_groups = []  # Para almacenar puntos que inicialmente no se asignan\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    \n",
    "    # Bucle iterativo de asignación\n",
    "    while len(df_remaining) > 0:\n",
    "        point = df_remaining.iloc[0]\n",
    "        df_remaining = df_remaining.iloc[1:].reset_index(drop=True)\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Buscar grupo candidato en el que el punto caiga dentro de una ventana local\n",
    "        for g in groups:\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            if not (g_theta_min <= theta_pt <= g_theta_max and g_r_min <= r_pt <= g_r_max):\n",
    "                continue\n",
    "            # Definir ventana local para ajuste\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            # Agregar el punto y reajustar la recta\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            # Validar que la variación en la pendiente no exceda el umbral\n",
    "            if old_slope is not None and abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) > slope_variation_threshold:\n",
    "                dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if best_distance is None or dist < best_distance:\n",
    "                        best_distance = dist\n",
    "                        best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            # Asigna el punto al grupo y actualiza sus parámetros\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = best_group['points'][(best_group['points']['theta'] >= window_min) & (best_group['points']['theta'] <= window_max)]\n",
    "            if len(subset) < 2:\n",
    "                subset = best_group['points']\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "        else:\n",
    "            # Si no se pudo asignar, se agrega a un grupo temporal\n",
    "            temp_groups.append(point.to_frame().T)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_remaining) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_points': pd.concat(temp_groups, ignore_index=True) if temp_groups else pd.DataFrame(),\n",
    "                'remaining': df_remaining.copy()\n",
    "            })\n",
    "    \n",
    "    # Reintentar asignar puntos temporales a los grupos existentes\n",
    "    if temp_groups:\n",
    "        temp_df = pd.concat(temp_groups, ignore_index=True)\n",
    "        for idx, point in temp_df.iterrows():\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if (g['points']['theta'].min() - theta_fit_window/2 <= point['theta'] <= g['points']['theta'].max() + theta_fit_window/2 and\n",
    "                    g['points']['r'].min() - r_fit_window/2 <= point['r'] <= g['points']['r'].max() + r_fit_window/2):\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    model.fit(g['points'][['theta']], g['points']['r'])\n",
    "                    g['slope'] = model.coef_[0]\n",
    "                    g['intercept'] = model.intercept_\n",
    "                    g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:\n",
    "                pass\n",
    "    \n",
    "    return states, groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Validación de Gaps en θ y en r\n",
    "# ========================\n",
    "\n",
    "def validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r):\n",
    "    \"\"\"\n",
    "    Valida que el grupo no tenga gaps en θ ni en r.\n",
    "    Primero se subdivide por gaps en θ y luego, para cada subgrupo, se subdivide por gaps en r.\n",
    "    Retorna una lista de grupos validados.\n",
    "    \"\"\"\n",
    "    subgroups_theta = subdivide_by_gap(group['points'], gap_threshold_theta, mode='theta')\n",
    "    refined_groups = []\n",
    "    for sg in subgroups_theta:\n",
    "        subgroups_r = subdivide_by_gap(sg, gap_threshold_r, mode='r')\n",
    "        for sgr in subgroups_r:\n",
    "            if len(sgr) >= 2:\n",
    "                model = LinearRegression()\n",
    "                model.fit(sgr[['theta']], sgr['r'])\n",
    "                slope = model.coef_[0]\n",
    "                intercept = model.intercept_\n",
    "                pa = calculate_pa(slope, intercept)\n",
    "            else:\n",
    "                slope, intercept, pa = None, None, None\n",
    "            refined_groups.append({\n",
    "                'group_id': group.get('group_id'),\n",
    "                'points': sgr,\n",
    "                'slope': slope,\n",
    "                'intercept': intercept,\n",
    "                'pa': pa\n",
    "            })\n",
    "    return refined_groups\n",
    "\n",
    "# ========================\n",
    "# Cálculo de la Dispersión\n",
    "# ========================\n",
    "\n",
    "def compute_dispersion(group):\n",
    "    \"\"\"\n",
    "    Calcula la dispersión del grupo como la desviación estándar de las distancias de cada punto\n",
    "    a la recta ajustada (usando calculate_distance).\n",
    "    \"\"\"\n",
    "    if group['slope'] is None or group['points'].empty:\n",
    "        return 0\n",
    "    distances = group['points'].apply(lambda row: calculate_distance(group['slope'], group['intercept'], row['theta'], row['r']), axis=1)\n",
    "    return distances.std()\n",
    "\n",
    "# ========================\n",
    "# Refinamiento Recursivo de Grupos basándose en la Dispersión\n",
    "# ========================\n",
    "\n",
    "def refine_group(group, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    \"\"\"\n",
    "    Refina recursivamente un grupo:\n",
    "      - Primero valida que no tenga gaps en θ ni en r.\n",
    "      - Si la dispersión (std de las distancias) es mayor que dispersion_threshold, se divide el grupo en dos\n",
    "        (por la mediana de θ) y se refina cada subgrupo.\n",
    "    Retorna una lista de grupos refinados.\n",
    "    \"\"\"\n",
    "    # Validar gaps en el grupo\n",
    "    valid_groups = validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r)\n",
    "    refined = []\n",
    "    for vg in valid_groups:\n",
    "        if len(vg['points']) < min_points:\n",
    "            refined.append(vg)\n",
    "            continue\n",
    "        disp = compute_dispersion(vg)\n",
    "        if disp <= dispersion_threshold:\n",
    "            refined.append(vg)\n",
    "        else:\n",
    "            # Dividir el grupo usando la mediana de θ\n",
    "            pts = vg['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "            median_index = len(pts) // 2\n",
    "            group1_pts = pts.iloc[:median_index].copy()\n",
    "            group2_pts = pts.iloc[median_index:].copy()\n",
    "            model = LinearRegression()\n",
    "            if len(group1_pts) >= 2:\n",
    "                model.fit(group1_pts[['theta']], group1_pts['r'])\n",
    "                slope1 = model.coef_[0]\n",
    "                intercept1 = model.intercept_\n",
    "                pa1 = calculate_pa(slope1, intercept1)\n",
    "            else:\n",
    "                slope1, intercept1, pa1 = None, None, None\n",
    "            if len(group2_pts) >= 2:\n",
    "                model.fit(group2_pts[['theta']], group2_pts['r'])\n",
    "                slope2 = model.coef_[0]\n",
    "                intercept2 = model.intercept_\n",
    "                pa2 = calculate_pa(slope2, intercept2)\n",
    "            else:\n",
    "                slope2, intercept2, pa2 = None, None, None\n",
    "            sub_group1 = {\n",
    "                'group_id': vg.get('group_id'),\n",
    "                'points': group1_pts,\n",
    "                'slope': slope1,\n",
    "                'intercept': intercept1,\n",
    "                'pa': pa1\n",
    "            }\n",
    "            sub_group2 = {\n",
    "                'group_id': vg.get('group_id'),\n",
    "                'points': group2_pts,\n",
    "                'slope': slope2,\n",
    "                'intercept': intercept2,\n",
    "                'pa': pa2\n",
    "            }\n",
    "            # Refinar recursivamente cada subgrupo\n",
    "            refined.extend(refine_group(sub_group1, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "            refined.extend(refine_group(sub_group2, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined\n",
    "\n",
    "def refine_all_groups(groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    \"\"\"\n",
    "    Aplica el refinamiento recursivo a todos los grupos de la lista.\n",
    "    \"\"\"\n",
    "    refined_groups = []\n",
    "    for g in groups:\n",
    "        refined_groups.extend(refine_group(g, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined_groups\n",
    "\n",
    "# ========================\n",
    "# Fusión y Subdivisión Final de Grupos (por gap en θ, como paso final)\n",
    "# ========================\n",
    "\n",
    "def subdivide_if_gap_final(group, max_theta_gap=15.0):\n",
    "    \"\"\"\n",
    "    Divide el grupo en dos segmentos si se detecta un gap significativo en la variable θ.\n",
    "    \"\"\"\n",
    "    pts = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_theta_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    model = LinearRegression()\n",
    "    if len(pts1) >= 2:\n",
    "        model.fit(pts1[['theta']], pts1['r'])\n",
    "        s1 = model.coef_[0]\n",
    "        i1 = model.intercept_\n",
    "        pa1 = calculate_pa(s1, i1)\n",
    "    else:\n",
    "        s1, i1, pa1 = None, None, None\n",
    "    if len(pts2) >= 2:\n",
    "        model.fit(pts2[['theta']], pts2['r'])\n",
    "        s2 = model.coef_[0]\n",
    "        i2 = model.intercept_\n",
    "        pa2 = calculate_pa(s2, i2)\n",
    "    else:\n",
    "        s2, i2, pa2 = None, None, None\n",
    "    seg1 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts1,\n",
    "        'slope': s1,\n",
    "        'intercept': i1,\n",
    "        'pa': pa1\n",
    "    }\n",
    "    seg2 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts2,\n",
    "        'slope': s2,\n",
    "        'intercept': i2,\n",
    "        'pa': pa2\n",
    "    }\n",
    "    return [seg1, seg2]\n",
    "\n",
    "def finalize_groups(groups, max_theta_gap=15.0, flexible_pa_range=(4.0, 30.0)):\n",
    "    \"\"\"\n",
    "    Aplica una subdivisión final por gaps en θ y filtra los grupos que cumplan con un rango flexible para pa.\n",
    "    \"\"\"\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap_final(g, max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    \n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    return valid_groups\n",
    "\n",
    "# ========================\n",
    "# Funciones de Visualización\n",
    "# ========================\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if 'temp_points' in state and not state['temp_points'].empty:\n",
    "        plt.scatter(state['temp_points']['theta'], state['temp_points']['r'], s=10, color='black', marker='x', label='Temp')\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_final_groups(final_groups, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, grp in enumerate(final_groups):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Segmento {idx+1}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Segmentos Finales (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Proceso Integrado\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_integrated(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5,\n",
    "    min_seed_points=60,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    snapshot_interval=10,\n",
    "    max_theta_gap=15.0,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    dispersion_threshold=0.35  # Parámetro para decidir si un grupo es muy disperso\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso integrado:\n",
    "      1. Detecta semillas robustas.\n",
    "      2. Asigna iterativamente los puntos restantes a grupos existentes.\n",
    "      3. Aplica una fusión/subdivisión final.\n",
    "      4. Refina recursivamente los grupos que tengan alta dispersión, validando que no tengan gaps en r ni en θ.\n",
    "      5. Al final, almacena en un diccionario aquellos grupos que tienen buena dispersión y cumplen los criterios.\n",
    "    \"\"\"\n",
    "    # 1. Detección de semillas\n",
    "    seed_clusters, df_filtered = detect_seeds(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points\n",
    "    )\n",
    "    \n",
    "    # 2. Asignación iterativa de puntos restantes\n",
    "    states, groups, df_filtered = iterative_assignment(\n",
    "        seed_clusters, df_filtered,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval\n",
    "    )\n",
    "    \n",
    "    # 3. Fusión y subdivisión final de grupos (por gaps en θ)\n",
    "    preliminary_groups = finalize_groups(groups, max_theta_gap=max_theta_gap, flexible_pa_range=flexible_pa_range)\n",
    "    \n",
    "    # 4. Refinamiento recursivo basado en la dispersión y validación de gaps en θ y r\n",
    "    refined_groups = refine_all_groups(preliminary_groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r)\n",
    "    \n",
    "    # 5. Almacenar en un diccionario los grupos que tienen buena dispersión y cumplen criterios\n",
    "    good_groups = {}\n",
    "    good_counter = 0\n",
    "    for g in refined_groups:\n",
    "        disp = compute_dispersion(g)\n",
    "        # Se consideran \"buenos\" aquellos grupos cuya dispersión sea menor o igual al umbral\n",
    "        # y cuyo ángulo (pa) esté en el rango flexible\n",
    "        if disp <= dispersion_threshold and g['slope'] is not None and g['intercept'] is not None and \\\n",
    "           g['pa'] is not None and flexible_pa_range[0] <= abs(g['pa']) <= flexible_pa_range[1]:\n",
    "            good_groups[good_counter] = {'group': g, 'dispersion': disp}\n",
    "            good_counter += 1\n",
    "    \n",
    "    return states, refined_groups, good_groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Ejecución Interactiva\n",
    "# ========================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parámetros de ejemplo (ajusta según tus necesidades)\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.55\n",
    "    theta_min = 20.0\n",
    "    theta_max = 270.0\n",
    "    theta_diff = 4.0\n",
    "    r_diff = 1.0\n",
    "    gap_threshold_theta = 1.5\n",
    "    gap_threshold_r = 1.5\n",
    "    min_seed_points = 50\n",
    "    theta_fit_window = 20.0\n",
    "    r_fit_window = 2.0\n",
    "    radius_tolerance = 0.345\n",
    "    slope_variation_threshold = 0.18\n",
    "    snapshot_interval = 10\n",
    "    max_theta_gap = 8.0\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    dispersion_threshold = 0.35  # Ajusta según la escala de las distancias\n",
    "    \n",
    "    # Ejecutar el proceso integrado\n",
    "    states, final_groups, good_groups, df_filtered = process_spiral_arms_integrated(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        dispersion_threshold=dispersion_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de estados intermedios: {len(states)}\")\n",
    "    print(f\"Total de segmentos finales refinados: {len(final_groups)}\")\n",
    "    print(f\"Total de grupos con buena dispersión: {len(good_groups)}\")\n",
    "    \n",
    "    # Función interactiva para visualizar estados intermedios\n",
    "    def view_state(state_index):\n",
    "        state = states[state_index]\n",
    "        plot_intermediate_state_cartesian(state, df_filtered)\n",
    "    \n",
    "    interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n",
    "    \n",
    "    # Visualizar los segmentos finales refinados\n",
    "    plot_final_groups(final_groups, df_filtered)\n",
    "    \n",
    "    # Visualizar los grupos con buena dispersión (convertidos a lista)\n",
    "    good_groups_list = [entry['group'] for entry in good_groups.values()]\n",
    "    if good_groups_list:\n",
    "        print(\"Mostrando grupos con buena dispersión:\")\n",
    "        plot_final_groups(good_groups_list, df_filtered)\n",
    "    else:\n",
    "        print(\"No se encontraron grupos que cumplan con los criterios de buena dispersión.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d101a134-62da-4393-9ba7-9b38a243abe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import deque\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones Auxiliares\n",
    "# ========================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"Calcula la distancia vertical (en r) de un punto a la recta definida por slope e intercept.\"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) de la recta a partir de su pendiente.\n",
    "    Si se dispone de intercepto, se usa para un cálculo ajustado.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Carga y Preprocesamiento de Datos\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold):\n",
    "    \"\"\"\n",
    "    Carga el CSV, calcula coordenadas polares y filtra puntos:\n",
    "      - Por rango de θ (en grados).\n",
    "      - Por percentil en 'rho_resta_final_exp'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# ========================\n",
    "# Detección de Semillas utilizando Grafos y BFS\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo conectando puntos si la diferencia en θ y en r es menor o igual a los umbrales.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Se excluye el mismo punto\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae las componentes conexas (clusters) del grafo usando búsqueda en anchura (BFS).\n",
    "    Cada componente se retorna como un DataFrame.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide el cluster en subclusters \n",
    "    si existe un gap mayor que gap_threshold.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def detect_seeds(id_halo, quartile_threshold=0.55, theta_min=50.0, theta_max=250.0, \n",
    "                 theta_diff=5.0, r_diff=1.0, gap_threshold_theta=2.5, gap_threshold_r=2.5, min_seed_points=60):\n",
    "    \"\"\"\n",
    "    Detecta semillas robustas usando la construcción de grafo, BFS y subdivisión por gaps.\n",
    "    Retorna una lista de DataFrames (cada uno es una semilla) y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    df_filtered = load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold)\n",
    "    \n",
    "    # Construir grafo y extraer componentes conexas\n",
    "    graph = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    # Subdividir cada cluster según gaps en θ y luego en r\n",
    "    seeds_all = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = []\n",
    "            for group in subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r'):\n",
    "                sub_r.append(group)\n",
    "            seeds_all.extend(sub_r)\n",
    "    \n",
    "    # Filtrar semillas que tengan al menos min_seed_points\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) >= min_seed_points]\n",
    "    print(f\"Total de semillas robustas (>= {min_seed_points} puntos): {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a alguna semilla\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    return seed_clusters, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Asignación Iterativa de Puntos a Grupos con Validación Local\n",
    "# ========================\n",
    "\n",
    "def iterative_assignment(seed_clusters, df_filtered, theta_fit_window=30.0, r_fit_window=10.0, \n",
    "                          radius_tolerance=0.45, slope_variation_threshold=0.20, snapshot_interval=10):\n",
    "    \"\"\"\n",
    "    Usa las semillas como grupos iniciales (con ajuste lineal para obtener pendiente, intercepto y ángulo)\n",
    "    y asigna iterativamente los puntos restantes (no pertenecientes a semillas) a estos grupos, validando\n",
    "    que la incorporación de cada punto mantenga la coherencia geométrica.\n",
    "    Retorna estados intermedios y los grupos finales.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    # Inicializar grupos a partir de semillas\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 2:\n",
    "            X_seed = seed[['theta']].values\n",
    "            y_seed = seed['r'].values\n",
    "            model.fit(X_seed, y_seed)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'group_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Puntos restantes que no están en semillas\n",
    "    df_remaining = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    df_remaining = df_remaining.sort_values(by='theta').reset_index(drop=True)\n",
    "    \n",
    "    temp_groups = []  # Para almacenar puntos que inicialmente no se asignan\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    \n",
    "    # Bucle iterativo de asignación\n",
    "    while len(df_remaining) > 0:\n",
    "        point = df_remaining.iloc[0]\n",
    "        df_remaining = df_remaining.iloc[1:].reset_index(drop=True)\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Buscar grupo candidato en el que el punto caiga dentro de una ventana local\n",
    "        for g in groups:\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            if not (g_theta_min <= theta_pt <= g_theta_max and g_r_min <= r_pt <= g_r_max):\n",
    "                continue\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            if old_slope is not None and abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) > slope_variation_threshold:\n",
    "                dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if best_distance is None or dist < best_distance:\n",
    "                        best_distance = dist\n",
    "                        best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = best_group['points'][(best_group['points']['theta'] >= window_min) & (best_group['points']['theta'] <= window_max)]\n",
    "            if len(subset) < 2:\n",
    "                subset = best_group['points']\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "        else:\n",
    "            temp_groups.append(point.to_frame().T)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_remaining) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_points': pd.concat(temp_groups, ignore_index=True) if temp_groups else pd.DataFrame(),\n",
    "                'remaining': df_remaining.copy()\n",
    "            })\n",
    "    \n",
    "    if temp_groups:\n",
    "        temp_df = pd.concat(temp_groups, ignore_index=True)\n",
    "        for idx, point in temp_df.iterrows():\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if (g['points']['theta'].min() - theta_fit_window/2 <= point['theta'] <= g['points']['theta'].max() + theta_fit_window/2 and\n",
    "                    g['points']['r'].min() - r_fit_window/2 <= point['r'] <= g['points']['r'].max() + r_fit_window/2):\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    model.fit(g['points'][['theta']], g['points']['r'])\n",
    "                    g['slope'] = model.coef_[0]\n",
    "                    g['intercept'] = model.intercept_\n",
    "                    g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:\n",
    "                pass\n",
    "    \n",
    "    return states, groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Validación de Gaps en θ y en r\n",
    "# ========================\n",
    "\n",
    "def validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r):\n",
    "    \"\"\"\n",
    "    Valida que el grupo no tenga gaps en θ ni en r.\n",
    "    Primero se subdivide por gaps en θ y luego, para cada subgrupo, se subdivide por gaps en r.\n",
    "    Retorna una lista de grupos validados.\n",
    "    \"\"\"\n",
    "    subgroups_theta = subdivide_by_gap(group['points'], gap_threshold_theta, mode='theta')\n",
    "    refined_groups = []\n",
    "    for sg in subgroups_theta:\n",
    "        subgroups_r = subdivide_by_gap(sg, gap_threshold_r, mode='r')\n",
    "        for sgr in subgroups_r:\n",
    "            if len(sgr) >= 2:\n",
    "                model = LinearRegression()\n",
    "                model.fit(sgr[['theta']], sgr['r'])\n",
    "                slope = model.coef_[0]\n",
    "                intercept = model.intercept_\n",
    "                pa = calculate_pa(slope, intercept)\n",
    "            else:\n",
    "                slope, intercept, pa = None, None, None\n",
    "            refined_groups.append({\n",
    "                'group_id': group.get('group_id'),\n",
    "                'points': sgr,\n",
    "                'slope': slope,\n",
    "                'intercept': intercept,\n",
    "                'pa': pa\n",
    "            })\n",
    "    return refined_groups\n",
    "\n",
    "# ========================\n",
    "# Cálculo de la Dispersión\n",
    "# ========================\n",
    "\n",
    "def compute_dispersion(group):\n",
    "    \"\"\"\n",
    "    Calcula la dispersión del grupo como la desviación estándar de las distancias de cada punto\n",
    "    a la recta ajustada (usando calculate_distance).\n",
    "    \"\"\"\n",
    "    if group['slope'] is None or group['points'].empty:\n",
    "        return 0\n",
    "    distances = group['points'].apply(lambda row: calculate_distance(group['slope'], group['intercept'], row['theta'], row['r']), axis=1)\n",
    "    return distances.std()\n",
    "\n",
    "# ========================\n",
    "# Refinamiento Recursivo de Grupos basándose en la Dispersión\n",
    "# ========================\n",
    "\n",
    "def refine_group(group, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    \"\"\"\n",
    "    Refina recursivamente un grupo:\n",
    "      - Primero valida que no tenga gaps en θ ni en r.\n",
    "      - Si la dispersión es mayor que dispersion_threshold, se divide el grupo en dos (por la mediana de θ)\n",
    "        y se refina cada subgrupo.\n",
    "    Retorna una lista de grupos refinados.\n",
    "    \"\"\"\n",
    "    valid_groups = validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r)\n",
    "    refined = []\n",
    "    for vg in valid_groups:\n",
    "        if len(vg['points']) < min_points:\n",
    "            refined.append(vg)\n",
    "            continue\n",
    "        disp = compute_dispersion(vg)\n",
    "        if disp <= dispersion_threshold:\n",
    "            refined.append(vg)\n",
    "        else:\n",
    "            pts = vg['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "            median_index = len(pts) // 2\n",
    "            group1_pts = pts.iloc[:median_index].copy()\n",
    "            group2_pts = pts.iloc[median_index:].copy()\n",
    "            model = LinearRegression()\n",
    "            if len(group1_pts) >= 2:\n",
    "                model.fit(group1_pts[['theta']], group1_pts['r'])\n",
    "                slope1 = model.coef_[0]\n",
    "                intercept1 = model.intercept_\n",
    "                pa1 = calculate_pa(slope1, intercept1)\n",
    "            else:\n",
    "                slope1, intercept1, pa1 = None, None, None\n",
    "            if len(group2_pts) >= 2:\n",
    "                model.fit(group2_pts[['theta']], group2_pts['r'])\n",
    "                slope2 = model.coef_[0]\n",
    "                intercept2 = model.intercept_\n",
    "                pa2 = calculate_pa(slope2, intercept2)\n",
    "            else:\n",
    "                slope2, intercept2, pa2 = None, None, None\n",
    "            sub_group1 = {\n",
    "                'group_id': vg.get('group_id'),\n",
    "                'points': group1_pts,\n",
    "                'slope': slope1,\n",
    "                'intercept': intercept1,\n",
    "                'pa': pa1\n",
    "            }\n",
    "            sub_group2 = {\n",
    "                'group_id': vg.get('group_id'),\n",
    "                'points': group2_pts,\n",
    "                'slope': slope2,\n",
    "                'intercept': intercept2,\n",
    "                'pa': pa2\n",
    "            }\n",
    "            refined.extend(refine_group(sub_group1, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "            refined.extend(refine_group(sub_group2, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined\n",
    "\n",
    "def refine_all_groups(groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    \"\"\"\n",
    "    Aplica el refinamiento recursivo a todos los grupos de la lista.\n",
    "    \"\"\"\n",
    "    refined_groups = []\n",
    "    for g in groups:\n",
    "        refined_groups.extend(refine_group(g, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined_groups\n",
    "\n",
    "# ========================\n",
    "# Fusión y Subdivisión Final de Grupos (por gap en θ)\n",
    "# ========================\n",
    "\n",
    "def subdivide_if_gap_final(group, max_theta_gap=15.0):\n",
    "    \"\"\"\n",
    "    Divide el grupo en dos segmentos si se detecta un gap significativo en la variable θ.\n",
    "    \"\"\"\n",
    "    pts = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_theta_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    model = LinearRegression()\n",
    "    if len(pts1) >= 2:\n",
    "        model.fit(pts1[['theta']], pts1['r'])\n",
    "        s1 = model.coef_[0]\n",
    "        i1 = model.intercept_\n",
    "        pa1 = calculate_pa(s1, i1)\n",
    "    else:\n",
    "        s1, i1, pa1 = None, None, None\n",
    "    if len(pts2) >= 2:\n",
    "        model.fit(pts2[['theta']], pts2['r'])\n",
    "        s2 = model.coef_[0]\n",
    "        i2 = model.intercept_\n",
    "        pa2 = calculate_pa(s2, i2)\n",
    "    else:\n",
    "        s2, i2, pa2 = None, None, None\n",
    "    seg1 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts1,\n",
    "        'slope': s1,\n",
    "        'intercept': i1,\n",
    "        'pa': pa1\n",
    "    }\n",
    "    seg2 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts2,\n",
    "        'slope': s2,\n",
    "        'intercept': i2,\n",
    "        'pa': pa2\n",
    "    }\n",
    "    return [seg1, seg2]\n",
    "\n",
    "def finalize_groups(groups, max_theta_gap=15.0, flexible_pa_range=(4.0, 30.0)):\n",
    "    \"\"\"\n",
    "    Aplica una subdivisión final por gaps en θ y filtra los grupos que cumplan con un rango flexible para pa.\n",
    "    \"\"\"\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap_final(g, max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    \n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    return valid_groups\n",
    "\n",
    "# ========================\n",
    "# Extrapolación de Good Groups\n",
    "# ========================\n",
    "\n",
    "def extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.28, theta_relative_margin=0.28, r_relative_margin=0.28):\n",
    "    \"\"\"\n",
    "    Para cada good group, se revisan los grupos candidatos en refined_groups (que no sean el mismo)\n",
    "    y si la diferencia relativa en pendiente es ≤ slope_variation_merge y los rangos de θ y r se solapan\n",
    "    dentro de un margen relativo (theta_relative_margin y r_relative_margin), se fusionan.\n",
    "    Se recalculan los parámetros y se actualiza el good group.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    updated_good_groups = {}\n",
    "    for key, entry in good_groups.items():\n",
    "        good_group = entry['group']\n",
    "        good_theta_min = good_group['points']['theta'].min()\n",
    "        good_theta_max = good_group['points']['theta'].max()\n",
    "        theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "        \n",
    "        good_r_min = good_group['points']['r'].min()\n",
    "        good_r_max = good_group['points']['r'].max()\n",
    "        r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "        \n",
    "        for candidate in refined_groups:\n",
    "            if candidate is good_group:\n",
    "                continue\n",
    "            if good_group['slope'] is None or candidate['slope'] is None:\n",
    "                continue\n",
    "            relative_diff = abs(candidate['slope'] - good_group['slope']) / (abs(good_group['slope']) + 1e-12)\n",
    "            if relative_diff <= slope_variation_merge:\n",
    "                cand_theta_min = candidate['points']['theta'].min()\n",
    "                cand_theta_max = candidate['points']['theta'].max()\n",
    "                cand_r_min = candidate['points']['r'].min()\n",
    "                cand_r_max = candidate['points']['r'].max()\n",
    "                theta_condition = (cand_theta_min >= good_theta_min - theta_relative_margin * theta_range and\n",
    "                                   cand_theta_max <= good_theta_max + theta_relative_margin * theta_range)\n",
    "                r_condition = (cand_r_min >= good_r_min - r_relative_margin * r_range and\n",
    "                               cand_r_max <= good_r_max + r_relative_margin * r_range)\n",
    "                if theta_condition and r_condition:\n",
    "                    merged_points = pd.concat([good_group['points'], candidate['points']], ignore_index=True)\n",
    "                    if len(merged_points) >= 2:\n",
    "                        model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                        new_slope = model.coef_[0]\n",
    "                        new_intercept = model.intercept_\n",
    "                        new_pa = calculate_pa(new_slope, new_intercept)\n",
    "                    else:\n",
    "                        new_slope, new_intercept, new_pa = good_group['slope'], good_group['intercept'], good_group['pa']\n",
    "                    good_group['points'] = merged_points\n",
    "                    good_group['slope'] = new_slope\n",
    "                    good_group['intercept'] = new_intercept\n",
    "                    good_group['pa'] = new_pa\n",
    "                    good_theta_min = good_group['points']['theta'].min()\n",
    "                    good_theta_max = good_group['points']['theta'].max()\n",
    "                    theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "                    good_r_min = good_group['points']['r'].min()\n",
    "                    good_r_max = good_group['points']['r'].max()\n",
    "                    r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "        updated_good_groups[key] = {'group': good_group, 'dispersion': compute_dispersion(good_group)}\n",
    "    return updated_good_groups\n",
    "\n",
    "# ========================\n",
    "# Funciones de Visualización\n",
    "# ========================\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if 'temp_points' in state and not state['temp_points'].empty:\n",
    "        plt.scatter(state['temp_points']['theta'], state['temp_points']['r'], s=10, color='black', marker='x', label='Temp')\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_final_groups(final_groups, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, grp in enumerate(final_groups):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Segmento {idx+1}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Segmentos Finales (Plano r, θ)\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Proceso Integrado\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_integrated(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5,\n",
    "    min_seed_points=60,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    snapshot_interval=10,\n",
    "    max_theta_gap=15.0,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    dispersion_threshold=0.35  # Umbral para dispersión\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso integrado:\n",
    "      1. Detecta semillas robustas.\n",
    "      2. Asigna iterativamente los puntos restantes a grupos.\n",
    "      3. Aplica fusión/subdivisión final.\n",
    "      4. Refina recursivamente los grupos con alta dispersión (sin gaps).\n",
    "      5. Almacena en un diccionario los grupos (good groups) que cumplen los criterios\n",
    "         y extrapola su ajuste incluyendo subgrupos con variación en pendiente ≤25%\n",
    "         y solapamiento en θ y r dentro del 18%.\n",
    "    \"\"\"\n",
    "    seed_clusters, df_filtered = detect_seeds(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points\n",
    "    )\n",
    "    \n",
    "    states, groups, df_filtered = iterative_assignment(\n",
    "        seed_clusters, df_filtered,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval\n",
    "    )\n",
    "    \n",
    "    preliminary_groups = finalize_groups(groups, max_theta_gap=max_theta_gap, flexible_pa_range=flexible_pa_range)\n",
    "    \n",
    "    refined_groups = refine_all_groups(preliminary_groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r)\n",
    "    \n",
    "    good_groups = {}\n",
    "    good_counter = 0\n",
    "    for g in refined_groups:\n",
    "        disp = compute_dispersion(g)\n",
    "        if disp <= dispersion_threshold and g['slope'] is not None and g['intercept'] is not None and \\\n",
    "           g['pa'] is not None and flexible_pa_range[0] <= abs(g['pa']) <= flexible_pa_range[1]:\n",
    "            good_groups[good_counter] = {'group': g, 'dispersion': disp}\n",
    "            good_counter += 1\n",
    "\n",
    "    updated_good_groups = extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.25, theta_relative_margin=0.18, r_relative_margin=0.18)\n",
    "    \n",
    "    return states, refined_groups, updated_good_groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Ejecución Interactiva\n",
    "# ========================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parámetros de ejemplo (ajusta según tus necesidades)\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.55\n",
    "    theta_min = 20.0\n",
    "    theta_max = 270.0\n",
    "    theta_diff = 4.0\n",
    "    r_diff = 1.0\n",
    "    gap_threshold_theta = 1.5\n",
    "    gap_threshold_r = 1.5\n",
    "    min_seed_points = 50\n",
    "    theta_fit_window = 20.0\n",
    "    r_fit_window = 2.0\n",
    "    radius_tolerance = 0.345\n",
    "    slope_variation_threshold = 0.18\n",
    "    snapshot_interval = 10\n",
    "    max_theta_gap = 8.0\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    dispersion_threshold = 0.35  # Ajusta según la escala de las distancias\n",
    "    \n",
    "    states, final_groups, good_groups, df_filtered = process_spiral_arms_integrated(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        dispersion_threshold=dispersion_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de estados intermedios: {len(states)}\")\n",
    "    print(f\"Total de segmentos finales refinados: {len(final_groups)}\")\n",
    "    print(f\"Total de Good Groups extrapolados: {len(good_groups)}\")\n",
    "    \n",
    "    def view_state(state_index):\n",
    "        state = states[state_index]\n",
    "        plot_intermediate_state_cartesian(state, df_filtered)\n",
    "    \n",
    "    interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n",
    "    \n",
    "    plot_final_groups(final_groups, df_filtered)\n",
    "    \n",
    "    good_groups_list = [entry['group'] for entry in good_groups.values()]\n",
    "    if good_groups_list:\n",
    "        print(\"Mostrando Good Groups extrapolados:\")\n",
    "        plot_final_groups(good_groups_list, df_filtered)\n",
    "    else:\n",
    "        print(\"No se encontraron grupos que cumplan con los criterios de buena dispersión.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654695a9-3948-472b-8d5c-55e30ddc35f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import deque\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# ========================\n",
    "# Funciones Auxiliares\n",
    "# ========================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"Calcula la distancia vertical (en r) de un punto a la recta definida por slope e intercept.\"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo (en grados) de la recta a partir de su pendiente.\n",
    "    Si se dispone de intercepto, se usa para un cálculo ajustado.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "# ========================\n",
    "# Carga y Preprocesamiento de Datos\n",
    "# ========================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold):\n",
    "    \"\"\"\n",
    "    Carga el CSV, calcula coordenadas polares y filtra puntos:\n",
    "      - Por rango de θ (en grados).\n",
    "      - Por percentil en 'rho_resta_final_exp'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    \n",
    "    # Filtrar por percentil en 'rho_resta_final_exp'\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# ========================\n",
    "# Detección de Semillas utilizando Grafos y BFS\n",
    "# ========================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo conectando puntos si la diferencia en θ y en r es menor o igual a los umbrales.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Se excluye el mismo punto\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae las componentes conexas (clusters) del grafo usando búsqueda en anchura (BFS).\n",
    "    Cada componente se retorna como un DataFrame.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena el DataFrame según 'theta' o 'r' y subdivide el cluster en subclusters \n",
    "    si existe un gap mayor que gap_threshold.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def detect_seeds(id_halo, quartile_threshold=0.55, theta_min=50.0, theta_max=250.0, \n",
    "                 theta_diff=5.0, r_diff=1.0, gap_threshold_theta=2.5, gap_threshold_r=2.5, min_seed_points=60):\n",
    "    \"\"\"\n",
    "    Detecta semillas robustas usando la construcción de grafo, BFS y subdivisión por gaps.\n",
    "    Retorna una lista de DataFrames (cada uno es una semilla) y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    df_filtered = load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold)\n",
    "    \n",
    "    # Construir grafo y extraer componentes conexas\n",
    "    graph = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    # Subdividir cada cluster según gaps en θ y luego en r\n",
    "    seeds_all = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = []\n",
    "            for group in subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r'):\n",
    "                sub_r.append(group)\n",
    "            seeds_all.extend(sub_r)\n",
    "    \n",
    "    # Filtrar semillas que tengan al menos min_seed_points\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) >= min_seed_points]\n",
    "    print(f\"Total de semillas robustas (>= {min_seed_points} puntos): {len(seed_clusters)}\")\n",
    "    \n",
    "    # Marcar en df_filtered los puntos que pertenecen a alguna semilla\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    return seed_clusters, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Asignación Iterativa de Puntos a Grupos con Validación Local\n",
    "# ========================\n",
    "\n",
    "def iterative_assignment(seed_clusters, df_filtered, theta_fit_window=30.0, r_fit_window=10.0, \n",
    "                          radius_tolerance=0.45, slope_variation_threshold=0.20, snapshot_interval=10):\n",
    "    \"\"\"\n",
    "    Usa las semillas como grupos iniciales (con ajuste lineal para obtener pendiente, intercepto y ángulo)\n",
    "    y asigna iterativamente los puntos restantes (no pertenecientes a semillas) a estos grupos, validando\n",
    "    que la incorporación de cada punto mantenga la coherencia geométrica.\n",
    "    Retorna estados intermedios y los grupos finales.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    # Inicializar grupos a partir de semillas\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 2:\n",
    "            X_seed = seed[['theta']].values\n",
    "            y_seed = seed['r'].values\n",
    "            model.fit(X_seed, y_seed)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        groups.append({\n",
    "            'group_id': idx,\n",
    "            'points': seed.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # Puntos restantes que no están en semillas\n",
    "    df_remaining = df_filtered[~df_filtered['in_seed']].copy().reset_index(drop=True)\n",
    "    df_remaining = df_remaining.sort_values(by='theta').reset_index(drop=True)\n",
    "    \n",
    "    temp_groups = []  # Para almacenar puntos que inicialmente no se asignan\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    \n",
    "    # Bucle iterativo de asignación\n",
    "    while len(df_remaining) > 0:\n",
    "        point = df_remaining.iloc[0]\n",
    "        df_remaining = df_remaining.iloc[1:].reset_index(drop=True)\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Buscar grupo candidato en el que el punto caiga dentro de una ventana local\n",
    "        for g in groups:\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            if not (g_theta_min <= theta_pt <= g_theta_max and g_r_min <= r_pt <= g_r_max):\n",
    "                continue\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            if old_slope is not None and abs(new_slope - old_slope) / (abs(old_slope) + 1e-12) > slope_variation_threshold:\n",
    "                dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "                if dist <= radius_tolerance:\n",
    "                    if best_distance is None or dist < best_distance:\n",
    "                        best_distance = dist\n",
    "                        best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = best_group['points'][(best_group['points']['theta'] >= window_min) & (best_group['points']['theta'] <= window_max)]\n",
    "            if len(subset) < 2:\n",
    "                subset = best_group['points']\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "        else:\n",
    "            temp_groups.append(point.to_frame().T)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % snapshot_interval == 0 or len(df_remaining) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_points': pd.concat(temp_groups, ignore_index=True) if temp_groups else pd.DataFrame(),\n",
    "                'remaining': df_remaining.copy()\n",
    "            })\n",
    "    \n",
    "    if temp_groups:\n",
    "        temp_df = pd.concat(temp_groups, ignore_index=True)\n",
    "        for idx, point in temp_df.iterrows():\n",
    "            assigned = False\n",
    "            for g in groups:\n",
    "                if (g['points']['theta'].min() - theta_fit_window/2 <= point['theta'] <= g['points']['theta'].max() + theta_fit_window/2 and\n",
    "                    g['points']['r'].min() - r_fit_window/2 <= point['r'] <= g['points']['r'].max() + r_fit_window/2):\n",
    "                    g['points'] = pd.concat([g['points'], point.to_frame().T], ignore_index=True)\n",
    "                    model.fit(g['points'][['theta']], g['points']['r'])\n",
    "                    g['slope'] = model.coef_[0]\n",
    "                    g['intercept'] = model.intercept_\n",
    "                    g['pa'] = calculate_pa(g['slope'], g['intercept'])\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:\n",
    "                pass\n",
    "    \n",
    "    return states, groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Validación de Gaps en θ y en r\n",
    "# ========================\n",
    "\n",
    "def validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r):\n",
    "    \"\"\"\n",
    "    Valida que el grupo no tenga gaps en θ ni en r.\n",
    "    Primero se subdivide por gaps en θ y luego, para cada subgrupo, se subdivide por gaps en r.\n",
    "    Retorna una lista de grupos validados.\n",
    "    \"\"\"\n",
    "    subgroups_theta = subdivide_by_gap(group['points'], gap_threshold_theta, mode='theta')\n",
    "    refined_groups = []\n",
    "    for sg in subgroups_theta:\n",
    "        subgroups_r = subdivide_by_gap(sg, gap_threshold_r, mode='r')\n",
    "        for sgr in subgroups_r:\n",
    "            if len(sgr) >= 2:\n",
    "                model = LinearRegression()\n",
    "                model.fit(sgr[['theta']], sgr['r'])\n",
    "                slope = model.coef_[0]\n",
    "                intercept = model.intercept_\n",
    "                pa = calculate_pa(slope, intercept)\n",
    "            else:\n",
    "                slope, intercept, pa = None, None, None\n",
    "            refined_groups.append({\n",
    "                'group_id': group.get('group_id'),\n",
    "                'points': sgr,\n",
    "                'slope': slope,\n",
    "                'intercept': intercept,\n",
    "                'pa': pa\n",
    "            })\n",
    "    return refined_groups\n",
    "\n",
    "# ========================\n",
    "# Cálculo de la Dispersión\n",
    "# ========================\n",
    "\n",
    "def compute_dispersion(group):\n",
    "    \"\"\"\n",
    "    Calcula la dispersión del grupo como la desviación estándar de las distancias de cada punto\n",
    "    a la recta ajustada (usando calculate_distance).\n",
    "    \"\"\"\n",
    "    if group['slope'] is None or group['points'].empty:\n",
    "        return 0\n",
    "    distances = group['points'].apply(lambda row: calculate_distance(group['slope'], group['intercept'], row['theta'], row['r']), axis=1)\n",
    "    return distances.std()\n",
    "\n",
    "# ========================\n",
    "# Refinamiento Recursivo de Grupos basándose en la Dispersión\n",
    "# ========================\n",
    "\n",
    "def refine_group(group, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    \"\"\"\n",
    "    Refina recursivamente un grupo:\n",
    "      - Primero valida que no tenga gaps en θ ni en r.\n",
    "      - Si la dispersión es mayor que dispersion_threshold, se divide el grupo en dos (por la mediana de θ)\n",
    "        y se refina cada subgrupo.\n",
    "    Retorna una lista de grupos refinados.\n",
    "    \"\"\"\n",
    "    valid_groups = validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r)\n",
    "    refined = []\n",
    "    for vg in valid_groups:\n",
    "        if len(vg['points']) < min_points:\n",
    "            refined.append(vg)\n",
    "            continue\n",
    "        disp = compute_dispersion(vg)\n",
    "        if disp <= dispersion_threshold:\n",
    "            refined.append(vg)\n",
    "        else:\n",
    "            pts = vg['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "            median_index = len(pts) // 2\n",
    "            group1_pts = pts.iloc[:median_index].copy()\n",
    "            group2_pts = pts.iloc[median_index:].copy()\n",
    "            model = LinearRegression()\n",
    "            if len(group1_pts) >= 2:\n",
    "                model.fit(group1_pts[['theta']], group1_pts['r'])\n",
    "                slope1 = model.coef_[0]\n",
    "                intercept1 = model.intercept_\n",
    "                pa1 = calculate_pa(slope1, intercept1)\n",
    "            else:\n",
    "                slope1, intercept1, pa1 = None, None, None\n",
    "            if len(group2_pts) >= 2:\n",
    "                model.fit(group2_pts[['theta']], group2_pts['r'])\n",
    "                slope2 = model.coef_[0]\n",
    "                intercept2 = model.intercept_\n",
    "                pa2 = calculate_pa(slope2, intercept2)\n",
    "            else:\n",
    "                slope2, intercept2, pa2 = None, None, None\n",
    "            sub_group1 = {\n",
    "                'group_id': vg.get('group_id'),\n",
    "                'points': group1_pts,\n",
    "                'slope': slope1,\n",
    "                'intercept': intercept1,\n",
    "                'pa': pa1\n",
    "            }\n",
    "            sub_group2 = {\n",
    "                'group_id': vg.get('group_id'),\n",
    "                'points': group2_pts,\n",
    "                'slope': slope2,\n",
    "                'intercept': intercept2,\n",
    "                'pa': pa2\n",
    "            }\n",
    "            refined.extend(refine_group(sub_group1, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "            refined.extend(refine_group(sub_group2, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined\n",
    "\n",
    "def refine_all_groups(groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    \"\"\"\n",
    "    Aplica el refinamiento recursivo a todos los grupos de la lista.\n",
    "    \"\"\"\n",
    "    refined_groups = []\n",
    "    for g in groups:\n",
    "        refined_groups.extend(refine_group(g, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined_groups\n",
    "\n",
    "# ========================\n",
    "# Fusión y Subdivisión Final de Grupos (por gap en θ)\n",
    "# ========================\n",
    "\n",
    "def subdivide_if_gap_final(group, max_theta_gap=15.0):\n",
    "    \"\"\"\n",
    "    Divide el grupo en dos segmentos si se detecta un gap significativo en la variable θ.\n",
    "    \"\"\"\n",
    "    pts = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_theta_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    model = LinearRegression()\n",
    "    if len(pts1) >= 2:\n",
    "        model.fit(pts1[['theta']], pts1['r'])\n",
    "        s1 = model.coef_[0]\n",
    "        i1 = model.intercept_\n",
    "        pa1 = calculate_pa(s1, i1)\n",
    "    else:\n",
    "        s1, i1, pa1 = None, None, None\n",
    "    if len(pts2) >= 2:\n",
    "        model.fit(pts2[['theta']], pts2['r'])\n",
    "        s2 = model.coef_[0]\n",
    "        i2 = model.intercept_\n",
    "        pa2 = calculate_pa(s2, i2)\n",
    "    else:\n",
    "        s2, i2, pa2 = None, None, None\n",
    "    seg1 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts1,\n",
    "        'slope': s1,\n",
    "        'intercept': i1,\n",
    "        'pa': pa1\n",
    "    }\n",
    "    seg2 = {\n",
    "        'group_id': group.get('group_id'),\n",
    "        'points': pts2,\n",
    "        'slope': s2,\n",
    "        'intercept': i2,\n",
    "        'pa': pa2\n",
    "    }\n",
    "    return [seg1, seg2]\n",
    "\n",
    "def finalize_groups(groups, max_theta_gap=15.0, flexible_pa_range=(4.0, 30.0)):\n",
    "    \"\"\"\n",
    "    Aplica una subdivisión final por gaps en θ y filtra los grupos que cumplan con un rango flexible para pa.\n",
    "    \"\"\"\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap_final(g, max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    \n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    return valid_groups\n",
    "\n",
    "# ========================\n",
    "# Extrapolación de Good Groups\n",
    "# ========================\n",
    "\n",
    "def extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.28, theta_relative_margin=0.18, r_relative_margin=0.18):\n",
    "    \"\"\"\n",
    "    Para cada good group, se revisan los grupos candidatos en refined_groups (que no sean el mismo)\n",
    "    y si la diferencia relativa en pendiente es ≤ slope_variation_merge (28% en este caso)\n",
    "    y los rangos de θ y r se solapan dentro de un margen relativo (theta_relative_margin y r_relative_margin),\n",
    "    se fusionan los puntos, se recalculan los parámetros y se actualiza el good group.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    updated_good_groups = {}\n",
    "    for key, entry in good_groups.items():\n",
    "        good_group = entry['group']\n",
    "        good_theta_min = good_group['points']['theta'].min()\n",
    "        good_theta_max = good_group['points']['theta'].max()\n",
    "        theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "        \n",
    "        good_r_min = good_group['points']['r'].min()\n",
    "        good_r_max = good_group['points']['r'].max()\n",
    "        r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "        \n",
    "        for candidate in refined_groups:\n",
    "            if candidate is good_group:\n",
    "                continue\n",
    "            if good_group['slope'] is None or candidate['slope'] is None:\n",
    "                continue\n",
    "            relative_diff = abs(candidate['slope'] - good_group['slope']) / (abs(good_group['slope']) + 1e-12)\n",
    "            if relative_diff <= slope_variation_merge:\n",
    "                cand_theta_min = candidate['points']['theta'].min()\n",
    "                cand_theta_max = candidate['points']['theta'].max()\n",
    "                cand_r_min = candidate['points']['r'].min()\n",
    "                cand_r_max = candidate['points']['r'].max()\n",
    "                theta_condition = (cand_theta_min >= good_theta_min - theta_relative_margin * theta_range and\n",
    "                                   cand_theta_max <= good_theta_max + theta_relative_margin * theta_range)\n",
    "                r_condition = (cand_r_min >= good_r_min - r_relative_margin * r_range and\n",
    "                               cand_r_max <= good_r_max + r_relative_margin * r_range)\n",
    "                if theta_condition and r_condition:\n",
    "                    merged_points = pd.concat([good_group['points'], candidate['points']], ignore_index=True)\n",
    "                    if len(merged_points) >= 2:\n",
    "                        model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                        new_slope = model.coef_[0]\n",
    "                        new_intercept = model.intercept_\n",
    "                        new_pa = calculate_pa(new_slope, new_intercept)\n",
    "                    else:\n",
    "                        new_slope, new_intercept, new_pa = good_group['slope'], good_group['intercept'], good_group['pa']\n",
    "                    good_group['points'] = merged_points\n",
    "                    good_group['slope'] = new_slope\n",
    "                    good_group['intercept'] = new_intercept\n",
    "                    good_group['pa'] = new_pa\n",
    "                    good_theta_min = good_group['points']['theta'].min()\n",
    "                    good_theta_max = good_group['points']['theta'].max()\n",
    "                    theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "                    good_r_min = good_group['points']['r'].min()\n",
    "                    good_r_max = good_group['points']['r'].max()\n",
    "                    r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "        updated_good_groups[key] = {'group': good_group, 'dispersion': compute_dispersion(good_group)}\n",
    "    return updated_good_groups\n",
    "\n",
    "# ========================\n",
    "# Funciones de Visualización\n",
    "# ========================\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Grupo {idx+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if 'temp_points' in state and not state['temp_points'].empty:\n",
    "        plt.scatter(state['temp_points']['theta'], state['temp_points']['r'], s=10, color='black', marker='x', label='Temp')\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_final_groups(final_groups, df_all):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, grp in enumerate(final_groups):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Segmento {idx+1}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Segmentos Finales (Plano r, θ)\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ========================\n",
    "# Proceso Integrado\n",
    "# ========================\n",
    "\n",
    "def process_spiral_arms_integrated(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5,\n",
    "    min_seed_points=60,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    snapshot_interval=10,\n",
    "    max_theta_gap=15.0,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    dispersion_threshold=0.35  # Umbral para dispersión\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso integrado:\n",
    "      1. Detecta semillas robustas.\n",
    "      2. Asigna iterativamente los puntos restantes a grupos.\n",
    "      3. Aplica fusión/subdivisión final.\n",
    "      4. Refina recursivamente los grupos con alta dispersión (sin gaps).\n",
    "      5. Almacena en un diccionario los grupos (good groups) que cumplen los criterios\n",
    "         y extrapola su ajuste incluyendo subgrupos con variación en pendiente ≤28%\n",
    "         y solapamiento en θ y r dentro del 18%.\n",
    "    \"\"\"\n",
    "    seed_clusters, df_filtered = detect_seeds(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points\n",
    "    )\n",
    "    \n",
    "    states, groups, df_filtered = iterative_assignment(\n",
    "        seed_clusters, df_filtered,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval\n",
    "    )\n",
    "    \n",
    "    preliminary_groups = finalize_groups(groups, max_theta_gap=max_theta_gap, flexible_pa_range=flexible_pa_range)\n",
    "    \n",
    "    refined_groups = refine_all_groups(preliminary_groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r)\n",
    "    \n",
    "    good_groups = {}\n",
    "    good_counter = 0\n",
    "    for g in refined_groups:\n",
    "        disp = compute_dispersion(g)\n",
    "        if disp <= dispersion_threshold and g['slope'] is not None and g['intercept'] is not None and \\\n",
    "           g['pa'] is not None and flexible_pa_range[0] <= abs(g['pa']) <= flexible_pa_range[1]:\n",
    "            good_groups[good_counter] = {'group': g, 'dispersion': disp}\n",
    "            good_counter += 1\n",
    "\n",
    "    updated_good_groups = extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.28, theta_relative_margin=0.18, r_relative_margin=0.18)\n",
    "    \n",
    "    return states, refined_groups, updated_good_groups, df_filtered\n",
    "\n",
    "# ========================\n",
    "# Ejecución Interactiva\n",
    "# ========================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parámetros de ejemplo (ajusta según tus necesidades)\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.55\n",
    "    theta_min = 20.0\n",
    "    theta_max = 270.0\n",
    "    theta_diff = 4.0\n",
    "    r_diff = 1.0\n",
    "    gap_threshold_theta = 1.5\n",
    "    gap_threshold_r = 1.5\n",
    "    min_seed_points = 50\n",
    "    theta_fit_window = 20.0\n",
    "    r_fit_window = 2.0\n",
    "    radius_tolerance = 0.345\n",
    "    slope_variation_threshold = 0.18\n",
    "    snapshot_interval = 10\n",
    "    max_theta_gap = 8.0\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    dispersion_threshold = 0.35  # Ajusta según la escala de las distancias\n",
    "    \n",
    "    states, final_groups, good_groups, df_filtered = process_spiral_arms_integrated(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        dispersion_threshold=dispersion_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de estados intermedios: {len(states)}\")\n",
    "    print(f\"Total de segmentos finales refinados: {len(final_groups)}\")\n",
    "    print(f\"Total de Good Groups extrapolados: {len(good_groups)}\")\n",
    "    \n",
    "    def view_state(state_index):\n",
    "        state = states[state_index]\n",
    "        plot_intermediate_state_cartesian(state, df_filtered)\n",
    "    \n",
    "    interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n",
    "    \n",
    "    plot_final_groups(final_groups, df_filtered)\n",
    "    \n",
    "    good_groups_list = [entry['group'] for entry in good_groups.values()]\n",
    "    if good_groups_list:\n",
    "        print(\"Mostrando Good Groups extrapolados:\")\n",
    "        plot_final_groups(good_groups_list, df_filtered)\n",
    "    else:\n",
    "        print(\"No se encontraron grupos que cumplan con los criterios de buena dispersión.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdb14177-d046-4a75-b4de-48902c33a384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3d1d5e5-4608-4543-84bb-314d427ef7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b45ce1d-f124-4a81-a89a-d08f0e0d4041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import deque\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Carga y Preprocesamiento de Datos\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold):\n",
    "    \"\"\"\n",
    "    Carga el archivo CSV 'data_rho_{id_halo}_filtered.csv' desde el directorio actual,\n",
    "    calcula las coordenadas polares y filtra los puntos según el percentil de 'rho_resta_final_exp'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    return df_filtered\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Obtención de Semillas con DBSCAN y Fusión de Clusters Cercanos\n",
    "# =============================================================================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold, mode='theta'):\n",
    "    order_col = 'theta' if mode=='theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def detect_seeds(id_halo, quartile_threshold=0.55, theta_min=50.0, theta_max=250.0, \n",
    "                 theta_diff=5.0, r_diff=1.0, gap_threshold_theta=2.5, gap_threshold_r=2.5, min_seed_points=60):\n",
    "    df_filtered = load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold)\n",
    "    graph = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    seeds_all = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            for group in subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r'):\n",
    "                seeds_all.append(group)\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) >= min_seed_points]\n",
    "    print(f\"Total de semillas robustas (>= {min_seed_points} puntos): {len(seed_clusters)}\")\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    return seed_clusters, df_filtered\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Funciones Auxiliares para Ajuste y Validación\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def is_valid_pa(pa, valid_range):\n",
    "    return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Asignación Iterativa y Refinamiento de Grupos\n",
    "# =============================================================================\n",
    "\n",
    "def iterative_assignment(seed_clusters, df_filtered, theta_fit_window=30.0, r_fit_window=10.0, \n",
    "                          radius_tolerance=0.45, slope_variation_threshold=0.20, snapshot_interval=10):\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 2:\n",
    "            X_seed = seed[['theta']].values\n",
    "            y_seed = seed['r'].values\n",
    "            model.fit(X_seed, y_seed)\n",
    "            groups.append({\n",
    "                'group_id': idx,\n",
    "                'points': seed.copy(),\n",
    "                'slope': model.coef_[0],\n",
    "                'intercept': model.intercept_,\n",
    "                'pa': calculate_pa(model.coef_[0], model.intercept_)\n",
    "            })\n",
    "    states = []  # No se generan estados intermedios en esta versión\n",
    "    return states, groups, df_filtered\n",
    "\n",
    "def validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r):\n",
    "    subgroups_theta = subdivide_by_gap(group['points'], gap_threshold_theta, mode='theta')\n",
    "    refined_groups = []\n",
    "    for sg in subgroups_theta:\n",
    "        for sgr in subdivide_by_gap(sg, gap_threshold_r, mode='r'):\n",
    "            if len(sgr) >= 2:\n",
    "                model = LinearRegression()\n",
    "                model.fit(sgr[['theta']], sgr['r'])\n",
    "                slope = model.coef_[0]\n",
    "                intercept = model.intercept_\n",
    "                pa = calculate_pa(slope, intercept)\n",
    "            else:\n",
    "                slope, intercept, pa = None, None, None\n",
    "            refined_groups.append({\n",
    "                'group_id': group.get('group_id'),\n",
    "                'points': sgr,\n",
    "                'slope': slope,\n",
    "                'intercept': intercept,\n",
    "                'pa': pa\n",
    "            })\n",
    "    return refined_groups\n",
    "\n",
    "def compute_dispersion(group):\n",
    "    if group['slope'] is None or group['points'].empty:\n",
    "        return 0\n",
    "    distances = group['points'].apply(lambda row: calculate_distance(group['slope'], group['intercept'], row['theta'], row['r']), axis=1)\n",
    "    return distances.std()\n",
    "\n",
    "def refine_group(group, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    valid_groups = validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r)\n",
    "    refined = []\n",
    "    for vg in valid_groups:\n",
    "        if len(vg['points']) < min_points:\n",
    "            refined.append(vg)\n",
    "            continue\n",
    "        disp = compute_dispersion(vg)\n",
    "        if disp <= dispersion_threshold:\n",
    "            refined.append(vg)\n",
    "        else:\n",
    "            pts = vg['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "            median_index = len(pts) // 2\n",
    "            group1_pts = pts.iloc[:median_index].copy()\n",
    "            group2_pts = pts.iloc[median_index:].copy()\n",
    "            model = LinearRegression()\n",
    "            if len(group1_pts) >= 2:\n",
    "                model.fit(group1_pts[['theta']], group1_pts['r'])\n",
    "                slope1, intercept1 = model.coef_[0], model.intercept_\n",
    "                pa1 = calculate_pa(slope1, intercept1)\n",
    "            else:\n",
    "                slope1, intercept1, pa1 = None, None, None\n",
    "            if len(group2_pts) >= 2:\n",
    "                model.fit(group2_pts[['theta']], group2_pts['r'])\n",
    "                slope2, intercept2 = model.coef_[0], model.intercept_\n",
    "                pa2 = calculate_pa(slope2, intercept2)\n",
    "            else:\n",
    "                slope2, intercept2, pa2 = None, None, None\n",
    "            sub_group1 = {'group_id': vg.get('group_id'), 'points': group1_pts, 'slope': slope1, 'intercept': intercept1, 'pa': pa1}\n",
    "            sub_group2 = {'group_id': vg.get('group_id'), 'points': group2_pts, 'slope': slope2, 'intercept': intercept2, 'pa': pa2}\n",
    "            refined.extend(refine_group(sub_group1, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "            refined.extend(refine_group(sub_group2, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined\n",
    "\n",
    "def refine_all_groups(groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    refined_groups = []\n",
    "    for g in groups:\n",
    "        refined_groups.extend(refine_group(g, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined_groups\n",
    "\n",
    "def subdivide_if_gap_final(group, max_theta_gap=15.0):\n",
    "    pts = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_theta_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    model = LinearRegression()\n",
    "    if len(pts1) >= 2:\n",
    "        model.fit(pts1[['theta']], pts1['r'])\n",
    "        s1, i1 = model.coef_[0], model.intercept_\n",
    "        pa1 = calculate_pa(s1, i1)\n",
    "    else:\n",
    "        s1, i1, pa1 = None, None, None\n",
    "    if len(pts2) >= 2:\n",
    "        model.fit(pts2[['theta']], pts2['r'])\n",
    "        s2, i2 = model.coef_[0], model.intercept_\n",
    "        pa2 = calculate_pa(s2, i2)\n",
    "    else:\n",
    "        s2, i2, pa2 = None, None, None\n",
    "    seg1 = {'group_id': group.get('group_id'), 'points': pts1, 'slope': s1, 'intercept': i1, 'pa': pa1}\n",
    "    seg2 = {'group_id': group.get('group_id'), 'points': pts2, 'slope': s2, 'intercept': i2, 'pa': pa2}\n",
    "    return [seg1, seg2]\n",
    "\n",
    "def finalize_groups(groups, max_theta_gap=15.0, flexible_pa_range=(4.0, 30.0)):\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap_final(g, max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    return valid_groups\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Extrapolación de Good Groups (Revisada)\n",
    "# =============================================================================\n",
    "\n",
    "def extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.28, theta_relative_margin=0.18, r_relative_margin=0.18):\n",
    "    \"\"\"\n",
    "    Para cada Good Group, se revisan los candidatos en refined_groups (diferentes del grupo)\n",
    "    y se fusionan si:\n",
    "      - La diferencia relativa en pendiente es ≤28%.\n",
    "      - El overlap ratio de los intervalos de θ y de r es ≥ 0.5.\n",
    "    Se recalculan los parámetros y se actualiza el Good Group.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    updated_good_groups = {}\n",
    "    for key, entry in good_groups.items():\n",
    "        good_group = entry['group']\n",
    "        good_theta_min = good_group['points']['theta'].min()\n",
    "        good_theta_max = good_group['points']['theta'].max()\n",
    "        good_r_min = good_group['points']['r'].min()\n",
    "        good_r_max = good_group['points']['r'].max()\n",
    "        theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "        r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "        for candidate in refined_groups:\n",
    "            if candidate is good_group:\n",
    "                continue\n",
    "            if good_group['slope'] is None or candidate['slope'] is None:\n",
    "                continue\n",
    "            relative_diff = abs(candidate['slope'] - good_group['slope']) / (abs(good_group['slope']) + 1e-12)\n",
    "            if relative_diff > slope_variation_merge:\n",
    "                continue\n",
    "            cand_theta_min = candidate['points']['theta'].min()\n",
    "            cand_theta_max = candidate['points']['theta'].max()\n",
    "            cand_r_min = candidate['points']['r'].min()\n",
    "            cand_r_max = candidate['points']['r'].max()\n",
    "            theta_overlap = max(0, min(good_theta_max, cand_theta_max) - max(good_theta_min, cand_theta_min))\n",
    "            theta_union = max(good_theta_max, cand_theta_max) - min(good_theta_min, cand_theta_min)\n",
    "            theta_overlap_ratio = theta_overlap / theta_union if theta_union > 0 else 0\n",
    "            r_overlap = max(0, min(good_r_max, cand_r_max) - max(good_r_min, cand_r_min))\n",
    "            r_union = max(good_r_max, cand_r_max) - min(good_r_min, cand_r_min)\n",
    "            r_overlap_ratio = r_overlap / r_union if r_union > 0 else 0\n",
    "            if theta_overlap_ratio >= 0.5 and r_overlap_ratio >= 0.5:\n",
    "                merged_points = pd.concat([good_group['points'], candidate['points']], ignore_index=True)\n",
    "                if len(merged_points) >= 2:\n",
    "                    model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                    new_slope = model.coef_[0]\n",
    "                    new_intercept = model.intercept_\n",
    "                    new_pa = calculate_pa(new_slope, new_intercept)\n",
    "                else:\n",
    "                    new_slope, new_intercept, new_pa = good_group['slope'], good_group['intercept'], good_group['pa']\n",
    "                good_group['points'] = merged_points\n",
    "                good_group['slope'] = new_slope\n",
    "                good_group['intercept'] = new_intercept\n",
    "                good_group['pa'] = new_pa\n",
    "                good_theta_min = good_group['points']['theta'].min()\n",
    "                good_theta_max = good_group['points']['theta'].max()\n",
    "                good_r_min = good_group['points']['r'].min()\n",
    "                good_r_max = good_group['points']['r'].max()\n",
    "                theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "                r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "        updated_good_groups[key] = {'group': good_group, 'dispersion': compute_dispersion(good_group)}\n",
    "    return updated_good_groups\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Funciones de Visualización\n",
    "# =============================================================================\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f'Grupo {idx+1}')\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if 'temp_points' in state and not state['temp_points'].empty:\n",
    "        plt.scatter(state['temp_points']['theta'], state['temp_points']['r'], s=10, color='black', marker='x', label='Temp')\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_final_groups(final_groups, df_all):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan']\n",
    "    for idx, grp in enumerate(final_groups):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Segmento {idx+1}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Segmentos Finales (Plano r, θ)\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Proceso Integrado Mejorado\n",
    "# =============================================================================\n",
    "\n",
    "def process_spiral_arms_integrated_improved(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5,\n",
    "    min_seed_points=60,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    snapshot_interval=10,\n",
    "    max_theta_gap=15.0,\n",
    "    flexible_pa_range=(4.0,30.0),\n",
    "    dispersion_threshold=0.35\n",
    "):\n",
    "    \"\"\"\n",
    "    Proceso completo mejorado:\n",
    "      1. Detecta semillas robustas.\n",
    "      2. Genera grupos iniciales mediante ajuste lineal.\n",
    "      3. Realiza una subdivisión final y refina los grupos según dispersión.\n",
    "      4. Selecciona Good Groups y extrapola su ajuste incorporando candidatos con variación de pendiente ≤28%\n",
    "         y overlap en θ y r ≥50%.\n",
    "    \"\"\"\n",
    "    seed_clusters, df_filtered = detect_seeds(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points\n",
    "    )\n",
    "    states, groups, df_filtered = iterative_assignment(seed_clusters, df_filtered, theta_fit_window, r_fit_window, radius_tolerance, slope_variation_threshold, snapshot_interval)\n",
    "    preliminary_groups = finalize_groups(groups, max_theta_gap=max_theta_gap, flexible_pa_range=flexible_pa_range)\n",
    "    refined_groups = refine_all_groups(preliminary_groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r)\n",
    "    good_groups = {}\n",
    "    good_counter = 0\n",
    "    for g in refined_groups:\n",
    "        disp = compute_dispersion(g)\n",
    "        if disp <= dispersion_threshold and g['slope'] is not None and g['intercept'] is not None and \\\n",
    "           g['pa'] is not None and flexible_pa_range[0] <= abs(g['pa']) <= flexible_pa_range[1]:\n",
    "            good_groups[good_counter] = {'group': g, 'dispersion': disp}\n",
    "            good_counter += 1\n",
    "    updated_good_groups = extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.208, theta_relative_margin=0.38, r_relative_margin=0.38)\n",
    "    return states, refined_groups, updated_good_groups, df_filtered\n",
    "\n",
    "# =============================================================================\n",
    "# Bloque Principal de Ejecución\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.55\n",
    "    theta_min = 20.0\n",
    "    theta_max = 290.0\n",
    "    theta_diff = 4.0\n",
    "    r_diff = 1.0\n",
    "    gap_threshold_theta = 1.5\n",
    "    gap_threshold_r = 1.5\n",
    "    min_seed_points = 50\n",
    "    theta_fit_window = 20.0\n",
    "    r_fit_window = 2.0\n",
    "    radius_tolerance = 0.345\n",
    "    slope_variation_threshold = 0.28\n",
    "    snapshot_interval = 10\n",
    "    max_theta_gap = 8.0\n",
    "    flexible_pa_range = (4.0, 40.0)\n",
    "    dispersion_threshold = 0.35\n",
    "\n",
    "    states, refined_groups, good_groups, df_filtered = process_spiral_arms_integrated_improved(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        dispersion_threshold=dispersion_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de grupos refinados: {len(refined_groups)}\")\n",
    "    print(f\"Total de Good Groups extrapolados: {len(good_groups)}\")\n",
    "    \n",
    "    good_groups_list = [entry['group'] for entry in good_groups.values()]\n",
    "    if good_groups_list:\n",
    "        print(\"Mostrando Good Groups extrapolados:\")\n",
    "        plot_final_groups(good_groups_list, df_filtered)\n",
    "    else:\n",
    "        print(\"No se encontraron Good Groups que cumplan con los criterios de buena dispersión.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632e1f75-3390-4aa1-907b-0eb0873b95b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import deque\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Carga y Preprocesamiento de Datos\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold):\n",
    "    \"\"\"\n",
    "    Carga el archivo CSV 'data_rho_{id_halo}_filtered.csv' desde el directorio actual,\n",
    "    calcula las coordenadas polares y filtra los puntos según el percentil de 'rho_resta_final_exp'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    return df_filtered\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Obtención de Semillas con DBSCAN y Fusión de Clusters Cercanos\n",
    "# =============================================================================\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold, mode='theta'):\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def detect_seeds(id_halo, quartile_threshold=0.55, theta_min=50.0, theta_max=250.0, \n",
    "                 theta_diff=5.0, r_diff=1.0, gap_threshold_theta=2.5, gap_threshold_r=2.5, min_seed_points=60):\n",
    "    df_filtered = load_and_filter_data(id_halo, theta_min, theta_max, quartile_threshold)\n",
    "    graph = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    seeds_all = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            for group in subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r'):\n",
    "                seeds_all.append(group)\n",
    "    seed_clusters = [seed for seed in seeds_all if len(seed) >= min_seed_points]\n",
    "    print(f\"Total de semillas robustas (>= {min_seed_points} puntos): {len(seed_clusters)}\")\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    return seed_clusters, df_filtered\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Funciones Auxiliares para Ajuste y Validación\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def is_valid_pa(pa, valid_range):\n",
    "    return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Asignación Iterativa y Refinamiento de Grupos\n",
    "# =============================================================================\n",
    "\n",
    "def iterative_assignment(seed_clusters, df_filtered, theta_fit_window=30.0, r_fit_window=10.0, \n",
    "                          radius_tolerance=0.45, slope_variation_threshold=0.20, snapshot_interval=10):\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 2:\n",
    "            X_seed = seed[['theta']].values\n",
    "            y_seed = seed['r'].values\n",
    "            model.fit(X_seed, y_seed)\n",
    "            groups.append({\n",
    "                'group_id': idx,\n",
    "                'points': seed.copy(),\n",
    "                'slope': model.coef_[0],\n",
    "                'intercept': model.intercept_,\n",
    "                'pa': calculate_pa(model.coef_[0], model.intercept_)\n",
    "            })\n",
    "    states = []  # Aquí no se generan estados intermedios\n",
    "    return states, groups, df_filtered\n",
    "\n",
    "def validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r):\n",
    "    subgroups_theta = subdivide_by_gap(group['points'], gap_threshold_theta, mode='theta')\n",
    "    refined_groups = []\n",
    "    for sg in subgroups_theta:\n",
    "        for sgr in subdivide_by_gap(sg, gap_threshold_r, mode='r'):\n",
    "            if len(sgr) >= 2:\n",
    "                model = LinearRegression()\n",
    "                model.fit(sgr[['theta']], sgr['r'])\n",
    "                slope = model.coef_[0]\n",
    "                intercept = model.intercept_\n",
    "                pa = calculate_pa(slope, intercept)\n",
    "            else:\n",
    "                slope, intercept, pa = None, None, None\n",
    "            refined_groups.append({\n",
    "                'group_id': group.get('group_id'),\n",
    "                'points': sgr,\n",
    "                'slope': slope,\n",
    "                'intercept': intercept,\n",
    "                'pa': pa\n",
    "            })\n",
    "    return refined_groups\n",
    "\n",
    "def compute_dispersion(group):\n",
    "    if group['slope'] is None or group['points'].empty:\n",
    "        return 0\n",
    "    distances = group['points'].apply(lambda row: calculate_distance(group['slope'], group['intercept'], row['theta'], row['r']), axis=1)\n",
    "    return distances.std()\n",
    "\n",
    "def refine_group(group, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    valid_groups = validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r)\n",
    "    refined = []\n",
    "    for vg in valid_groups:\n",
    "        if len(vg['points']) < min_points:\n",
    "            refined.append(vg)\n",
    "            continue\n",
    "        disp = compute_dispersion(vg)\n",
    "        if disp <= dispersion_threshold:\n",
    "            refined.append(vg)\n",
    "        else:\n",
    "            pts = vg['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "            median_index = len(pts) // 2\n",
    "            group1_pts = pts.iloc[:median_index].copy()\n",
    "            group2_pts = pts.iloc[median_index:].copy()\n",
    "            model = LinearRegression()\n",
    "            if len(group1_pts) >= 2:\n",
    "                model.fit(group1_pts[['theta']], group1_pts['r'])\n",
    "                slope1, intercept1 = model.coef_[0], model.intercept_\n",
    "                pa1 = calculate_pa(slope1, intercept1)\n",
    "            else:\n",
    "                slope1, intercept1, pa1 = None, None, None\n",
    "            if len(group2_pts) >= 2:\n",
    "                model.fit(group2_pts[['theta']], group2_pts['r'])\n",
    "                slope2, intercept2 = model.coef_[0], model.intercept_\n",
    "                pa2 = calculate_pa(slope2, intercept2)\n",
    "            else:\n",
    "                slope2, intercept2, pa2 = None, None, None\n",
    "            sub_group1 = {'group_id': vg.get('group_id'), 'points': group1_pts, 'slope': slope1, 'intercept': intercept1, 'pa': pa1}\n",
    "            sub_group2 = {'group_id': vg.get('group_id'), 'points': group2_pts, 'slope': slope2, 'intercept': intercept2, 'pa': pa2}\n",
    "            refined.extend(refine_group(sub_group1, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "            refined.extend(refine_group(sub_group2, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined\n",
    "\n",
    "def refine_all_groups(groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    refined_groups = []\n",
    "    for g in groups:\n",
    "        refined_groups.extend(refine_group(g, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined_groups\n",
    "\n",
    "def subdivide_if_gap_final(group, max_theta_gap=15.0):\n",
    "    pts = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_theta_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    model = LinearRegression()\n",
    "    if len(pts1) >= 2:\n",
    "        model.fit(pts1[['theta']], pts1['r'])\n",
    "        s1, i1 = model.coef_[0], model.intercept_\n",
    "        pa1 = calculate_pa(s1, i1)\n",
    "    else:\n",
    "        s1, i1, pa1 = None, None, None\n",
    "    if len(pts2) >= 2:\n",
    "        model.fit(pts2[['theta']], pts2['r'])\n",
    "        s2, i2 = model.coef_[0], model.intercept_\n",
    "        pa2 = calculate_pa(s2, i2)\n",
    "    else:\n",
    "        s2, i2, pa2 = None, None, None\n",
    "    seg1 = {'group_id': group.get('group_id'), 'points': pts1, 'slope': s1, 'intercept': i1, 'pa': pa1}\n",
    "    seg2 = {'group_id': group.get('group_id'), 'points': pts2, 'slope': s2, 'intercept': i2, 'pa': pa2}\n",
    "    return [seg1, seg2]\n",
    "\n",
    "def finalize_groups(groups, max_theta_gap=15.0, flexible_pa_range=(4.0, 30.0)):\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap_final(g, max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    return valid_groups\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Extrapolación de Good Groups (Revisada y Mejorada)\n",
    "# =============================================================================\n",
    "\n",
    "def extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.28, theta_relative_margin=0.18, r_relative_margin=0.18, max_dispersion_increase=1.2):\n",
    "    \"\"\"\n",
    "    Para cada Good Group, se revisan los candidatos en refined_groups (diferentes del grupo)\n",
    "    y se fusionan si:\n",
    "      - La diferencia relativa en pendiente es ≤ 28%.\n",
    "      - El overlap ratio de los intervalos de θ y r es ≥ 0.5.\n",
    "      - La dispersión del grupo fusionado no aumenta más que un factor max_dispersion_increase\n",
    "        respecto a la dispersión original del Good Group.\n",
    "    Se recalculan los parámetros y se actualiza el Good Group.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    updated_good_groups = {}\n",
    "    for key, entry in good_groups.items():\n",
    "        good_group = entry['group']\n",
    "        original_disp = compute_dispersion(good_group)\n",
    "        good_theta_min = good_group['points']['theta'].min()\n",
    "        good_theta_max = good_group['points']['theta'].max()\n",
    "        good_r_min = good_group['points']['r'].min()\n",
    "        good_r_max = good_group['points']['r'].max()\n",
    "        theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "        r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "        for candidate in refined_groups:\n",
    "            if candidate is good_group:\n",
    "                continue\n",
    "            if good_group['slope'] is None or candidate['slope'] is None:\n",
    "                continue\n",
    "            relative_diff = abs(candidate['slope'] - good_group['slope']) / (abs(good_group['slope']) + 1e-12)\n",
    "            if relative_diff > slope_variation_merge:\n",
    "                continue\n",
    "            cand_theta_min = candidate['points']['theta'].min()\n",
    "            cand_theta_max = candidate['points']['theta'].max()\n",
    "            cand_r_min = candidate['points']['r'].min()\n",
    "            cand_r_max = candidate['points']['r'].max()\n",
    "            theta_overlap = max(0, min(good_theta_max, cand_theta_max) - max(good_theta_min, cand_theta_min))\n",
    "            theta_union = max(good_theta_max, cand_theta_max) - min(good_theta_min, cand_theta_min)\n",
    "            theta_overlap_ratio = theta_overlap / theta_union if theta_union > 0 else 0\n",
    "            r_overlap = max(0, min(good_r_max, cand_r_max) - max(good_r_min, cand_r_min))\n",
    "            r_union = max(good_r_max, cand_r_max) - min(good_r_min, cand_r_min)\n",
    "            r_overlap_ratio = r_overlap / r_union if r_union > 0 else 0\n",
    "            if theta_overlap_ratio >= 0.5 and r_overlap_ratio >= 0.5:\n",
    "                merged_points = pd.concat([good_group['points'], candidate['points']], ignore_index=True)\n",
    "                if len(merged_points) >= 2:\n",
    "                    model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                    new_slope = model.coef_[0]\n",
    "                    new_intercept = model.intercept_\n",
    "                    new_pa = calculate_pa(new_slope, new_intercept)\n",
    "                else:\n",
    "                    new_slope, new_intercept, new_pa = good_group['slope'], good_group['intercept'], good_group['pa']\n",
    "                # Calcular la dispersión del grupo fusionado\n",
    "                temp_group = {'points': merged_points, 'slope': new_slope, 'intercept': new_intercept, 'pa': new_pa}\n",
    "                merged_disp = compute_dispersion(temp_group)\n",
    "                if merged_disp <= original_disp * max_dispersion_increase:\n",
    "                    good_group['points'] = merged_points\n",
    "                    good_group['slope'] = new_slope\n",
    "                    good_group['intercept'] = new_intercept\n",
    "                    good_group['pa'] = new_pa\n",
    "                    good_theta_min = good_group['points']['theta'].min()\n",
    "                    good_theta_max = good_group['points']['theta'].max()\n",
    "                    good_r_min = good_group['points']['r'].min()\n",
    "                    good_r_max = good_group['points']['r'].max()\n",
    "                    theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "                    r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "                    original_disp = merged_disp\n",
    "        updated_good_groups[key] = {'group': good_group, 'dispersion': compute_dispersion(good_group)}\n",
    "    return updated_good_groups\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Funciones de Visualización\n",
    "# =============================================================================\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f'Grupo {idx+1}')\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if 'temp_points' in state and not state['temp_points'].empty:\n",
    "        plt.scatter(state['temp_points']['theta'], state['temp_points']['r'], s=10, color='black', marker='x', label='Temp')\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_final_groups(final_groups, df_all):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan']\n",
    "    for idx, grp in enumerate(final_groups):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Segmento {idx+1}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Segmentos Finales (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Proceso Integrado Mejorado\n",
    "# =============================================================================\n",
    "\n",
    "def process_spiral_arms_integrated_improved(\n",
    "    id_halo,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50.0,\n",
    "    theta_max=250.0,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5,\n",
    "    min_seed_points=60,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    snapshot_interval=10,\n",
    "    max_theta_gap=15.0,\n",
    "    flexible_pa_range=(4.0,30.0),\n",
    "    dispersion_threshold=0.35\n",
    "):\n",
    "    \"\"\"\n",
    "    Proceso completo mejorado:\n",
    "      1. Detecta semillas robustas.\n",
    "      2. Genera grupos iniciales mediante ajuste lineal.\n",
    "      3. Realiza una subdivisión final y refina los grupos según dispersión.\n",
    "      4. Selecciona Good Groups y extrapola su ajuste incorporando candidatos con variación en pendiente ≤28%\n",
    "         y overlap en θ y r ≥50%, siempre que la fusión no incremente excesivamente la dispersión.\n",
    "    \"\"\"\n",
    "    seed_clusters, df_filtered = detect_seeds(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points\n",
    "    )\n",
    "    states, groups, df_filtered = iterative_assignment(seed_clusters, df_filtered, theta_fit_window, r_fit_window, radius_tolerance, slope_variation_threshold, snapshot_interval)\n",
    "    preliminary_groups = finalize_groups(groups, max_theta_gap=max_theta_gap, flexible_pa_range=flexible_pa_range)\n",
    "    refined_groups = refine_all_groups(preliminary_groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r)\n",
    "    good_groups = {}\n",
    "    good_counter = 0\n",
    "    for g in refined_groups:\n",
    "        disp = compute_dispersion(g)\n",
    "        if disp <= dispersion_threshold and g['slope'] is not None and g['intercept'] is not None and \\\n",
    "           g['pa'] is not None and flexible_pa_range[0] <= abs(g['pa']) <= flexible_pa_range[1]:\n",
    "            good_groups[good_counter] = {'group': g, 'dispersion': disp}\n",
    "            good_counter += 1\n",
    "    updated_good_groups = extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.28, theta_relative_margin=0.18, r_relative_margin=0.18, max_dispersion_increase=1.2)\n",
    "    return states, refined_groups, updated_good_groups, df_filtered\n",
    "\n",
    "# =============================================================================\n",
    "# Bloque Principal de Ejecución\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    id_halo = \"17\"\n",
    "    quartile_threshold = 0.55\n",
    "    theta_min = 20.0\n",
    "    theta_max = 270.0\n",
    "    theta_diff = 4.0\n",
    "    r_diff = 1.0\n",
    "    gap_threshold_theta = 1.5\n",
    "    gap_threshold_r = 1.5\n",
    "    min_seed_points = 50\n",
    "    theta_fit_window = 20.0\n",
    "    r_fit_window = 2.0\n",
    "    radius_tolerance = 0.345\n",
    "    slope_variation_threshold = 0.28\n",
    "    snapshot_interval = 10\n",
    "    max_theta_gap = 8.0\n",
    "    flexible_pa_range = (4.0, 40.0)\n",
    "    dispersion_threshold = 0.35\n",
    "\n",
    "    states, refined_groups, good_groups, df_filtered = process_spiral_arms_integrated_improved(\n",
    "        id_halo=id_halo,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_min=theta_min,\n",
    "        theta_max=theta_max,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r,\n",
    "        min_seed_points=min_seed_points,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        dispersion_threshold=dispersion_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de grupos refinados: {len(refined_groups)}\")\n",
    "    print(f\"Total de Good Groups extrapolados: {len(good_groups)}\")\n",
    "    \n",
    "    good_groups_list = [entry['group'] for entry in good_groups.values()]\n",
    "    if good_groups_list:\n",
    "        print(\"Mostrando Good Groups extrapolados:\")\n",
    "        plot_final_groups(good_groups_list, df_filtered)\n",
    "    else:\n",
    "        print(\"No se encontraron Good Groups que cumplan con los criterios de buena dispersión.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dcc4315-ddb8-49d3-94ab-4e809041984a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "232b8fb9-7309-47c9-b2b3-3ecc2d4e7abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import deque\n",
    "import copy\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Carga y Preprocesamiento de Datos\n",
    "# =============================================================================\n",
    "\n",
    "def load_data(id_halo, quartile_threshold):\n",
    "    \"\"\"\n",
    "    Lee el archivo CSV desde '../DATA/processed/TracingPoints/' y calcula las\n",
    "    coordenadas polares, filtrando los puntos según el percentil de 'rho_resta_final_exp'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'data_rho_{id_halo}_filtered.csv')\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    df_filtered.sort_values(by='rho_resta_final_exp', ascending=False, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    return df_filtered\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Obtención de Semillas con DBSCAN y Fusión de Clusters Cercanos\n",
    "# =============================================================================\n",
    "\n",
    "def merge_close_clusters(clusters, clusters_tolerance):\n",
    "    \"\"\"\n",
    "    Fusiona clusters cuyos puntos estén lo suficientemente cerca según clusters_tolerance.\n",
    "    \"\"\"\n",
    "    merged_clusters = {}\n",
    "    used_keys = set()\n",
    "    for key1, points1 in clusters.items():\n",
    "        if key1 in used_keys:\n",
    "            continue\n",
    "        merged_points = points1.copy()\n",
    "        close_keys = [key1]\n",
    "        for key2, points2 in clusters.items():\n",
    "            if key2 == key1 or key2 in used_keys:\n",
    "                continue\n",
    "            d = np.sqrt((points1['r'].values[:, None] - points2['r'].values)**2 +\n",
    "                        (points1['theta'].values[:, None] - points2['theta'].values)**2)\n",
    "            if np.min(d) < clusters_tolerance:\n",
    "                merged_points = pd.concat([merged_points, points2], ignore_index=True)\n",
    "                used_keys.add(key2)\n",
    "                close_keys.append(key2)\n",
    "        merged_clusters[tuple(close_keys)] = merged_points\n",
    "        used_keys.add(key1)\n",
    "    return merged_clusters\n",
    "\n",
    "def dbscan_seeds(df, eps=0.198, min_samples=18, min_seed_points=60):\n",
    "    \"\"\"\n",
    "    Aplica DBSCAN en el espacio cartesiano, transforma los clusters a polar y\n",
    "    devuelve las semillas (clusters) que tengan al menos min_seed_points.\n",
    "    \"\"\"\n",
    "    X = df[['x', 'y']].values\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    df['cluster'] = db.fit_predict(X)\n",
    "    \n",
    "    clusters_polar = {}\n",
    "    for cluster_id in np.unique(df['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        cluster_points = df[df['cluster'] == cluster_id]\n",
    "        clusters_polar[cluster_id] = cluster_points[['r', 'theta']]\n",
    "    \n",
    "    clusters_tolerance = 0.70\n",
    "    clusters_polar = merge_close_clusters(clusters_polar, clusters_tolerance)\n",
    "    \n",
    "    seed_clusters = [cluster for cluster in clusters_polar.values() if len(cluster) >= min_seed_points]\n",
    "    print(f\"Total de semillas robustas (>= {min_seed_points} puntos): {len(seed_clusters)}\")\n",
    "    df['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed in seed_clusters:\n",
    "        seed_indices.update(seed.index.tolist())\n",
    "    df.loc[list(seed_indices), 'in_seed'] = True\n",
    "    return seed_clusters\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Funciones Auxiliares para Ajuste y Validación\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def is_valid_pa(pa, valid_range):\n",
    "    return valid_range[0] <= abs(pa) <= valid_range[1]\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Asignación Iterativa y Refinamiento de Grupos\n",
    "# =============================================================================\n",
    "\n",
    "def iterative_assignment(seed_clusters, theta_fit_window=30.0, r_fit_window=10.0, \n",
    "                          radius_tolerance=0.45, slope_variation_threshold=0.20, snapshot_interval=10):\n",
    "    \"\"\"\n",
    "    Genera grupos iniciales a partir de las semillas aplicando una regresión lineal.\n",
    "    En esta versión se asume que la mayor parte de los puntos relevantes ya se encuentran en las semillas.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for idx, seed in enumerate(seed_clusters):\n",
    "        if len(seed) >= 2:\n",
    "            X = seed[['theta']].values\n",
    "            y = seed['r'].values\n",
    "            model.fit(X, y)\n",
    "            groups.append({\n",
    "                'group_id': idx,\n",
    "                'points': seed.copy(),\n",
    "                'slope': model.coef_[0],\n",
    "                'intercept': model.intercept_,\n",
    "                'pa': calculate_pa(model.coef_[0], model.intercept_)\n",
    "            })\n",
    "    return groups\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold, mode='theta'):\n",
    "    order_col = 'theta' if mode=='theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r):\n",
    "    subgroups_theta = subdivide_by_gap(group['points'], gap_threshold_theta, mode='theta')\n",
    "    refined_groups = []\n",
    "    for sg in subgroups_theta:\n",
    "        subgroups_r = subdivide_by_gap(sg, gap_threshold_r, mode='r')\n",
    "        for sgr in subgroups_r:\n",
    "            if len(sgr) >= 2:\n",
    "                model = LinearRegression()\n",
    "                model.fit(sgr[['theta']], sgr['r'])\n",
    "                slope = model.coef_[0]\n",
    "                intercept = model.intercept_\n",
    "                pa = calculate_pa(slope, intercept)\n",
    "            else:\n",
    "                slope, intercept, pa = None, None, None\n",
    "            refined_groups.append({\n",
    "                'group_id': group.get('group_id'),\n",
    "                'points': sgr,\n",
    "                'slope': slope,\n",
    "                'intercept': intercept,\n",
    "                'pa': pa\n",
    "            })\n",
    "    return refined_groups\n",
    "\n",
    "def compute_dispersion(group):\n",
    "    if group['slope'] is None or group['points'].empty:\n",
    "        return 0\n",
    "    distances = group['points'].apply(lambda row: calculate_distance(group['slope'], group['intercept'], row['theta'], row['r']), axis=1)\n",
    "    return distances.std()\n",
    "\n",
    "def refine_group(group, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    valid_groups = validate_group_no_gaps(group, gap_threshold_theta, gap_threshold_r)\n",
    "    refined = []\n",
    "    for vg in valid_groups:\n",
    "        if len(vg['points']) < min_points:\n",
    "            refined.append(vg)\n",
    "            continue\n",
    "        disp = compute_dispersion(vg)\n",
    "        if disp <= dispersion_threshold:\n",
    "            refined.append(vg)\n",
    "        else:\n",
    "            pts = vg['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "            median_index = len(pts) // 2\n",
    "            group1_pts = pts.iloc[:median_index].copy()\n",
    "            group2_pts = pts.iloc[median_index:].copy()\n",
    "            model = LinearRegression()\n",
    "            if len(group1_pts) >= 2:\n",
    "                model.fit(group1_pts[['theta']], group1_pts['r'])\n",
    "                slope1, intercept1 = model.coef_[0], model.intercept_\n",
    "                pa1 = calculate_pa(slope1, intercept1)\n",
    "            else:\n",
    "                slope1, intercept1, pa1 = None, None, None\n",
    "            if len(group2_pts) >= 2:\n",
    "                model.fit(group2_pts[['theta']], group2_pts['r'])\n",
    "                slope2, intercept2 = model.coef_[0], model.intercept_\n",
    "                pa2 = calculate_pa(slope2, intercept2)\n",
    "            else:\n",
    "                slope2, intercept2, pa2 = None, None, None\n",
    "            sub_group1 = {'group_id': vg.get('group_id'), 'points': group1_pts, 'slope': slope1, 'intercept': intercept1, 'pa': pa1}\n",
    "            sub_group2 = {'group_id': vg.get('group_id'), 'points': group2_pts, 'slope': slope2, 'intercept': intercept2, 'pa': pa2}\n",
    "            refined.extend(refine_group(sub_group1, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "            refined.extend(refine_group(sub_group2, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined\n",
    "\n",
    "def refine_all_groups(groups, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points=5):\n",
    "    refined_groups = []\n",
    "    for g in groups:\n",
    "        refined_groups.extend(refine_group(g, dispersion_threshold, gap_threshold_theta, gap_threshold_r, min_points))\n",
    "    return refined_groups\n",
    "\n",
    "def subdivide_if_gap_final(group, max_theta_gap=15.0):\n",
    "    pts = group['points'].sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_theta_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    model = LinearRegression()\n",
    "    if len(pts1) >= 2:\n",
    "        model.fit(pts1[['theta']], pts1['r'])\n",
    "        s1, i1 = model.coef_[0], model.intercept_\n",
    "        pa1 = calculate_pa(s1, i1)\n",
    "    else:\n",
    "        s1, i1, pa1 = None, None, None\n",
    "    if len(pts2) >= 2:\n",
    "        model.fit(pts2[['theta']], pts2['r'])\n",
    "        s2, i2 = model.coef_[0], model.intercept_\n",
    "        pa2 = calculate_pa(s2, i2)\n",
    "    else:\n",
    "        s2, i2, pa2 = None, None, None\n",
    "    seg1 = {'group_id': group.get('group_id'), 'points': pts1, 'slope': s1, 'intercept': i1, 'pa': pa1}\n",
    "    seg2 = {'group_id': group.get('group_id'), 'points': pts2, 'slope': s2, 'intercept': i2, 'pa': pa2}\n",
    "    return [seg1, seg2]\n",
    "\n",
    "def finalize_groups(groups, max_theta_gap=15.0, flexible_pa_range=(4.0, 30.0)):\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap_final(g, max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    return valid_groups\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Extrapolación de Good Groups\n",
    "# =============================================================================\n",
    "\n",
    "def extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.28, theta_relative_margin=0.18, r_relative_margin=0.18):\n",
    "    \"\"\"\n",
    "    Para cada Good Group, revisa candidatos en refined_groups (diferentes del grupo)\n",
    "    y, si la diferencia relativa en pendiente es ≤28% y los rangos de θ y r se solapan dentro\n",
    "    del 18% de su rango, fusiona los puntos y recalcula el ajuste.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    updated_good_groups = {}\n",
    "    for key, entry in good_groups.items():\n",
    "        good_group = entry['group']\n",
    "        good_theta_min = good_group['points']['theta'].min()\n",
    "        good_theta_max = good_group['points']['theta'].max()\n",
    "        theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "        \n",
    "        good_r_min = good_group['points']['r'].min()\n",
    "        good_r_max = good_group['points']['r'].max()\n",
    "        r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "        \n",
    "        for candidate in refined_groups:\n",
    "            if candidate is good_group:\n",
    "                continue\n",
    "            if good_group['slope'] is None or candidate['slope'] is None:\n",
    "                continue\n",
    "            relative_diff = abs(candidate['slope'] - good_group['slope']) / (abs(good_group['slope']) + 1e-12)\n",
    "            if relative_diff <= slope_variation_merge:\n",
    "                cand_theta_min = candidate['points']['theta'].min()\n",
    "                cand_theta_max = candidate['points']['theta'].max()\n",
    "                cand_r_min = candidate['points']['r'].min()\n",
    "                cand_r_max = candidate['points']['r'].max()\n",
    "                theta_condition = (cand_theta_min >= good_theta_min - theta_relative_margin * theta_range and\n",
    "                                   cand_theta_max <= good_theta_max + theta_relative_margin * theta_range)\n",
    "                r_condition = (cand_r_min >= good_r_min - r_relative_margin * r_range and\n",
    "                               cand_r_max <= good_r_max + r_relative_margin * r_range)\n",
    "                if theta_condition and r_condition:\n",
    "                    merged_points = pd.concat([good_group['points'], candidate['points']], ignore_index=True)\n",
    "                    if len(merged_points) >= 2:\n",
    "                        model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                        new_slope = model.coef_[0]\n",
    "                        new_intercept = model.intercept_\n",
    "                        new_pa = calculate_pa(new_slope, new_intercept)\n",
    "                    else:\n",
    "                        new_slope, new_intercept, new_pa = good_group['slope'], good_group['intercept'], good_group['pa']\n",
    "                    good_group['points'] = merged_points\n",
    "                    good_group['slope'] = new_slope\n",
    "                    good_group['intercept'] = new_intercept\n",
    "                    good_group['pa'] = new_pa\n",
    "                    good_theta_min = good_group['points']['theta'].min()\n",
    "                    good_theta_max = good_group['points']['theta'].max()\n",
    "                    theta_range = good_theta_max - good_theta_min if good_theta_max - good_theta_min != 0 else 1.0\n",
    "                    good_r_min = good_group['points']['r'].min()\n",
    "                    good_r_max = good_group['points']['r'].max()\n",
    "                    r_range = good_r_max - good_r_min if good_r_max - good_r_min != 0 else 1.0\n",
    "        updated_good_groups[key] = {'group': good_group, 'dispersion': compute_dispersion(good_group)}\n",
    "    return updated_good_groups\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Funciones de Visualización\n",
    "# =============================================================================\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f'Grupo {idx+1}')\n",
    "        if g['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    if 'temp_points' in state and not state['temp_points'].empty:\n",
    "        plt.scatter(state['temp_points']['theta'], state['temp_points']['r'], s=10, color='black', marker='x', label='Temp')\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_final_groups(final_groups, df_all):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos completos')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan']\n",
    "    for idx, grp in enumerate(final_groups):\n",
    "        pts = grp['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=color, label=f\"Segmento {idx+1}\")\n",
    "        if grp['slope'] is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = grp['slope'] * theta_line + grp['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', color=color, linewidth=1.5)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Segmentos Finales (Plano r, θ)\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Proceso Integrado Mejorado\n",
    "# =============================================================================\n",
    "\n",
    "def process_spiral_arms_integrated_improved(\n",
    "    id_halo,\n",
    "    data_folder,\n",
    "    quartile_threshold=0.55,\n",
    "    eps=0.198,\n",
    "    min_samples=18,\n",
    "    min_seed_points=60,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    snapshot_interval=10,\n",
    "    max_theta_gap=15.0,\n",
    "    flexible_pa_range=(4.0,30.0),\n",
    "    dispersion_threshold=0.35\n",
    "):\n",
    "    \"\"\"\n",
    "    Proceso completo mejorado:\n",
    "      1. Lee y filtra los datos para el halo.\n",
    "      2. Obtiene semillas aplicando DBSCAN en el espacio cartesiano y transforma a polar (fusionando clusters cercanos).\n",
    "      3. Genera grupos iniciales mediante ajuste lineal.\n",
    "      4. Realiza una subdivisión final de los grupos.\n",
    "      5. Refina los grupos según su dispersión.\n",
    "      6. Selecciona los Good Groups (grupos con dispersión aceptable y PA válido).\n",
    "      7. Extrapola los Good Groups incorporando candidatos cuyo ajuste varíe ≤28%\n",
    "         y cuyos rangos en θ y r se solapen dentro del 18%.\n",
    "    \"\"\"\n",
    "    # Paso 1: Cargar datos\n",
    "    df = load_data(id_halo, quartile_threshold)\n",
    "    # Paso 2: Obtener semillas\n",
    "    seed_clusters = dbscan_seeds(df, eps=eps, min_samples=min_samples, min_seed_points=min_seed_points)\n",
    "    # Paso 3: Grupos iniciales a partir de semillas\n",
    "    groups = iterative_assignment(seed_clusters, theta_fit_window, r_fit_window, radius_tolerance, slope_variation_threshold, snapshot_interval)\n",
    "    # Paso 4: Subdivisión final de grupos\n",
    "    prelim_groups = finalize_groups(groups, max_theta_gap=max_theta_gap, flexible_pa_range=flexible_pa_range)\n",
    "    # Paso 5: Refinar grupos según dispersión\n",
    "    refined_groups = refine_all_groups(prelim_groups, dispersion_threshold, gap_threshold_theta=2.5, gap_threshold_r=2.5)\n",
    "    # Paso 6: Seleccionar Good Groups\n",
    "    good_groups = {}\n",
    "    good_counter = 0\n",
    "    for g in refined_groups:\n",
    "        disp = compute_dispersion(g)\n",
    "        if disp <= dispersion_threshold and g['slope'] is not None and g['intercept'] is not None and \\\n",
    "           g['pa'] is not None and flexible_pa_range[0] <= abs(g['pa']) <= flexible_pa_range[1]:\n",
    "            good_groups[good_counter] = {'group': g, 'dispersion': disp}\n",
    "            good_counter += 1\n",
    "    # Paso 7: Extrapolar ajustes en los Good Groups\n",
    "    updated_good_groups = extrapolate_good_groups(good_groups, refined_groups, slope_variation_merge=0.28, theta_relative_margin=0.18, r_relative_margin=0.18)\n",
    "    \n",
    "    return df, groups, prelim_groups, refined_groups, updated_good_groups\n",
    "\n",
    "# =============================================================================\n",
    "# Bloque Principal de Ejecución\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parámetros (ajusta según sea necesario)\n",
    "    id_halo = \"17\"\n",
    "    data_folder = \"../DATA/processed/TracingPoints\"\n",
    "    quartile_threshold = 0.55\n",
    "    eps = 0.198\n",
    "    min_samples = 18\n",
    "    min_seed_points = 60\n",
    "    theta_fit_window = 30.0\n",
    "    r_fit_window = 3.0\n",
    "    radius_tolerance = 0.45\n",
    "    slope_variation_threshold = 0.20\n",
    "    snapshot_interval = 10\n",
    "    max_theta_gap = 15.0\n",
    "    flexible_pa_range = (4.0, 30.0)\n",
    "    dispersion_threshold = 0.35\n",
    "\n",
    "    df, groups, prelim_groups, refined_groups, good_groups = process_spiral_arms_integrated_improved(\n",
    "        id_halo=id_halo,\n",
    "        data_folder=data_folder,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        eps=eps,\n",
    "        min_samples=min_samples,\n",
    "        min_seed_points=min_seed_points,\n",
    "        theta_fit_window=theta_fit_window,\n",
    "        r_fit_window=r_fit_window,\n",
    "        radius_tolerance=radius_tolerance,\n",
    "        slope_variation_threshold=slope_variation_threshold,\n",
    "        snapshot_interval=snapshot_interval,\n",
    "        max_theta_gap=max_theta_gap,\n",
    "        flexible_pa_range=flexible_pa_range,\n",
    "        dispersion_threshold=dispersion_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de grupos iniciales: {len(groups)}\")\n",
    "    print(f\"Total de grupos preliminares: {len(prelim_groups)}\")\n",
    "    print(f\"Total de grupos refinados: {len(refined_groups)}\")\n",
    "    print(f\"Total de Good Groups extrapolados: {len(good_groups)}\")\n",
    "    \n",
    "    good_groups_list = [entry['group'] for entry in good_groups.values()]\n",
    "    if good_groups_list:\n",
    "        print(\"Mostrando Good Groups extrapolados:\")\n",
    "        plot_final_groups(good_groups_list, df)\n",
    "    else:\n",
    "        print(\"No se encontraron Good Groups que cumplan con los criterios.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80c639e7-9b34-4463-a80e-fe4804f097f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### último intento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b36d34e-68ea-4f0e-b977-0e21b0e18fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIONES AUXILIARES DEL CÓDIGO 2 ==========\n",
    "# =========================================================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min=50, theta_max=250):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula coordenadas polares y filtra\n",
    "    según el rango angular [theta_min, theta_max].\n",
    "    Ajusta la columna 'theta' para que esté en [0, 360).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    return df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo donde cada punto se conecta a otros\n",
    "    si las diferencias en θ y r son <= a los umbrales dados.\n",
    "    Retorna: (graph, n) donde graph[i] es la lista de vecinos de i.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Vecinos cumplen (dtheta <= theta_diff) & (dr <= r_diff), excluyendo i mismo\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas del grafo usando BFS.\n",
    "    Cada componente es un DataFrame con los puntos de ese subgrupo.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide el DataFrame (ordenado por 'theta' o 'r') en subclusters\n",
    "    si hay saltos (gaps) mayores a gap_threshold.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    \n",
    "    # Si no hay gap mayor al umbral, devolvemos un solo subcluster\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    \n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    df_base,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Toma un DF (ya filtrado en θ si es necesario), filtra por quantile\n",
    "    en 'rho_resta_final_exp', luego construye grafo y extrae\n",
    "    componentes BFS. Finalmente, subdivide cada componente\n",
    "    por gaps en θ y r.\n",
    "    \n",
    "    Devuelve una lista de DataFrames (clusters finales) y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    threshold = df_base['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df_base[df_base['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        # Subdividir por gaps en θ\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            # Subdividir por gaps en r\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIONES AUXILIARES DEL CÓDIGO 1 ==========\n",
    "# =========================================================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"\n",
    "    Calcula la distancia vertical de un punto (theta, r)\n",
    "    a la recta r_pred = slope * theta + intercept.\n",
    "    \"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo PA en grados dado slope e intercept.\n",
    "    Si intercept es 0, se retorna NaN.\n",
    "    Ajustar la fórmula según tu convención.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window):\n",
    "    \"\"\"\n",
    "    Agrega un punto a un temp_group si encaja (según min/max theta/r).\n",
    "    De lo contrario, crea un nuevo temp_group.\n",
    "    \"\"\"\n",
    "    assigned = False\n",
    "    for tg in temp_groups:\n",
    "        tg_theta_min = tg['points']['theta'].min()\n",
    "        tg_theta_max = tg['points']['theta'].max()\n",
    "        tg_r_min = tg['points']['r'].min()\n",
    "        tg_r_max = tg['points']['r'].max()\n",
    "        if ((point['theta'] >= tg_theta_min - theta_fit_window/2) and \n",
    "            (point['theta'] <= tg_theta_max + theta_fit_window/2)) and \\\n",
    "           ((point['r'] >= tg_r_min - r_fit_window/2) and \n",
    "            (point['r'] <= tg_r_max + r_fit_window/2)):\n",
    "            tg['points'] = pd.concat([tg['points'], point.to_frame().T], ignore_index=True)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        new_tg = {'points': point.to_frame().T.copy()}\n",
    "        temp_groups.append(new_tg)\n",
    "\n",
    "def review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range):\n",
    "    \"\"\"\n",
    "    Revisa los temp_groups. \n",
    "    - Si tienen <3 puntos, no se ajustan todavía.\n",
    "    - Si >=3, se hace ajuste lineal y se trata de fusionar/promover.\n",
    "    - Se validan distancias y variación de pendiente con los grupos principales.\n",
    "    \"\"\"\n",
    "    new_temp_groups = []\n",
    "    for tg in temp_groups:\n",
    "        # Si muy pocos puntos, los dejamos tal cual\n",
    "        if len(tg['points']) < 3:\n",
    "            new_temp_groups.append(tg)\n",
    "            continue\n",
    "        \n",
    "        # Ajuste lineal del temp_group\n",
    "        X_tg = tg['points'][['theta']]\n",
    "        y_tg = tg['points']['r']\n",
    "        model.fit(X_tg, y_tg)\n",
    "        tg_slope = model.coef_[0]\n",
    "        tg_intercept = model.intercept_\n",
    "        \n",
    "        # Promediamos la theta para ver dónde \"cae\"\n",
    "        tg_mean = tg['points']['theta'].mean()\n",
    "        merged = False\n",
    "        \n",
    "        # Intentamos ver si se fusiona con algún grupo principal\n",
    "        for mg in groups:\n",
    "            mg_min = mg['points']['theta'].min()\n",
    "            mg_max = mg['points']['theta'].max()\n",
    "            if mg_min <= tg_mean <= mg_max:\n",
    "                # Calculamos distancias con la pendiente del mg\n",
    "                dist_values = tg['points'].apply(\n",
    "                    lambda row: calculate_distance(mg['slope'], mg['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if dist_values.mean() < radius_tolerance:\n",
    "                    # Fusionar\n",
    "                    mg['points'] = pd.concat([mg['points'], tg['points']], ignore_index=True)\n",
    "                    model.fit(mg['points'][['theta']], mg['points']['r'])\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = mg['slope']\n",
    "                    # Ver si la variación de pendiente es aceptable\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) < slope_variation_threshold:\n",
    "                        mg['slope'] = new_slope\n",
    "                        mg['intercept'] = model.intercept_\n",
    "                        mg['pa'] = calculate_pa(mg['slope'], mg['intercept'])\n",
    "                    merged = True\n",
    "                    break\n",
    "        if not merged:\n",
    "            # No se fusionó -> crear un nuevo grupo principal\n",
    "            new_group = {\n",
    "                'cluster_id': None,\n",
    "                'points': tg['points'],\n",
    "                'slope': tg_slope,\n",
    "                'intercept': tg_intercept,\n",
    "                'pa': calculate_pa(tg_slope, tg_intercept)\n",
    "            }\n",
    "            groups.append(new_group)\n",
    "    \n",
    "    # Limpiar temp_groups y reinsertar los que quedaban\n",
    "    temp_groups.clear()\n",
    "    for tg in new_temp_groups:\n",
    "        temp_groups.append(tg)\n",
    "\n",
    "def subdivide_if_gap(group, max_gap=15.0):\n",
    "    \"\"\"\n",
    "    Toma un grupo (dict con 'points') y, si hay un gap grande en θ, \n",
    "    lo divide en dos subgrupos. Retorna una lista (1 o 2 subgrupos).\n",
    "    \"\"\"\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        model = LinearRegression()\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        # No dividimos si hay muy pocos puntos\n",
    "        return [group]\n",
    "    \n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    \n",
    "    # Tomamos el primer gap\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    \n",
    "    s1, i1, pa1 = fit_line(pts1)\n",
    "    s2, i2, pa2 = fit_line(pts2)\n",
    "    \n",
    "    seg1 = {\n",
    "        'cluster_id': group['cluster_id'],\n",
    "        'points': pts1,\n",
    "        'slope': s1,\n",
    "        'intercept': i1,\n",
    "        'pa': pa1\n",
    "    }\n",
    "    seg2 = {\n",
    "        'cluster_id': group['cluster_id'],\n",
    "        'points': pts2,\n",
    "        'slope': s2,\n",
    "        'intercept': i2,\n",
    "        'pa': pa2\n",
    "    }\n",
    "    return [seg1, seg2]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIÓN PRINCIPAL UNIFICADA ===============\n",
    "# =========================================================\n",
    "\n",
    "def process_spiral_arms_unificado(\n",
    "    id_halo,\n",
    "    # --- Parámetros BFS / seeds ---\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50, theta_max=250,\n",
    "    theta_diff=5.0,          # vecindad horizontal BFS\n",
    "    r_diff=1.0,              # vecindad radial BFS\n",
    "    gap_threshold_theta=2.5, # gap en θ\n",
    "    gap_threshold_r=2.5,     # gap en r\n",
    "    min_seed_size=60,        # tamaño mínimo de semilla\n",
    "\n",
    "    # --- Parámetros del proceso iterativo (código 1) ---\n",
    "    radius_tolerance=0.45,    \n",
    "    slope_variation_threshold=0.20,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    max_theta_gap=15.0,      # para subdividir\n",
    "    snapshot_interval=10,    # intervalo para guardar states\n",
    "    review_interval=50,      # cada cuántas iteraciones revisamos temp_groups\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    fusion_theta_threshold=7.0,\n",
    "    fusion_r_threshold=7.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Fusión de técnicas:\n",
    "    1) Se obtienen \"semillas\" via BFS y subdivisión por gaps (código 2).\n",
    "    2) Se inicializan grupos con slope/intercept calculados.\n",
    "    3) Se itera sobre el resto de puntos al estilo del código 1, \n",
    "       validando distancias, pendiente, etc.\n",
    "    4) Se revisan temp_groups y se promueven/fusionan cuando aplicable.\n",
    "    5) Al final se subdividen los grupos si hay gaps grandes en θ.\n",
    "    6) Se filtran los grupos cuya pa no cae en [flexible_pa_range].\n",
    "\n",
    "    Retorna:\n",
    "      - states: lista de diccionarios con estados intermedios\n",
    "      - final_groups: lista de grupos resultantes\n",
    "      - df_filtered: DataFrame filtrado por quartile\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Cargar datos base y filtrar por theta\n",
    "    df_base = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    \n",
    "    # 2. Semillas mediante BFS + gaps\n",
    "    seeds_all, df_filtered = process_clusters_rectangular_gaps(\n",
    "        df_base,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    \n",
    "    # 3. Quedarnos con seeds grandes\n",
    "    seed_clusters = [s for s in seeds_all if len(s) >= min_seed_size]\n",
    "    \n",
    "    # 4. Marcar en df_filtered cuáles puntos son semilla\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed_df in seed_clusters:\n",
    "        seed_indices.update(seed_df.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 5. Inicializar grupos con slope/intercept a partir de las semillas\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for idx, seed_pts in enumerate(seed_clusters):\n",
    "        if len(seed_pts) >= 2:\n",
    "            X_ = seed_pts[['theta']]\n",
    "            y_ = seed_pts['r']\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        \n",
    "        groups.append({\n",
    "            'cluster_id': idx,\n",
    "            'points': seed_pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # 6. Puntos restantes a df_local\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    df_local.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 7. Iteración al estilo \"código 1\"\n",
    "    temp_groups = []\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    \n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        \n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Intentar asignar a un grupo existente\n",
    "        for g in groups:\n",
    "            if g['slope'] is None:\n",
    "                # Si no tiene slope no podemos verificar\n",
    "                continue\n",
    "            \n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            \n",
    "            # Chequeo rápido: ¿está el punto dentro de [theta_min,theta_max] y [r_min,r_max] con algo de margen?\n",
    "            # (Opcional, a tu gusto: si no se cumple, skip)\n",
    "            if not (g_theta_min - theta_fit_window <= theta_pt <= g_theta_max + theta_fit_window):\n",
    "                continue\n",
    "            if not (g_r_min - r_fit_window <= r_pt <= g_r_max + r_fit_window):\n",
    "                continue\n",
    "            \n",
    "            # Tomar subset local alrededor del punto\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            \n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Ajustar recta al subset\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            \n",
    "            # Verificamos variación de pendiente\n",
    "            if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) > slope_variation_threshold:\n",
    "                continue\n",
    "            \n",
    "            # Chequeamos la distancia del punto a esta nueva recta\n",
    "            dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "            if dist > radius_tolerance:\n",
    "                continue\n",
    "            \n",
    "            # Si pasa todo, es candidato\n",
    "            if (best_distance is None) or (dist < best_distance):\n",
    "                best_distance = dist\n",
    "                best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            # Asignamos el punto al best_group\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            \n",
    "            # Reajustamos la pendiente del grupo (con subset local o todo el grupo, a tu criterio)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset_bg = best_group['points'][\n",
    "                (best_group['points']['theta'] >= window_min) & \n",
    "                (best_group['points']['theta'] <= window_max)\n",
    "            ]\n",
    "            if len(subset_bg) < 2:\n",
    "                # Si quedamos con un subset muy chico, usamos todos los puntos\n",
    "                subset_bg = best_group['points']\n",
    "            \n",
    "            model.fit(subset_bg[['theta']], subset_bg['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "            \n",
    "            # Fusión fija con grupos cercanos (opcional, usando la lógica de \"código 1\")\n",
    "            merged_flag = True\n",
    "            while merged_flag:\n",
    "                merged_flag = False\n",
    "                for other_group in groups:\n",
    "                    if other_group is best_group:\n",
    "                        continue\n",
    "                    if other_group['slope'] is None:\n",
    "                        continue\n",
    "                    best_theta_min = best_group['points']['theta'].min()\n",
    "                    best_theta_max = best_group['points']['theta'].max()\n",
    "                    best_r_min = best_group['points']['r'].min()\n",
    "                    best_r_max = best_group['points']['r'].max()\n",
    "\n",
    "                    other_theta_min = other_group['points']['theta'].min()\n",
    "                    other_theta_max = other_group['points']['theta'].max()\n",
    "                    other_r_min = other_group['points']['r'].min()\n",
    "                    other_r_max = other_group['points']['r'].max()\n",
    "\n",
    "                    # Criterio de fusión: si los límites en θ y r están lo bastante cerca\n",
    "                    if (abs(best_theta_min - other_theta_max) < fusion_theta_threshold and\n",
    "                        abs(best_theta_max - other_theta_min) < fusion_theta_threshold and\n",
    "                        abs(best_r_min - other_r_max) < fusion_r_threshold and\n",
    "                        abs(best_r_max - other_r_min) < fusion_r_threshold):\n",
    "                        \n",
    "                        # Fusionar\n",
    "                        merged_points = pd.concat([best_group['points'], other_group['points']], ignore_index=True)\n",
    "                        model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                        best_group['points'] = merged_points\n",
    "                        best_group['slope'] = model.coef_[0]\n",
    "                        best_group['intercept'] = model.intercept_\n",
    "                        best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "                        for idx, grp in enumerate(groups):\n",
    "                            if grp is other_group:\n",
    "                                groups.pop(idx)\n",
    "                                break\n",
    "\n",
    "                        merged_flag = True\n",
    "                        break\n",
    "            \n",
    "        else:\n",
    "            # No se pudo asignar -> va a temp_groups\n",
    "            add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window)\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Revisar temp_groups cada 'review_interval' iteraciones\n",
    "        if iteration % review_interval == 0:\n",
    "            review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "        \n",
    "        # Guardar snapshot\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_groups': copy.deepcopy(temp_groups),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # Revisar temp_groups final\n",
    "    if len(temp_groups) > 0:\n",
    "        review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "    \n",
    "    # Subdividir si hay gaps\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    \n",
    "    # Filtrar grupos con pa fuera de flexible_pa_range\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    # Guardar estado final\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'temp_groups': copy.deepcopy(temp_groups),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ OPCIONAL: FUNCIÓN DE PLOTEO =================\n",
    "# =========================================================\n",
    "\n",
    "def plot_intermediate_state_unificado(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica un estado en el plano (theta, r).\n",
    "    df_all son todos los puntos usados (por ejemplo df_filtered).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Puntos de fondo\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos totales')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, label=f\"Grupo {idx+1}\")\n",
    "        if g.get('slope') is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', linewidth=1.5)\n",
    "    \n",
    "    # Temp groups (opcional)\n",
    "    if 'temp_groups' in state and state['temp_groups']:\n",
    "        for idx, tg in enumerate(state['temp_groups']):\n",
    "            pts = tg['points']\n",
    "            plt.scatter(pts['theta'], pts['r'], s=10, color='black', marker='x', label=f\"Temp {idx+1}\")\n",
    "    \n",
    "    plt.xlabel(\"theta (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']}\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ EJEMPLO DE USO =============================\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Ejemplo rápido de cómo llamar a la función unificada.\n",
    "    Ajusta parámetros y rutas a tus necesidades.\n",
    "    \"\"\"\n",
    "\n",
    "    states, final_groups, df_filtered = process_spiral_arms_unificado(\n",
    "        id_halo=\"17\",\n",
    "        quartile_threshold=0.155,\n",
    "        theta_min=50,\n",
    "        theta_max=250,\n",
    "        theta_diff=2.0,\n",
    "        r_diff=0.468,\n",
    "        gap_threshold_theta=1.89,\n",
    "        gap_threshold_r=0.523,\n",
    "        min_seed_size=50,\n",
    "        radius_tolerance=0.45,\n",
    "        slope_variation_threshold=0.20,\n",
    "        flexible_pa_range=(4.0, 30.0),\n",
    "        max_theta_gap=15.0,\n",
    "        snapshot_interval=10,\n",
    "        review_interval=30,\n",
    "        theta_fit_window=30.0,\n",
    "        r_fit_window=10.0,\n",
    "        fusion_theta_threshold=7.0,\n",
    "        fusion_r_threshold=7.0\n",
    "    )\n",
    "\n",
    "    print(f\"Total de estados guardados: {len(states)}\")\n",
    "    print(f\"Total de grupos finales: {len(final_groups)}\")\n",
    "    \n",
    "    # Visualizar un estado específico\n",
    "    # (Requiere Jupyter o similar para interact(...))\n",
    "    # from ipywidgets import interact, IntSlider\n",
    "    # def view_state(i=0):\n",
    "    #     plot_intermediate_state_unificado(states[i], df_filtered)\n",
    "    # interact(view_state, i=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n",
    "\n",
    "    \n",
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state_cartesian(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd3aeb6-0876-42f3-a0a6-0be9e325cf9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIONES AUXILIARES DEL CÓDIGO 2 ==========\n",
    "# =========================================================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min=50, theta_max=250):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula coordenadas polares y filtra\n",
    "    según el rango angular [theta_min, theta_max].\n",
    "    Ajusta la columna 'theta' para que esté en [0, 360).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    return df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo donde cada punto se conecta a otros\n",
    "    si las diferencias en θ y r son <= a los umbrales dados.\n",
    "    Retorna: (graph, n) donde graph[i] es la lista de vecinos de i.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Vecinos cumplen (dtheta <= theta_diff) & (dr <= r_diff), excluyendo i mismo\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas del grafo usando BFS.\n",
    "    Cada componente es un DataFrame con los puntos de ese subgrupo.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide el DataFrame (ordenado por 'theta' o 'r') en subclusters\n",
    "    si hay saltos (gaps) mayores a gap_threshold.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    \n",
    "    # Si no hay gap mayor al umbral, devolvemos un solo subcluster\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    \n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    df_base,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=1.5,       # Más pequeño => semillas iniciales más pequeñas\n",
    "    r_diff=0.3,           # Más pequeño => semillas iniciales más pequeñas\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Toma un DF (ya filtrado en θ si es necesario), filtra por quantile\n",
    "    en 'rho_resta_final_exp', luego construye grafo y extrae\n",
    "    componentes BFS. Finalmente, subdivide cada componente\n",
    "    por gaps en θ y r.\n",
    "    \n",
    "    Devuelve una lista de DataFrames (clusters finales) y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    threshold = df_base['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df_base[df_base['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        # Subdividir por gaps en θ\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            # Subdividir por gaps en r\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIONES AUXILIARES DEL CÓDIGO 1 ==========\n",
    "# =========================================================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"\n",
    "    Calcula la distancia vertical de un punto (theta, r)\n",
    "    a la recta r_pred = slope * theta + intercept.\n",
    "    \"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo PA en grados dado slope e intercept.\n",
    "    Si intercept es 0, se retorna NaN.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window):\n",
    "    \"\"\"\n",
    "    Agrega un punto a un temp_group si encaja (según min/max theta/r).\n",
    "    De lo contrario, crea un nuevo temp_group.\n",
    "    \"\"\"\n",
    "    assigned = False\n",
    "    for tg in temp_groups:\n",
    "        tg_theta_min = tg['points']['theta'].min()\n",
    "        tg_theta_max = tg['points']['theta'].max()\n",
    "        tg_r_min = tg['points']['r'].min()\n",
    "        tg_r_max = tg['points']['r'].max()\n",
    "        if ((point['theta'] >= tg_theta_min - theta_fit_window/2) and \n",
    "            (point['theta'] <= tg_theta_max + theta_fit_window/2)) and \\\n",
    "           ((point['r'] >= tg_r_min - r_fit_window/2) and \n",
    "            (point['r'] <= tg_r_max + r_fit_window/2)):\n",
    "            tg['points'] = pd.concat([tg['points'], point.to_frame().T], ignore_index=True)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        new_tg = {'points': point.to_frame().T.copy()}\n",
    "        temp_groups.append(new_tg)\n",
    "\n",
    "def review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range):\n",
    "    \"\"\"\n",
    "    Revisa los temp_groups. \n",
    "    - Si tienen <3 puntos, no se ajustan todavía.\n",
    "    - Si >=3, se hace ajuste lineal y se trata de fusionar/promover.\n",
    "    - Se validan distancias y variación de pendiente con los grupos principales.\n",
    "    \"\"\"\n",
    "    new_temp_groups = []\n",
    "    for tg in temp_groups:\n",
    "        # Si muy pocos puntos, los dejamos tal cual\n",
    "        if len(tg['points']) < 3:\n",
    "            new_temp_groups.append(tg)\n",
    "            continue\n",
    "        \n",
    "        # Ajuste lineal del temp_group\n",
    "        X_tg = tg['points'][['theta']]\n",
    "        y_tg = tg['points']['r']\n",
    "        model.fit(X_tg, y_tg)\n",
    "        tg_slope = model.coef_[0]\n",
    "        tg_intercept = model.intercept_\n",
    "        \n",
    "        # Promediamos la theta para ver dónde \"cae\"\n",
    "        tg_mean = tg['points']['theta'].mean()\n",
    "        merged = False\n",
    "        \n",
    "        # Intentamos ver si se fusiona con algún grupo principal\n",
    "        for mg in groups:\n",
    "            mg_min = mg['points']['theta'].min()\n",
    "            mg_max = mg['points']['theta'].max()\n",
    "            if mg_min <= tg_mean <= mg_max:\n",
    "                # Calculamos distancias con la pendiente del mg\n",
    "                dist_values = tg['points'].apply(\n",
    "                    lambda row: calculate_distance(mg['slope'], mg['intercept'], row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if dist_values.mean() < radius_tolerance:\n",
    "                    # Fusionar\n",
    "                    mg['points'] = pd.concat([mg['points'], tg['points']], ignore_index=True)\n",
    "                    model.fit(mg['points'][['theta']], mg['points']['r'])\n",
    "                    new_slope = model.coef_[0]\n",
    "                    old_slope = mg['slope']\n",
    "                    # Ver si la variación de pendiente es aceptable\n",
    "                    if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) < slope_variation_threshold:\n",
    "                        mg['slope'] = new_slope\n",
    "                        mg['intercept'] = model.intercept_\n",
    "                        mg['pa'] = calculate_pa(mg['slope'], mg['intercept'])\n",
    "                    merged = True\n",
    "                    break\n",
    "        if not merged:\n",
    "            # No se fusionó -> crear un nuevo grupo principal\n",
    "            new_group = {\n",
    "                'cluster_id': None,\n",
    "                'points': tg['points'],\n",
    "                'slope': tg_slope,\n",
    "                'intercept': tg_intercept,\n",
    "                'pa': calculate_pa(tg_slope, tg_intercept)\n",
    "            }\n",
    "            groups.append(new_group)\n",
    "    \n",
    "    # Limpiar temp_groups y reinsertar los que quedaban\n",
    "    temp_groups.clear()\n",
    "    for tg in new_temp_groups:\n",
    "        temp_groups.append(tg)\n",
    "\n",
    "def subdivide_if_gap(group, max_gap=15.0):\n",
    "    \"\"\"\n",
    "    Toma un grupo (dict con 'points') y, si hay un gap grande en θ, \n",
    "    lo divide en dos subgrupos. Retorna una lista (1 o 2 subgrupos).\n",
    "    \"\"\"\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        model = LinearRegression()\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        # No dividimos si hay muy pocos puntos\n",
    "        return [group]\n",
    "    \n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    \n",
    "    # Tomamos el primer gap\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    \n",
    "    s1, i1, pa1 = fit_line(pts1)\n",
    "    s2, i2, pa2 = fit_line(pts2)\n",
    "    \n",
    "    seg1 = {\n",
    "        'cluster_id': group['cluster_id'],\n",
    "        'points': pts1,\n",
    "        'slope': s1,\n",
    "        'intercept': i1,\n",
    "        'pa': pa1\n",
    "    }\n",
    "    seg2 = {\n",
    "        'cluster_id': group['cluster_id'],\n",
    "        'points': pts2,\n",
    "        'slope': s2,\n",
    "        'intercept': i2,\n",
    "        'pa': pa2\n",
    "    }\n",
    "    return [seg1, seg2]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIÓN PRINCIPAL UNIFICADA ===============\n",
    "# =========================================================\n",
    "\n",
    "def process_spiral_arms_unificado(\n",
    "    id_halo,\n",
    "    # --- Parámetros BFS / seeds ---\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50, theta_max=250,\n",
    "    theta_diff=1.5,          # Ventanas pequeñas para BFS\n",
    "    r_diff=0.3,              # Ventanas pequeñas para BFS\n",
    "    gap_threshold_theta=2.5, # gap en θ\n",
    "    gap_threshold_r=2.5,     # gap en r\n",
    "    min_seed_size=60,        # tamaño mínimo de semilla\n",
    "\n",
    "    # --- Parámetros del proceso iterativo (código 1) ---\n",
    "    radius_tolerance=0.45,    \n",
    "    slope_variation_threshold=0.20,   # Umbral normal de variación\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    max_theta_gap=15.0,      # para subdividir\n",
    "    snapshot_interval=10,    # intervalo para guardar states\n",
    "    review_interval=50,      # cada cuántas iteraciones revisamos temp_groups\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    fusion_theta_threshold=7.0,\n",
    "    fusion_r_threshold=7.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Fusión de técnicas con modificaciones:\n",
    "      - Ventanas BFS más pequeñas (theta_diff, r_diff)\n",
    "      - Extrapolación 20% en θ y r\n",
    "      - Variación de pendiente hasta 30% en la zona extrapolada\n",
    "\n",
    "    Retorna:\n",
    "      - states: lista de diccionarios con estados intermedios\n",
    "      - final_groups: lista de grupos resultantes\n",
    "      - df_filtered: DataFrame filtrado por quartile\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Cargar datos base y filtrar por theta\n",
    "    df_base = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    \n",
    "    # 2. Semillas mediante BFS + gaps (ventanas más pequeñas)\n",
    "    seeds_all, df_filtered = process_clusters_rectangular_gaps(\n",
    "        df_base,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    \n",
    "    # 3. Quedarnos con seeds grandes\n",
    "    seed_clusters = [s for s in seeds_all if len(s) >= min_seed_size]\n",
    "    \n",
    "    # 4. Marcar en df_filtered cuáles puntos son semilla\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed_df in seed_clusters:\n",
    "        seed_indices.update(seed_df.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 5. Inicializar grupos con slope/intercept a partir de las semillas\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for idx, seed_pts in enumerate(seed_clusters):\n",
    "        if len(seed_pts) >= 2:\n",
    "            X_ = seed_pts[['theta']]\n",
    "            y_ = seed_pts['r']\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        else:\n",
    "            slope_init, intercept_init, pa_init = None, None, None\n",
    "        \n",
    "        groups.append({\n",
    "            'cluster_id': idx,\n",
    "            'points': seed_pts.copy(),\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # 6. Puntos restantes a df_local\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    df_local.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 7. Iteración\n",
    "    temp_groups = []\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    \n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        \n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        for g in groups:\n",
    "            if g['slope'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Hallar rangos en θ, r\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            \n",
    "            # Calculamos la extensión normal\n",
    "            # y la región extendida (20%)\n",
    "            theta_range = g_theta_max - g_theta_min\n",
    "            r_range = g_r_max - g_r_min\n",
    "            \n",
    "            # Zona normal\n",
    "            normal_theta_min = g_theta_min\n",
    "            normal_theta_max = g_theta_max\n",
    "            normal_r_min = g_r_min\n",
    "            normal_r_max = g_r_max\n",
    "            \n",
    "            # Extensión: 20% adicional a cada lado\n",
    "            ext_theta = 0.2 * theta_range\n",
    "            ext_r = 0.2 * r_range\n",
    "            \n",
    "            extended_theta_min = normal_theta_min - ext_theta\n",
    "            extended_theta_max = normal_theta_max + ext_theta\n",
    "            \n",
    "            extended_r_min = normal_r_min - ext_r\n",
    "            extended_r_max = normal_r_max + ext_r\n",
    "            \n",
    "            # Ver si el punto cae al menos en la región extendida\n",
    "            if not (extended_theta_min <= theta_pt <= extended_theta_max) or not (extended_r_min <= r_pt <= extended_r_max):\n",
    "                # Ni siquiera está en la zona extrapolada => skip\n",
    "                continue\n",
    "            \n",
    "            # Determinar si está en la zona \"normal\" o en la \"extendida\"\n",
    "            # para definir la variación de pendiente permitida\n",
    "            if (normal_theta_min <= theta_pt <= normal_theta_max) and (normal_r_min <= r_pt <= normal_r_max):\n",
    "                local_slope_threshold = slope_variation_threshold  # e.g. 20%\n",
    "            else:\n",
    "                # Está en la región extrapolada => 30%\n",
    "                local_slope_threshold = 0.30\n",
    "            \n",
    "            # Tomar subset local (como antes)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            \n",
    "            if len(subset) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Ajustar recta al subset\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            \n",
    "            # Verificamos variación de pendiente con 'local_slope_threshold'\n",
    "            if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) > local_slope_threshold:\n",
    "                continue\n",
    "            \n",
    "            # Chequeamos la distancia del punto a esta nueva recta\n",
    "            dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "            if dist > radius_tolerance:\n",
    "                continue\n",
    "            \n",
    "            # Si pasa todo, es candidato\n",
    "            if (best_distance is None) or (dist < best_distance):\n",
    "                best_distance = dist\n",
    "                best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            # Asignar el punto\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            \n",
    "            # Reajustamos la pendiente (con subset local o todo el grupo)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset_bg = best_group['points'][\n",
    "                (best_group['points']['theta'] >= window_min) & \n",
    "                (best_group['points']['theta'] <= window_max)\n",
    "            ]\n",
    "            if len(subset_bg) < 2:\n",
    "                subset_bg = best_group['points']\n",
    "            \n",
    "            model.fit(subset_bg[['theta']], subset_bg['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "            \n",
    "            # Fusión\n",
    "            merged_flag = True\n",
    "            while merged_flag:\n",
    "                merged_flag = False\n",
    "                for other_group in groups:\n",
    "                    if other_group is best_group or other_group['slope'] is None:\n",
    "                        continue\n",
    "                    best_theta_min = best_group['points']['theta'].min()\n",
    "                    best_theta_max = best_group['points']['theta'].max()\n",
    "                    best_r_min = best_group['points']['r'].min()\n",
    "                    best_r_max = best_group['points']['r'].max()\n",
    "\n",
    "                    other_theta_min = other_group['points']['theta'].min()\n",
    "                    other_theta_max = other_group['points']['theta'].max()\n",
    "                    other_r_min = other_group['points']['r'].min()\n",
    "                    other_r_max = other_group['points']['r'].max()\n",
    "\n",
    "                    # Criterio de fusión: si los límites en θ y r se solapan lo suficiente\n",
    "                    if (abs(best_theta_min - other_theta_max) < fusion_theta_threshold and\n",
    "                        abs(best_theta_max - other_theta_min) < fusion_theta_threshold and\n",
    "                        abs(best_r_min - other_r_max) < fusion_r_threshold and\n",
    "                        abs(best_r_max - other_r_min) < fusion_r_threshold):\n",
    "                        \n",
    "                        merged_points = pd.concat([best_group['points'], other_group['points']], ignore_index=True)\n",
    "                        model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                        best_group['points'] = merged_points\n",
    "                        best_group['slope'] = model.coef_[0]\n",
    "                        best_group['intercept'] = model.intercept_\n",
    "                        best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "                        for idx, grp in enumerate(groups):\n",
    "                            if grp is other_group:\n",
    "                                groups.pop(idx)\n",
    "                                break\n",
    "                        merged_flag = True\n",
    "                        break\n",
    "            \n",
    "        else:\n",
    "            # Si no hay grupo candidato, va a temp_groups\n",
    "            add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window)\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Revisar temp_groups cada 'review_interval'\n",
    "        if iteration % review_interval == 0:\n",
    "            review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "        \n",
    "        # Guardar snapshot\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_groups': copy.deepcopy(temp_groups),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # Revisar temp_groups final\n",
    "    if len(temp_groups) > 0:\n",
    "        review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "    \n",
    "    # Subdividir si hay gaps\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    \n",
    "    # Filtrar grupos con pa fuera de flexible_pa_range\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    # Guardar estado final\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'temp_groups': copy.deepcopy(temp_groups),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ OPCIONAL: FUNCIÓN DE PLOTEO ===============\n",
    "# =========================================================\n",
    "\n",
    "def plot_intermediate_state_unificado(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica un estado en el plano (theta, r).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos totales')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, label=f\"Grupo {idx+1}\")\n",
    "        if g.get('slope') is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', linewidth=1.5)\n",
    "    \n",
    "    if 'temp_groups' in state and state['temp_groups']:\n",
    "        for idx, tg in enumerate(state['temp_groups']):\n",
    "            pts = tg['points']\n",
    "            plt.scatter(pts['theta'], pts['r'], s=10, color='black', marker='x', label=f\"Temp {idx+1}\")\n",
    "    \n",
    "    plt.xlabel(\"theta (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ EJEMPLO DE USO =============================\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Ejemplo rápido de cómo llamar a la función unificada con los cambios.\n",
    "    \"\"\"\n",
    "\n",
    "    states, final_groups, df_filtered = process_spiral_arms_unificado(\n",
    "        id_halo=\"17\",\n",
    "        quartile_threshold=0.155,\n",
    "        theta_min=50,\n",
    "        theta_max=250,\n",
    "        # Ventanas BFS más pequeñas:\n",
    "        theta_diff=1.5,\n",
    "        r_diff=0.3,\n",
    "        gap_threshold_theta=1.89,\n",
    "        gap_threshold_r=0.523,\n",
    "        min_seed_size=50,\n",
    "        radius_tolerance=0.45,\n",
    "        # Umbral normal 20%, se usa 30% cuando el punto esté en región extrapolada:\n",
    "        slope_variation_threshold=0.20,\n",
    "        flexible_pa_range=(4.0, 30.0),\n",
    "        max_theta_gap=15.0,\n",
    "        snapshot_interval=10,\n",
    "        review_interval=30,\n",
    "        theta_fit_window=30.0,\n",
    "        r_fit_window=10.0,\n",
    "        fusion_theta_threshold=7.0,\n",
    "        fusion_r_threshold=7.0\n",
    "    )\n",
    "\n",
    "    print(f\"Total de estados guardados: {len(states)}\")\n",
    "    print(f\"Total de grupos finales: {len(final_groups)}\")\n",
    "    \n",
    "    # En un entorno interactivo:\n",
    "    # from ipywidgets import interact, IntSlider\n",
    "    # def view_state(i=0):\n",
    "    #     plot_intermediate_state_unificado(states[i], df_filtered)\n",
    "    # interact(view_state, i=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d2b79b-33a4-4b9b-a9ae-fd9c4e779d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state_cartesian(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3ffbddf-6ca6-4764-bdd4-1fbb8f95bf39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## buena aproximación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b80e81-2639-47c8-8407-51ea61aecf1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIONES AUXILIARES BFS (CÓDIGO 2) ========\n",
    "# =========================================================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min=50, theta_max=250):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula coordenadas polares y filtra\n",
    "    según el rango angular [theta_min, theta_max].\n",
    "    Ajusta la columna 'theta' para que esté en [0, 360).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    return df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo donde cada punto se conecta a otros\n",
    "    si las diferencias en θ y r son <= a los umbrales dados.\n",
    "    Retorna: (graph, n) donde graph[i] es la lista de vecinos de i.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Vecinos cumplen (dtheta <= theta_diff) & (dr <= r_diff), excluyendo i mismo\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas del grafo usando BFS.\n",
    "    Cada componente es un DataFrame con los puntos de ese subgrupo.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide el DataFrame (ordenado por 'theta' o 'r') en subclusters\n",
    "    si hay saltos (gaps) mayores a gap_threshold.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    \n",
    "    # Si no hay gap mayor al umbral, devolvemos un solo subcluster\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    \n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    df_base,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=2.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Toma un DF, filtra por quantile en 'rho_resta_final_exp', \n",
    "    construye grafo y extrae componentes BFS. \n",
    "    Luego subdivide cada componente por gaps en θ y r.\n",
    "    Devuelve una lista de DataFrames (clusters finales) y el DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    threshold = df_base['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df_base[df_base['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        # Subdividir por gaps en θ\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            # Subdividir por gaps en r\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIONES AUXILIARES (CÓDIGO 1) ============\n",
    "# =========================================================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"\n",
    "    Calcula la distancia vertical de un punto (theta, r)\n",
    "    a la recta r_pred = slope * theta + intercept.\n",
    "    \"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el ángulo PA en grados dado slope e intercept.\n",
    "    Si intercept es 0, se retorna NaN.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window):\n",
    "    \"\"\"\n",
    "    Agrega un punto a un temp_group si encaja (según min/max theta/r).\n",
    "    De lo contrario, crea un nuevo temp_group.\n",
    "    \"\"\"\n",
    "    assigned = False\n",
    "    for tg in temp_groups:\n",
    "        tg_theta_min = tg['points']['theta'].min()\n",
    "        tg_theta_max = tg['points']['theta'].max()\n",
    "        tg_r_min = tg['points']['r'].min()\n",
    "        tg_r_max = tg['points']['r'].max()\n",
    "        if ((point['theta'] >= tg_theta_min - theta_fit_window/2) and \n",
    "            (point['theta'] <= tg_theta_max + theta_fit_window/2)) and \\\n",
    "           ((point['r'] >= tg_r_min - r_fit_window/2) and \n",
    "            (point['r'] <= tg_r_max + r_fit_window/2)):\n",
    "            tg['points'] = pd.concat([tg['points'], point.to_frame().T], ignore_index=True)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        new_tg = {'points': point.to_frame().T.copy()}\n",
    "        temp_groups.append(new_tg)\n",
    "\n",
    "def review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range):\n",
    "    \"\"\"\n",
    "    Revisa los temp_groups. \n",
    "    - Si tienen <3 puntos, no se ajustan todavía.\n",
    "    - Si >=3, se hace ajuste lineal y se trata de fusionar/promover.\n",
    "    - Se validan distancias y variación de pendiente con los grupos principales.\n",
    "    \"\"\"\n",
    "    new_temp_groups = []\n",
    "    for tg in temp_groups:\n",
    "        # Si muy pocos puntos, los dejamos tal cual\n",
    "        if len(tg['points']) < 3:\n",
    "            new_temp_groups.append(tg)\n",
    "            continue\n",
    "        \n",
    "        # Ajuste lineal del temp_group\n",
    "        X_tg = tg['points'][['theta']]\n",
    "        y_tg = tg['points']['r']\n",
    "        model.fit(X_tg, y_tg)\n",
    "        tg_slope = model.coef_[0]\n",
    "        tg_intercept = model.intercept_\n",
    "        \n",
    "        # Promediamos la theta para ver dónde \"cae\"\n",
    "        tg_mean = tg['points']['theta'].mean()\n",
    "        merged = False\n",
    "        \n",
    "        # Intentamos ver si se fusiona con algún grupo principal\n",
    "        for mg in groups:\n",
    "            # Si mg no tiene slope, podemos forzar un ajuste rápido\n",
    "            # con los puntos actuales + mg, si se desea\n",
    "            if mg['slope'] is not None:\n",
    "                mg_min = mg['points']['theta'].min()\n",
    "                mg_max = mg['points']['theta'].max()\n",
    "                if mg_min <= tg_mean <= mg_max:\n",
    "                    dist_values = tg['points'].apply(\n",
    "                        lambda row: calculate_distance(mg['slope'], mg['intercept'], row['theta'], row['r']),\n",
    "                        axis=1\n",
    "                    )\n",
    "                    if dist_values.mean() < radius_tolerance:\n",
    "                        # Fusionar\n",
    "                        mg['points'] = pd.concat([mg['points'], tg['points']], ignore_index=True)\n",
    "                        # Reajustar slope en mg\n",
    "                        X_mg = mg['points'][['theta']]\n",
    "                        y_mg = mg['points']['r']\n",
    "                        model.fit(X_mg, y_mg)\n",
    "                        new_slope = model.coef_[0]\n",
    "                        old_slope = mg['slope']\n",
    "                        if old_slope is not None:\n",
    "                            if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) < slope_variation_threshold:\n",
    "                                mg['slope'] = new_slope\n",
    "                                mg['intercept'] = model.intercept_\n",
    "                                mg['pa'] = calculate_pa(mg['slope'], mg['intercept'])\n",
    "                        merged = True\n",
    "                        break\n",
    "            else:\n",
    "                # mg sin slope => Ver si queremos fusionar\n",
    "                # (Podrías hacer un check de solapamiento en θ/r)\n",
    "                pass\n",
    "        \n",
    "        if not merged:\n",
    "            # Crear un nuevo grupo principal\n",
    "            new_group = {\n",
    "                'cluster_id': None,\n",
    "                'points': tg['points'],\n",
    "                'slope': tg_slope,\n",
    "                'intercept': tg_intercept,\n",
    "                'pa': calculate_pa(tg_slope, tg_intercept)\n",
    "            }\n",
    "            groups.append(new_group)\n",
    "    \n",
    "    temp_groups.clear()\n",
    "    for tg in new_temp_groups:\n",
    "        temp_groups.append(tg)\n",
    "\n",
    "def subdivide_if_gap(group, max_gap=15.0):\n",
    "    \"\"\"\n",
    "    Toma un grupo (dict con 'points') y, si hay un gap grande en θ, \n",
    "    lo divide en dos subgrupos. Retorna una lista (1 o 2 subgrupos).\n",
    "    \"\"\"\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        model = LinearRegression()\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    \n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    \n",
    "    # Partimos en el primer gap\n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    \n",
    "    s1, i1, pa1 = fit_line(pts1)\n",
    "    s2, i2, pa2 = fit_line(pts2)\n",
    "    \n",
    "    seg1 = {\n",
    "        'cluster_id': group['cluster_id'],\n",
    "        'points': pts1,\n",
    "        'slope': s1,\n",
    "        'intercept': i1,\n",
    "        'pa': pa1\n",
    "    }\n",
    "    seg2 = {\n",
    "        'cluster_id': group['cluster_id'],\n",
    "        'points': pts2,\n",
    "        'slope': s2,\n",
    "        'intercept': i2,\n",
    "        'pa': pa2\n",
    "    }\n",
    "    return [seg1, seg2]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIÓN PRINCIPAL UNIFICADA ===============\n",
    "# =========================================================\n",
    "\n",
    "def process_spiral_arms_unificado(\n",
    "    id_halo,\n",
    "    # Parámetros BFS / seeds\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50, theta_max=250,\n",
    "    theta_diff=2.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5,\n",
    "    min_seed_size=60,\n",
    "\n",
    "    # Parámetros de la iteración (código 1)\n",
    "    radius_tolerance=0.45,    \n",
    "    slope_variation_threshold=0.20,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    max_theta_gap=15.0,\n",
    "    snapshot_interval=10,\n",
    "    review_interval=50,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    fusion_theta_threshold=7.0,\n",
    "    fusion_r_threshold=7.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Fusión de técnicas (sin ajuste inicial a las semillas):\n",
    "      1) Se obtienen \"semillas\" via BFS y subdivisión por gaps.\n",
    "         *No* se calcula slope/intercept para ellas.\n",
    "      2) Se descartan (o marcan) los puntos de las semillas y\n",
    "         se itera sobre el resto.\n",
    "      3) Si un punto encaja en un grupo que no tiene slope:\n",
    "         al tener >=2 puntos, se calcula por primera vez la slope.\n",
    "      4) Si un grupo ya tiene slope, se valida dist. a la recta,\n",
    "         variación de pendiente, etc.\n",
    "      5) Fusión de grupos si se superponen.\n",
    "      6) Al final, subdividir y filtrar por PA.\n",
    "\n",
    "    Retorna:\n",
    "      - states: lista de estados\n",
    "      - final_groups: lista final\n",
    "      - df_filtered: DataFrame filtrado\n",
    "    \"\"\"\n",
    "    # 1. Cargar datos base\n",
    "    df_base = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    \n",
    "    # 2. Semillas BFS + gaps\n",
    "    seeds_all, df_filtered = process_clusters_rectangular_gaps(\n",
    "        df_base,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    \n",
    "    # 3. Quedarnos con seeds grandes\n",
    "    seed_clusters = [s for s in seeds_all if len(s) >= min_seed_size]\n",
    "    \n",
    "    # 4. Marcar en df_filtered cuáles puntos son semilla\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed_df in seed_clusters:\n",
    "        seed_indices.update(seed_df.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 5. Crear grupos \"semilla\" sin slope/intercept:\n",
    "    groups = []\n",
    "    for idx, seed_pts in enumerate(seed_clusters):\n",
    "        groups.append({\n",
    "            'cluster_id': idx,\n",
    "            'points': seed_pts.copy(),\n",
    "            'slope': None,\n",
    "            'intercept': None,\n",
    "            'pa': None\n",
    "        })\n",
    "    \n",
    "    # 6. Puntos restantes (no en seed)\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    df_local.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 7. Iteración\n",
    "    model = LinearRegression()\n",
    "    temp_groups = []\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    \n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        \n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        for g in groups:\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            \n",
    "            # Verificamos si el punto puede entrar en la ventana\n",
    "            if not (g_theta_min - theta_fit_window <= theta_pt <= g_theta_max + theta_fit_window):\n",
    "                continue\n",
    "            if not (g_r_min - r_fit_window <= r_pt <= g_r_max + r_fit_window):\n",
    "                continue\n",
    "            \n",
    "            # pre-subset\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][(g['points']['theta'] >= window_min) & (g['points']['theta'] <= window_max)]\n",
    "            subset = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset) < 2:\n",
    "                # Con 1 solo punto no podemos calcular slope -> skip\n",
    "                continue\n",
    "            \n",
    "            # Ajustar recta para ver la slope resultante\n",
    "            model.fit(subset[['theta']], subset['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            \n",
    "            if g['slope'] is None:\n",
    "                # Grupo seed sin slope: no comparamos la variación, \n",
    "                # pero sí la distancia\n",
    "                dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "                if dist > radius_tolerance:\n",
    "                    continue\n",
    "            else:\n",
    "                # Si ya tenemos slope, comparamos la variación\n",
    "                old_slope = g['slope']\n",
    "                if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) > slope_variation_threshold:\n",
    "                    continue\n",
    "                dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "                if dist > radius_tolerance:\n",
    "                    continue\n",
    "            \n",
    "            # Si llega aquí, es candidato\n",
    "            if best_distance is None or dist < best_distance:\n",
    "                best_distance = dist\n",
    "                best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            # Asignamos el punto\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            \n",
    "            # Recalculamos slope si es posible (>=2 pts)\n",
    "            if len(best_group['points']) >= 2:\n",
    "                # Tomar subset local\n",
    "                window_min = theta_pt - theta_fit_window/2\n",
    "                window_max = theta_pt + theta_fit_window/2\n",
    "                subset_bg = best_group['points'][\n",
    "                    (best_group['points']['theta'] >= window_min) &\n",
    "                    (best_group['points']['theta'] <= window_max)\n",
    "                ]\n",
    "                if len(subset_bg) < 2:\n",
    "                    subset_bg = best_group['points']\n",
    "                \n",
    "                model.fit(subset_bg[['theta']], subset_bg['r'])\n",
    "                best_group['slope'] = model.coef_[0]\n",
    "                best_group['intercept'] = model.intercept_\n",
    "                best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "            \n",
    "            # Fusión\n",
    "            merged_flag = True\n",
    "            while merged_flag:\n",
    "                merged_flag = False\n",
    "                for other_group in groups:\n",
    "                    if other_group is best_group:\n",
    "                        continue\n",
    "                    # Chequeo superposición\n",
    "                    # Solo fusionar si best_group['slope'] no es None\n",
    "                    # (Opcional)\n",
    "                    if best_group['slope'] is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculamos límites\n",
    "                    bg_th_min = best_group['points']['theta'].min()\n",
    "                    bg_th_max = best_group['points']['theta'].max()\n",
    "                    bg_r_min = best_group['points']['r'].min()\n",
    "                    bg_r_max = best_group['points']['r'].max()\n",
    "\n",
    "                    og_th_min = other_group['points']['theta'].min()\n",
    "                    og_th_max = other_group['points']['theta'].max()\n",
    "                    og_r_min = other_group['points']['r'].min()\n",
    "                    og_r_max = other_group['points']['r'].max()\n",
    "\n",
    "                    # Criterio: si se superponen bastante\n",
    "                    if (abs(bg_th_min - og_th_max) < fusion_theta_threshold and\n",
    "                        abs(bg_th_max - og_th_min) < fusion_theta_threshold and\n",
    "                        abs(bg_r_min - og_r_max) < fusion_r_threshold and\n",
    "                        abs(bg_r_max - og_r_min) < fusion_r_threshold):\n",
    "                        \n",
    "                        merged_points = pd.concat([best_group['points'], other_group['points']], ignore_index=True)\n",
    "                        model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                        best_group['points'] = merged_points\n",
    "                        best_group['slope'] = model.coef_[0]\n",
    "                        best_group['intercept'] = model.intercept_\n",
    "                        best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "                        \n",
    "                        # Eliminar other_group\n",
    "                        for idx, gg in enumerate(groups):\n",
    "                            if gg is other_group:\n",
    "                                groups.pop(idx)\n",
    "                                break\n",
    "                        \n",
    "                        merged_flag = True\n",
    "                        break\n",
    "        \n",
    "        else:\n",
    "            # No se asigna -> temp_groups\n",
    "            add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window)\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Revisar temp_groups cada 'review_interval'\n",
    "        if iteration % review_interval == 0:\n",
    "            review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "        \n",
    "        # Guardar snapshot\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_groups': copy.deepcopy(temp_groups),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # Revisar temp_groups final\n",
    "    if len(temp_groups) > 0:\n",
    "        review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "    \n",
    "    # Subdividir si hay gaps\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    \n",
    "    # Filtrar grupos con pa fuera de flexible_pa_range\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    # Guardar estado final\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'temp_groups': copy.deepcopy(temp_groups),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIÓN DE PLOTEO (OPCIONAL) ===============\n",
    "# =========================================================\n",
    "\n",
    "def plot_intermediate_state_unificado(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica un estado en el plano (theta, r).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos totales')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, label=f\"Grupo {idx+1}\")\n",
    "        if g.get('slope') is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', linewidth=1.5)\n",
    "    \n",
    "    if 'temp_groups' in state and state['temp_groups']:\n",
    "        for idx, tg in enumerate(state['temp_groups']):\n",
    "            pts = tg['points']\n",
    "            plt.scatter(pts['theta'], pts['r'], s=10, color='black', marker='x', label=f\"Temp {idx+1}\")\n",
    "    \n",
    "    plt.xlabel(\"theta (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ EJEMPLO DE USO (OPCIONAL) ==================\n",
    "# =========================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Ejemplo rápido de cómo llamar a la función unificada.\n",
    "    Ajusta parámetros y rutas a tus necesidades.\n",
    "    \"\"\"\n",
    "\n",
    "    states, final_groups, df_filtered = process_spiral_arms_unificado(\n",
    "        id_halo=\"17\",\n",
    "        quartile_threshold=0.155,\n",
    "        theta_min=50,\n",
    "        theta_max=250,\n",
    "        theta_diff=2.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=2.0,\n",
    "        gap_threshold_r=2.0,\n",
    "        min_seed_size=30,\n",
    "        radius_tolerance=0.45,\n",
    "        slope_variation_threshold=0.20,\n",
    "        flexible_pa_range=(4.0, 30.0),\n",
    "        max_theta_gap=15.0,\n",
    "        snapshot_interval=10,\n",
    "        review_interval=30,\n",
    "        theta_fit_window=30.0,\n",
    "        r_fit_window=10.0,\n",
    "        fusion_theta_threshold=7.0,\n",
    "        fusion_r_threshold=7.0\n",
    "    )\n",
    "\n",
    "    print(f\"Total de estados guardados: {len(states)}\")\n",
    "    print(f\"Total de grupos finales: {len(final_groups)}\")\n",
    "    \n",
    "    # from ipywidgets import interact, IntSlider\n",
    "    # def view_state(i=0):\n",
    "    #     plot_intermediate_state_unificado(states[i], df_filtered)\n",
    "    # interact(view_state, i=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da9b67b1-26e1-4e8d-97cb-f4eeff3b0592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state_cartesian(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adbdd02f-9634-4fe8-89b5-c13ba374e06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### nueva aprox. los parametros anteriores dieron buenos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bc1af1-9b71-4612-8e8e-5b3715fc6cc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIONES AUXILIARES BFS (CÓDIGO 2) ========\n",
    "# =========================================================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min=50, theta_max=250):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula coordenadas polares y filtra\n",
    "    según el rango angular [theta_min, theta_max].\n",
    "    Ajusta la columna 'theta' para que esté en [0, 360).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"data_rho_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    return df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo donde cada punto se conecta a otros\n",
    "    si las diferencias en θ y r son <= a los umbrales dados.\n",
    "    Retorna: (graph, n).\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(theta_arr)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Vecinos cumplen (dtheta <= theta_diff) & (dr <= r_diff), excluyendo i mismo\n",
    "        neighbors = np.where((dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i))[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas del grafo usando BFS.\n",
    "    Cada componente es un DataFrame con los puntos de ese subgrupo.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy().reset_index(drop=True)\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide el DataFrame (ordenado por 'theta' o 'r') en subclusters\n",
    "    si hay saltos (gaps) mayores a gap_threshold.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col).reset_index(drop=True)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    \n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    \n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    df_base,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=2.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Toma un DF, filtra por quantile en 'rho_resta_final_exp', \n",
    "    construye grafo y extrae componentes BFS. \n",
    "    Luego subdivide cada componente por gaps en θ y r.\n",
    "    Devuelve una lista de DataFrames (clusters finales) y el DF filtrado.\n",
    "    \"\"\"\n",
    "    threshold = df_base['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df_base[df_base['rho_resta_final_exp'] > threshold].copy().reset_index(drop=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIONES AUXILIARES (CÓDIGO 1) ============\n",
    "# =========================================================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    \"\"\"Distancia vertical de un punto (theta, r) a la recta r = slope*theta + intercept.\"\"\"\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"Calcula el ángulo PA en grados dado slope e intercept.\"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window):\n",
    "    \"\"\"\n",
    "    Agrega un punto a un temp_group si encaja, o crea uno nuevo si no encaja en ninguno.\n",
    "    \"\"\"\n",
    "    assigned = False\n",
    "    for tg in temp_groups:\n",
    "        tg_theta_min = tg['points']['theta'].min()\n",
    "        tg_theta_max = tg['points']['theta'].max()\n",
    "        tg_r_min = tg['points']['r'].min()\n",
    "        tg_r_max = tg['points']['r'].max()\n",
    "        if ((point['theta'] >= tg_theta_min - theta_fit_window/2) and \n",
    "            (point['theta'] <= tg_theta_max + theta_fit_window/2)) and \\\n",
    "           ((point['r'] >= tg_r_min - r_fit_window/2) and \n",
    "            (point['r'] <= tg_r_max + r_fit_window/2)):\n",
    "            tg['points'] = pd.concat([tg['points'], point.to_frame().T], ignore_index=True)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        new_tg = {'points': point.to_frame().T.copy()}\n",
    "        temp_groups.append(new_tg)\n",
    "\n",
    "def review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range):\n",
    "    \"\"\"\n",
    "    Revisa los temp_groups. \n",
    "    - Si tienen <3 puntos: se quedan allí.\n",
    "    - Si >=3, se ajusta recta y se intenta fusionar/promover a 'groups'.\n",
    "    \"\"\"\n",
    "    new_temp_groups = []\n",
    "    for tg in temp_groups:\n",
    "        if len(tg['points']) < 3:\n",
    "            new_temp_groups.append(tg)\n",
    "            continue\n",
    "        \n",
    "        # Ajuste lineal del temp_group\n",
    "        X_tg = tg['points'][['theta']]\n",
    "        y_tg = tg['points']['r']\n",
    "        model.fit(X_tg, y_tg)\n",
    "        tg_slope = model.coef_[0]\n",
    "        tg_intercept = model.intercept_\n",
    "        tg_mean = tg['points']['theta'].mean()\n",
    "        \n",
    "        merged = False\n",
    "        for mg in groups:\n",
    "            if mg['slope'] is None:\n",
    "                # Si mg no tiene slope, se puede fusionar si la distancia es baja\n",
    "                # y luego mg obtendrá slope\n",
    "                dist_values = tg['points'].apply(\n",
    "                    lambda row: calculate_distance(tg_slope, tg_intercept, row['theta'], row['r']),\n",
    "                    axis=1\n",
    "                )\n",
    "                if dist_values.mean() < radius_tolerance:\n",
    "                    mg['points'] = pd.concat([mg['points'], tg['points']], ignore_index=True)\n",
    "                    merged = True\n",
    "                    break\n",
    "            else:\n",
    "                mg_min = mg['points']['theta'].min()\n",
    "                mg_max = mg['points']['theta'].max()\n",
    "                if mg_min <= tg_mean <= mg_max:\n",
    "                    dist_values = tg['points'].apply(\n",
    "                        lambda row: calculate_distance(mg['slope'], mg['intercept'], row['theta'], row['r']),\n",
    "                        axis=1\n",
    "                    )\n",
    "                    if dist_values.mean() < radius_tolerance:\n",
    "                        mg['points'] = pd.concat([mg['points'], tg['points']], ignore_index=True)\n",
    "                        # Reajustamos slope\n",
    "                        model.fit(mg['points'][['theta']], mg['points']['r'])\n",
    "                        new_slope = model.coef_[0]\n",
    "                        old_slope = mg['slope']\n",
    "                        if abs(new_slope - old_slope)/(abs(old_slope)+1e-12) < slope_variation_threshold:\n",
    "                            mg['slope'] = new_slope\n",
    "                            mg['intercept'] = model.intercept_\n",
    "                            mg['pa'] = calculate_pa(mg['slope'], mg['intercept'])\n",
    "                        merged = True\n",
    "                        break\n",
    "        \n",
    "        if not merged:\n",
    "            # Crear nuevo grupo real\n",
    "            new_group = {\n",
    "                'cluster_id': None,\n",
    "                'points': tg['points'],\n",
    "                'slope': tg_slope,\n",
    "                'intercept': tg_intercept,\n",
    "                'pa': calculate_pa(tg_slope, tg_intercept)\n",
    "            }\n",
    "            groups.append(new_group)\n",
    "    \n",
    "    temp_groups.clear()\n",
    "    for tg in new_temp_groups:\n",
    "        temp_groups.append(tg)\n",
    "\n",
    "def subdivide_if_gap(group, max_gap=15.0):\n",
    "    \"\"\"\n",
    "    Si hay un gap grande en θ, divide el grupo en 2.\n",
    "    \"\"\"\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        model = LinearRegression()\n",
    "        model.fit(pts_df[['theta']], pts_df['r'])\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    pts = group['points'].copy().sort_values(by='theta').reset_index(drop=True)\n",
    "    if len(pts) < 4:\n",
    "        return [group]\n",
    "    \n",
    "    theta_vals = pts['theta'].values\n",
    "    dtheta = np.diff(theta_vals)\n",
    "    gap_idx = np.where(dtheta > max_gap)[0]\n",
    "    if len(gap_idx) == 0:\n",
    "        return [group]\n",
    "    \n",
    "    split_idx = gap_idx[0]\n",
    "    pts1 = pts.iloc[:split_idx+1]\n",
    "    pts2 = pts.iloc[split_idx+1:]\n",
    "    \n",
    "    s1, i1, pa1 = fit_line(pts1)\n",
    "    s2, i2, pa2 = fit_line(pts2)\n",
    "    \n",
    "    seg1 = {\n",
    "        'cluster_id': group['cluster_id'],\n",
    "        'points': pts1,\n",
    "        'slope': s1,\n",
    "        'intercept': i1,\n",
    "        'pa': pa1\n",
    "    }\n",
    "    seg2 = {\n",
    "        'cluster_id': group['cluster_id'],\n",
    "        'points': pts2,\n",
    "        'slope': s2,\n",
    "        'intercept': i2,\n",
    "        'pa': pa2\n",
    "    }\n",
    "    return [seg1, seg2]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIÓN PRINCIPAL UNIFICADA ===============\n",
    "# =========================================================\n",
    "\n",
    "def process_spiral_arms_unificado(\n",
    "    id_halo,\n",
    "    # Parámetros BFS / seeds\n",
    "    quartile_threshold=0.55,\n",
    "    theta_min=50, theta_max=250,\n",
    "    theta_diff=2.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5,\n",
    "    min_seed_size=60,\n",
    "\n",
    "    # Parámetros para la iteración\n",
    "    radius_tolerance=0.45,    \n",
    "    slope_variation_threshold=0.20,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    max_theta_gap=15.0,\n",
    "    snapshot_interval=10,\n",
    "    review_interval=50,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    fusion_theta_threshold=7.0,\n",
    "    fusion_r_threshold=7.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Flujo unificado y optimizado:\n",
    "      1) Obtiene “semillas” con BFS. No se ajusta slope en ellas.\n",
    "      2) Itera sobre los puntos restantes:\n",
    "         - Verifica radius_tolerance con cada seed/grupo.\n",
    "         - Si el grupo no tenía slope, lo calculamos en cuanto tenga >=2 puntos.\n",
    "         - Fusiona “temprano” con grupos contiguos en la lista (si cumplen criterios).\n",
    "      3) temp_groups para puntos que no encajan.\n",
    "      4) Al final subdivide y filtra por PA.\n",
    "\n",
    "    Retorna:\n",
    "      states, final_groups, df_filtered\n",
    "    \"\"\"\n",
    "    # 1. Cargar y filtrar datos base\n",
    "    df_base = load_and_filter_data(id_halo, theta_min, theta_max)\n",
    "    \n",
    "    # 2. Semillas BFS + subdivisión\n",
    "    seeds_all, df_filtered = process_clusters_rectangular_gaps(\n",
    "        df_base,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    \n",
    "    # 3. Conservar solo semillas grandes\n",
    "    seed_clusters = [s for s in seeds_all if len(s) >= min_seed_size]\n",
    "    \n",
    "    # 4. Marcar en df_filtered quiénes son semilla\n",
    "    df_filtered['in_seed'] = False\n",
    "    seed_indices = set()\n",
    "    for seed_df in seed_clusters:\n",
    "        seed_indices.update(seed_df.index.tolist())\n",
    "    df_filtered.loc[list(seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # 5. Crear grupos semilla sin slope\n",
    "    groups = []\n",
    "    for idx, seed_pts in enumerate(seed_clusters):\n",
    "        groups.append({\n",
    "            'cluster_id': idx,\n",
    "            'points': seed_pts.copy(),\n",
    "            'slope': None,\n",
    "            'intercept': None,\n",
    "            'pa': None\n",
    "        })\n",
    "    \n",
    "    # 6. Puntos restantes\n",
    "    df_local = df_filtered[~df_filtered['in_seed']].copy()\n",
    "    df_local.sort_values(by='theta', inplace=True)\n",
    "    df_local.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    temp_groups = []\n",
    "    states = []\n",
    "    iteration = 0\n",
    "    \n",
    "    while len(df_local) > 0:\n",
    "        point = df_local.iloc[0]\n",
    "        df_local = df_local.iloc[1:]\n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        \n",
    "        best_group = None\n",
    "        best_dist = None\n",
    "        \n",
    "        # ========== ASIGNACIÓN RÁPIDA POR radius_tolerance ==========\n",
    "        # Recorremos todos los grupos\n",
    "        for g_idx, g in enumerate(groups):\n",
    "            # Calcular distancia media del point a la \"nube\" de g\n",
    "            # Para no recalcular slope, comparamos simplemente la distancia euclídea \n",
    "            # en el plano (theta, r) con el centro, O BIEN la \"vertical\" si ya tiene slope.\n",
    "            \n",
    "            if g['slope'] is None:\n",
    "                # Grupo sin slope => usar distancia (theta,r) al centro\n",
    "                g_mean_theta = g['points']['theta'].mean()\n",
    "                g_mean_r = g['points']['r'].mean()\n",
    "                dist_val = np.sqrt((theta_pt - g_mean_theta)**2 + (r_pt - g_mean_r)**2)\n",
    "                if dist_val < radius_tolerance:  \n",
    "                    # Lo consideramos candidato\n",
    "                    if best_dist is None or dist_val < best_dist:\n",
    "                        best_dist = dist_val\n",
    "                        best_group = g\n",
    "            else:\n",
    "                # Si ya tiene slope, calculamos la distancia vertical\n",
    "                dist_val = calculate_distance(g['slope'], g['intercept'], theta_pt, r_pt)\n",
    "                if dist_val < radius_tolerance:\n",
    "                    if best_dist is None or dist_val < best_dist:\n",
    "                        best_dist = dist_val\n",
    "                        best_group = g\n",
    "        \n",
    "        # ========== SI ENCONTRÓ GRUPO, LO ASIGNAMOS DE INMEDIATO ==========\n",
    "        if best_group is not None:\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            \n",
    "            # Si no tenía slope y ahora puede calcularse\n",
    "            if best_group['slope'] is None and len(best_group['points']) >= 2:\n",
    "                # Ajustar una recta con todos sus puntos\n",
    "                Xb = best_group['points'][['theta']]\n",
    "                yb = best_group['points']['r']\n",
    "                model.fit(Xb, yb)\n",
    "                best_group['slope'] = model.coef_[0]\n",
    "                best_group['intercept'] = model.intercept_\n",
    "                best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "            \n",
    "            # ========== FUSIÓN TEMPRANA CON GRUPOS CONTIGUOS ==============\n",
    "            # Revisamos el \"vecindario\" del best_group en la lista 'groups'.\n",
    "            # O revisamos toda la lista si no son muchos.\n",
    "            merged_flag = True\n",
    "            while merged_flag:\n",
    "                merged_flag = False\n",
    "                for other_group in groups:\n",
    "                    if other_group is best_group:\n",
    "                        continue\n",
    "                    if best_group['slope'] is None and other_group['slope'] is None:\n",
    "                        # Podríamos fusionar 2 grupos seed sin slope \n",
    "                        # si su distancia en (theta,r) es pequeña\n",
    "                        # (a tu gusto)\n",
    "                        pass\n",
    "                    elif best_group['slope'] is not None:\n",
    "                        # Checar superposición en θ y r\n",
    "                        bg_th_min = best_group['points']['theta'].min()\n",
    "                        bg_th_max = best_group['points']['theta'].max()\n",
    "                        bg_r_min = best_group['points']['r'].min()\n",
    "                        bg_r_max = best_group['points']['r'].max()\n",
    "\n",
    "                        og_th_min = other_group['points']['theta'].min()\n",
    "                        og_th_max = other_group['points']['theta'].max()\n",
    "                        og_r_min = other_group['points']['r'].min()\n",
    "                        og_r_max = other_group['points']['r'].max()\n",
    "\n",
    "                        if (abs(bg_th_min - og_th_max) < fusion_theta_threshold and\n",
    "                            abs(bg_th_max - og_th_min) < fusion_theta_threshold and\n",
    "                            abs(bg_r_min - og_r_max) < fusion_r_threshold and\n",
    "                            abs(bg_r_max - og_r_min) < fusion_r_threshold):\n",
    "\n",
    "                            # Fusionar: unimos puntos y recalculamos slope\n",
    "                            merged_pts = pd.concat([best_group['points'], other_group['points']], ignore_index=True)\n",
    "                            model.fit(merged_pts[['theta']], merged_pts['r'])\n",
    "                            best_group['points'] = merged_pts\n",
    "                            best_group['slope'] = model.coef_[0]\n",
    "                            best_group['intercept'] = model.intercept_\n",
    "                            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "                            # Quitamos other_group\n",
    "                            for idxg, grp_ in enumerate(groups):\n",
    "                                if grp_ is other_group:\n",
    "                                    groups.pop(idxg)\n",
    "                                    break\n",
    "                            merged_flag = True\n",
    "                            break\n",
    "        else:\n",
    "            # No encontró grupo => temp_groups\n",
    "            add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window)\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # ========== REVIEW TEMP GROUPS CADA review_interval ==========\n",
    "        if iteration % review_interval == 0:\n",
    "            review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "        \n",
    "        # ========== SNAPSHOT ==========\n",
    "        if iteration % snapshot_interval == 0 or len(df_local) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_groups': copy.deepcopy(temp_groups),\n",
    "                'remaining': df_local.copy()\n",
    "            })\n",
    "    \n",
    "    # ========== FINAL REVIEW DE temp_groups ==========\n",
    "    if len(temp_groups) > 0:\n",
    "        review_temp_groups(temp_groups, groups, model, radius_tolerance, slope_variation_threshold, flexible_pa_range)\n",
    "    \n",
    "    # ========== SUBDIVIDIR POR GAPS ==========\n",
    "    final_groups = []\n",
    "    for g in groups:\n",
    "        segs = subdivide_if_gap(g, max_gap=max_theta_gap)\n",
    "        final_groups.extend(segs)\n",
    "    \n",
    "    # ========== FILTRAR POR RANGO PA ==========\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    # ========== ULTIMO ESTADO ==========\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'temp_groups': copy.deepcopy(temp_groups),\n",
    "        'remaining': df_local.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df_filtered\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ FUNCIÓN DE PLOTEO (OPCIONAL) ===============\n",
    "# =========================================================\n",
    "\n",
    "def plot_intermediate_state_unificado(state, df_all):\n",
    "    \"\"\"\n",
    "    Grafica un estado en el plano (theta, r).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, color='lightgrey', alpha=0.5, label='Datos totales')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, label=f\"Grupo {idx+1}\")\n",
    "        if g.get('slope') is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', linewidth=1.5)\n",
    "    \n",
    "    if 'temp_groups' in state and state['temp_groups']:\n",
    "        for idx, tg in enumerate(state['temp_groups']):\n",
    "            pts = tg['points']\n",
    "            plt.scatter(pts['theta'], pts['r'], s=10, color='black', marker='x', label=f\"Temp {idx+1}\")\n",
    "    \n",
    "    plt.xlabel(\"theta (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ============ EJEMPLO DE USO (OPCIONAL) ==================\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Ejemplo rápido de cómo llamar a la función con la lógica optimizada.\n",
    "    Ajusta parámetros y rutas según tus necesidades.\n",
    "    \"\"\"\n",
    "    states, final_groups, df_filtered = process_spiral_arms_unificado(\n",
    "        id_halo=\"11\",\n",
    "        quartile_threshold=0.155,\n",
    "        theta_min=30,\n",
    "        theta_max=280,\n",
    "        theta_diff=2.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=1.80,\n",
    "        gap_threshold_r=1.80,\n",
    "        min_seed_size=50,\n",
    "        radius_tolerance=0.45,\n",
    "        slope_variation_threshold=0.25,\n",
    "        flexible_pa_range=(4.0,30.0),\n",
    "        max_theta_gap=15.0,\n",
    "        snapshot_interval=10,\n",
    "        review_interval=30,\n",
    "        theta_fit_window=28.0,\n",
    "        r_fit_window=9.0,\n",
    "        fusion_theta_threshold=7.0,\n",
    "        fusion_r_threshold=7.0\n",
    "    )\n",
    "\n",
    "    print(f\"Total de estados guardados: {len(states)}\")\n",
    "    print(f\"Total de grupos finales: {len(final_groups)}\")\n",
    "    \n",
    "    # Uso interactivo en Jupyter:\n",
    "    # from ipywidgets import interact, IntSlider\n",
    "    # def view_state(i=0):\n",
    "    #     plot_intermediate_state_unificado(states[i], df_filtered)\n",
    "    # interact(view_state, i=IntSlider(min=0, max=len(states)-1, step=1, value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b09835a8-16e5-4221-8335-09bbb9feddfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state_cartesian(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10cc8b45-4e21-4c72-a17e-aea8361c8f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ultimo ultimo intento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f1c3ca-1fc8-42de-b1e5-8a0d3d331dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ===================================================\n",
    "# 1. Funciones auxiliares para generar semillas (BFS)\n",
    "# ===================================================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Carga datos desde CSV, calcula coordenadas polares (r, theta) \n",
    "    y filtra según rango [theta_min, theta_max].\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    # Filtrar en [theta_min, theta_max]\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    # Asegurarse de que las columnas sean float\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un \"grafo\" (lista de adyacencia) donde cada punto se conecta\n",
    "    a los vecinos cuya distancia en θ es <= theta_diff y en r <= r_diff.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    \n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        # Vecinos: índice j donde dtheta[j] <= theta_diff, dr[j] <= r_diff y j != i\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors = np.where(mask_neighbors)[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Hace un recorrido BFS sobre el grafo para extraer componentes conexas.\n",
    "    Cada componente es un subconjunto de filas (por posición entera).\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False] * n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            # comp_indices son las posiciones de las filas en df_points\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide un DataFrame (ordenado por 'theta' o por 'r') si encuentra gaps\n",
    "    mayores que el umbral en la diferencia sucesiva.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)  # Mantenemos índices originales\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    \n",
    "    # Si no hay saltos (gaps) más grandes que el umbral, se retorna tal cual.\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    \n",
    "    # Caso contrario, se parte en subclusters.\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    # Añadimos el último fragmento\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "def process_clusters_rectangular_gaps(\n",
    "    df,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=5.0,\n",
    "    r_diff=1.0,\n",
    "    gap_threshold_theta=2.5,\n",
    "    gap_threshold_r=2.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Toma el DataFrame ya filtrado (en [theta_min, theta_max]), \n",
    "    aplica un corte por percentil en 'rho_resta_final_exp', \n",
    "    arma el grafo rectangular y lo subdivide por gaps en θ y r.\n",
    "    Retorna lista de clusters \"en bruto\".\n",
    "    \"\"\"\n",
    "    # 1) Filtrar por percentil, SIN reset_index(drop=True)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    # Importante: Creamos una columna \"orig_index\" para no perder la referencia\n",
    "    df_filtered['orig_index'] = df_filtered.index  \n",
    "    # Ahora sí, reindexamos para BFS\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 2) Construir grafo (trabaja con posiciones 0..N-1)\n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    # 3) Extraer componentes con BFS\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    # 4) Subdividir cada cluster por gaps (theta, r)\n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters\n",
    "\n",
    "# ===================================================\n",
    "# 2. Funciones de ajuste y validación (de “código 1”)\n",
    "# ===================================================\n",
    "\n",
    "def calculate_distance(slope, intercept, theta, r):\n",
    "    pred_r = slope * theta + intercept\n",
    "    return abs(pred_r - r)\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window):\n",
    "    assigned = False\n",
    "    for tg in temp_groups:\n",
    "        tg_theta_min = tg['points']['theta'].min()\n",
    "        tg_theta_max = tg['points']['theta'].max()\n",
    "        tg_r_min = tg['points']['r'].min()\n",
    "        tg_r_max = tg['points']['r'].max()\n",
    "        if ((point['theta'] >= tg_theta_min - theta_fit_window/2) and \n",
    "            (point['theta'] <= tg_theta_max + theta_fit_window/2)) and \\\n",
    "           ((point['r'] >= tg_r_min - r_fit_window/2) and \n",
    "            (point['r'] <= tg_r_max + r_fit_window/2)):\n",
    "            tg['points'] = pd.concat([tg['points'], point.to_frame().T], ignore_index=True)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        new_tg = {'points': point.to_frame().T.copy()}\n",
    "        temp_groups.append(new_tg)\n",
    "\n",
    "def review_temp_groups(temp_groups, groups, model,\n",
    "                       radius_tolerance, slope_variation_threshold, flexible_pa_range):\n",
    "    new_temp_groups = []\n",
    "    for tg in temp_groups:\n",
    "        if len(tg['points']) < 3:\n",
    "            new_temp_groups.append(tg)\n",
    "            continue\n",
    "        X_tg = tg['points'][['theta']]\n",
    "        y_tg = tg['points']['r']\n",
    "        model.fit(X_tg, y_tg)\n",
    "        tg_slope = model.coef_[0]\n",
    "        tg_intercept = model.intercept_\n",
    "        tg_mean = tg['points']['theta'].mean()\n",
    "        \n",
    "        merged = False\n",
    "        for mg in groups:\n",
    "            mg_min = mg['points']['theta'].min()\n",
    "            mg_max = mg['points']['theta'].max()\n",
    "            if mg_min <= tg_mean <= mg_max:\n",
    "                dist_values = tg['points'].apply(\n",
    "                    lambda row: calculate_distance(\n",
    "                        mg['slope'], mg['intercept'],\n",
    "                        row['theta'], row['r']\n",
    "                    ),\n",
    "                    axis=1\n",
    "                )\n",
    "                if dist_values.mean() < radius_tolerance:\n",
    "                    mg['points'] = pd.concat([mg['points'], tg['points']], ignore_index=True)\n",
    "                    model.fit(mg['points'][['theta']], mg['points']['r'])\n",
    "                    new_slope = model.coef_[0]\n",
    "                    if abs(new_slope - mg['slope'])/(abs(mg['slope'])+1e-12) < slope_variation_threshold:\n",
    "                        mg['slope'] = new_slope\n",
    "                        mg['intercept'] = model.intercept_\n",
    "                        mg['pa'] = calculate_pa(mg['slope'], mg['intercept'])\n",
    "                    merged = True\n",
    "                    break\n",
    "        if not merged:\n",
    "            new_group = {\n",
    "                'cluster_id': None,\n",
    "                'points': tg['points'],\n",
    "                'slope': tg_slope,\n",
    "                'intercept': tg_intercept,\n",
    "                'pa': calculate_pa(tg_slope, tg_intercept)\n",
    "            }\n",
    "            groups.append(new_group)\n",
    "    temp_groups.clear()\n",
    "    for tg in new_temp_groups:\n",
    "        temp_groups.append(tg)\n",
    "\n",
    "# ===================================================\n",
    "# 3. Función final que INTEGRA BFS + Ajuste Iterativo\n",
    "# ===================================================\n",
    "\n",
    "def process_spiral_arms_integrated(\n",
    "    id_halo,\n",
    "    # Parámetros BFS (para semillas)\n",
    "    theta_min=50, \n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    # Parámetros de la iteración (tipo código 1)\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    flexible_pa_range=(4.0, 30.0), \n",
    "    max_theta_gap=15.0,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    fusion_theta_threshold=7.0,\n",
    "    fusion_r_threshold=7.0,\n",
    "    snapshot_interval=10,\n",
    "    do_subdivide=True,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Construye \"semillas\" con BFS sin pendiente.\n",
    "    2) Excluye esos puntos semilla de la iteración posterior.\n",
    "    3) Itera sobre el resto con la mecánica de \"código 1\":\n",
    "       - temp_groups, groups\n",
    "       - fusión y ajustes de pendiente\n",
    "       - subdivisión final si hay gap\n",
    "    4) Retorna los estados y los grupos finales, junto con df completo ya filtrado.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------\n",
    "    # A) Cargar df y filtrar en [theta_min, theta_max]\n",
    "    # -------------------------------------------\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix=file_prefix)\n",
    "    # Vamos a mantener este df con su índice original,\n",
    "    # para poder marcar in_seed=True sin problemas.\n",
    "    # df.index: array de valores (pueden ser 0.., o no consecutivos si df venía de algo mayor)\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    # B) Hallar clusters BFS \"en bruto\" + subdivisión\n",
    "    # -------------------------------------------\n",
    "    # Ojo: process_clusters_rectangular_gaps() \n",
    "    # internamente crea df_filtered con 'orig_index' y reindexa para BFS.\n",
    "    clusters_bfs = process_clusters_rectangular_gaps(\n",
    "        df,\n",
    "        quartile_threshold=quartile_threshold,\n",
    "        theta_diff=theta_diff,\n",
    "        r_diff=r_diff,\n",
    "        gap_threshold_theta=gap_threshold_theta,\n",
    "        gap_threshold_r=gap_threshold_r\n",
    "    )\n",
    "    \n",
    "    # clusters_bfs es una lista de DataFrames. \n",
    "    # Cada DataFrame conserva la columna 'orig_index' para volver a df original.\n",
    "    \n",
    "    # Semillas = clusters con al menos 2 puntos\n",
    "    seed_clusters = [c for c in clusters_bfs if len(c) >= 2]\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    # C) Marcar en df quién está en semilla, usando 'orig_index'\n",
    "    # -------------------------------------------\n",
    "    df['in_seed'] = False\n",
    "    all_seed_indices = set()\n",
    "    \n",
    "    for seed_df in seed_clusters:\n",
    "        # La columna 'orig_index' contiene los índices del df original\n",
    "        # que formaron parte de ese cluster\n",
    "        all_seed_indices.update(seed_df['orig_index'].to_list())\n",
    "    \n",
    "    # Ahora sí podemos marcar\n",
    "    df.loc[list(all_seed_indices), 'in_seed'] = True\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    # D) Inicializar \"groups\" a partir de las semillas\n",
    "    #    con slope/intercept si >=2 puntos.\n",
    "    # -------------------------------------------\n",
    "    model = LinearRegression()\n",
    "    groups = []\n",
    "    for idx, seed_df in enumerate(seed_clusters):\n",
    "        seed_df = seed_df.copy()\n",
    "        slope_init, intercept_init, pa_init = None, None, None\n",
    "        if len(seed_df) >= 2:\n",
    "            X_ = seed_df[['theta']].values\n",
    "            y_ = seed_df['r'].values\n",
    "            model.fit(X_, y_)\n",
    "            slope_init = model.coef_[0]\n",
    "            intercept_init = model.intercept_\n",
    "            pa_init = calculate_pa(slope_init, intercept_init)\n",
    "        groups.append({\n",
    "            'cluster_id': idx,\n",
    "            'points': seed_df,  # semilla\n",
    "            'slope': slope_init,\n",
    "            'intercept': intercept_init,\n",
    "            'pa': pa_init\n",
    "        })\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    # E) Crear DataFrame de puntos \"no semilla\" para iterar\n",
    "    # -------------------------------------------\n",
    "    df_non_seed = df[~df['in_seed']].copy()\n",
    "    df_non_seed['theta'] = df_non_seed['theta'].astype(float)\n",
    "    df_non_seed['r'] = df_non_seed['r'].astype(float)\n",
    "    # Ordenar\n",
    "    df_non_seed.sort_values(by='theta', inplace=True)\n",
    "    df_non_seed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # F) Iterar con la mecánica \"código 1\"\n",
    "    # -------------------------------------------\n",
    "    states = []\n",
    "    temp_groups = []\n",
    "    iteration = 0\n",
    "    temp_review_interval = snapshot_interval * 15  # Como en el original\n",
    "    \n",
    "    while len(df_non_seed) > 0:\n",
    "        point = df_non_seed.iloc[0]\n",
    "        df_non_seed = df_non_seed.iloc[1:]\n",
    "        \n",
    "        theta_pt = point['theta']\n",
    "        r_pt = point['r']\n",
    "        best_group = None\n",
    "        best_distance = None\n",
    "        \n",
    "        # Intentar asignar a un grupo existente\n",
    "        for g in groups:\n",
    "            if g['slope'] is None:\n",
    "                # Si no tiene slope, es un grupo con <2 puntos semilla\n",
    "                if len(g['points']) < 1:\n",
    "                    continue\n",
    "                # Con 1 punto, podríamos asignar por bounding box. Ej:\n",
    "                g_theta_min = g['points']['theta'].min()\n",
    "                g_theta_max = g['points']['theta'].max()\n",
    "                g_r_min = g['points']['r'].min()\n",
    "                g_r_max = g['points']['r'].max()\n",
    "                if (theta_pt >= g_theta_min - theta_fit_window/2 and \n",
    "                    theta_pt <= g_theta_max + theta_fit_window/2 and\n",
    "                    r_pt >= g_r_min - r_fit_window/2 and\n",
    "                    r_pt <= g_r_max + r_fit_window/2):\n",
    "                    best_group = g\n",
    "                    best_distance = 0.0\n",
    "                continue\n",
    "            \n",
    "            # Caso normal\n",
    "            g_theta_min = g['points']['theta'].min()\n",
    "            g_theta_max = g['points']['theta'].max()\n",
    "            g_r_min = g['points']['r'].min()\n",
    "            g_r_max = g['points']['r'].max()\n",
    "            if not (g_theta_min <= theta_pt <= g_theta_max and \n",
    "                    g_r_min <= r_pt <= g_r_max):\n",
    "                continue\n",
    "            \n",
    "            # Ajuste local\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset = g['points'][\n",
    "                (g['points']['theta'] >= window_min) & \n",
    "                (g['points']['theta'] <= window_max)\n",
    "            ]\n",
    "            if len(subset) == 0:\n",
    "                continue\n",
    "            \n",
    "            r_min = subset['r'].min()\n",
    "            r_max = subset['r'].max()\n",
    "            if not (r_min <= r_pt <= r_max):\n",
    "                continue\n",
    "            \n",
    "            subset_test = pd.concat([subset, point.to_frame().T], ignore_index=True)\n",
    "            if len(subset_test) < 2:\n",
    "                continue\n",
    "            \n",
    "            model.fit(subset_test[['theta']], subset_test['r'])\n",
    "            new_slope = model.coef_[0]\n",
    "            new_intercept = model.intercept_\n",
    "            old_slope = g['slope']\n",
    "            \n",
    "            slope_var = abs(new_slope - old_slope)/(abs(old_slope)+1e-12)\n",
    "            if slope_var > slope_variation_threshold:\n",
    "                continue\n",
    "            \n",
    "            dist = calculate_distance(new_slope, new_intercept, theta_pt, r_pt)\n",
    "            if dist > radius_tolerance:\n",
    "                continue\n",
    "            \n",
    "            # Si llegamos aquí, es buen candidato\n",
    "            if best_distance is None or dist < best_distance:\n",
    "                best_distance = dist\n",
    "                best_group = g\n",
    "        \n",
    "        if best_group is not None:\n",
    "            best_group['points'] = pd.concat([best_group['points'], point.to_frame().T], ignore_index=True)\n",
    "            window_min = theta_pt - theta_fit_window/2\n",
    "            window_max = theta_pt + theta_fit_window/2\n",
    "            subset_local = best_group['points'][\n",
    "                (best_group['points']['theta'] >= window_min) &\n",
    "                (best_group['points']['theta'] <= window_max)\n",
    "            ]\n",
    "            if len(subset_local) < 2:\n",
    "                subset_local = best_group['points']\n",
    "            \n",
    "            model.fit(subset_local[['theta']], subset_local['r'])\n",
    "            best_group['slope'] = model.coef_[0]\n",
    "            best_group['intercept'] = model.intercept_\n",
    "            best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "            \n",
    "            # Fusión con grupos vecinos\n",
    "            merged_flag = True\n",
    "            while merged_flag:\n",
    "                merged_flag = False\n",
    "                for other_group in groups:\n",
    "                    if other_group is best_group:\n",
    "                        continue\n",
    "                    if len(other_group['points']) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    best_theta_min = best_group['points']['theta'].min()\n",
    "                    best_theta_max = best_group['points']['theta'].max()\n",
    "                    best_r_min = best_group['points']['r'].min()\n",
    "                    best_r_max = best_group['points']['r'].max()\n",
    "                    \n",
    "                    og_theta_min = other_group['points']['theta'].min()\n",
    "                    og_theta_max = other_group['points']['theta'].max()\n",
    "                    og_r_min = other_group['points']['r'].min()\n",
    "                    og_r_max = other_group['points']['r'].max()\n",
    "                    \n",
    "                    if (abs(best_theta_min - og_theta_max) < fusion_theta_threshold and\n",
    "                        abs(best_theta_max - og_theta_min) < fusion_theta_threshold and\n",
    "                        abs(best_r_min - og_r_max) < fusion_r_threshold and\n",
    "                        abs(best_r_max - og_r_min) < fusion_r_threshold):\n",
    "                        merged_points = pd.concat([best_group['points'], other_group['points']], ignore_index=True)\n",
    "                        model.fit(merged_points[['theta']], merged_points['r'])\n",
    "                        best_group['points'] = merged_points\n",
    "                        best_group['slope'] = model.coef_[0]\n",
    "                        best_group['intercept'] = model.intercept_\n",
    "                        best_group['pa'] = calculate_pa(best_group['slope'], best_group['intercept'])\n",
    "                        \n",
    "                        idx_to_remove = None\n",
    "                        for ix, gg in enumerate(groups):\n",
    "                            if gg is other_group:\n",
    "                                idx_to_remove = ix\n",
    "                                break\n",
    "                        if idx_to_remove is not None:\n",
    "                            groups.pop(idx_to_remove)\n",
    "                        merged_flag = True\n",
    "                        break\n",
    "        else:\n",
    "            # No se pudo asignar -> temp_group\n",
    "            add_to_temp_groups(point, temp_groups, theta_fit_window, r_fit_window)\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration % temp_review_interval == 0:\n",
    "            review_temp_groups(temp_groups, groups, model,\n",
    "                               radius_tolerance, slope_variation_threshold,\n",
    "                               flexible_pa_range)\n",
    "        \n",
    "        if iteration % snapshot_interval == 0 or len(df_non_seed) == 0:\n",
    "            states.append({\n",
    "                'iteration': iteration,\n",
    "                'groups': copy.deepcopy(groups),\n",
    "                'temp_groups': copy.deepcopy(temp_groups),\n",
    "                'remaining': df_non_seed.copy()\n",
    "            })\n",
    "    \n",
    "    # Al terminar:\n",
    "    if len(temp_groups) > 0:\n",
    "        review_temp_groups(temp_groups, groups, model,\n",
    "                           radius_tolerance, slope_variation_threshold,\n",
    "                           flexible_pa_range)\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    # G) Subdividir grupos si hay gap grande en θ (opcional)\n",
    "    # -------------------------------------------\n",
    "    def fit_line(pts_df):\n",
    "        if len(pts_df) < 2:\n",
    "            return None, None, None\n",
    "        X_ = pts_df[['theta']].values\n",
    "        y_ = pts_df['r'].values\n",
    "        model.fit(X_, y_)\n",
    "        s_ = model.coef_[0]\n",
    "        i_ = model.intercept_\n",
    "        pa_ = calculate_pa(s_, i_)\n",
    "        return s_, i_, pa_\n",
    "    \n",
    "    def subdivide_if_gap(group, max_gap=15.0):\n",
    "        pts = group['points'].copy().sort_values(by='theta')\n",
    "        if len(pts) < 4:\n",
    "            return [group]\n",
    "        theta_vals = pts['theta'].values\n",
    "        dtheta = np.diff(theta_vals)\n",
    "        gap_idx = np.where(dtheta > max_gap)[0]\n",
    "        if len(gap_idx) == 0:\n",
    "            return [group]\n",
    "        split_idx = gap_idx[0]\n",
    "        pts1 = pts.iloc[:split_idx+1]\n",
    "        pts2 = pts.iloc[split_idx+1:]\n",
    "        s1, i1, pa1 = fit_line(pts1)\n",
    "        s2, i2, pa2 = fit_line(pts2)\n",
    "        seg1 = {\n",
    "            'cluster_id': group.get('cluster_id', None),\n",
    "            'points': pts1,\n",
    "            'slope': s1,\n",
    "            'intercept': i1,\n",
    "            'pa': pa1\n",
    "        }\n",
    "        seg2 = {\n",
    "            'cluster_id': group.get('cluster_id', None),\n",
    "            'points': pts2,\n",
    "            'slope': s2,\n",
    "            'intercept': i2,\n",
    "            'pa': pa2\n",
    "        }\n",
    "        return [seg1, seg2]\n",
    "    \n",
    "    final_groups = []\n",
    "    if do_subdivide:\n",
    "        for g in groups:\n",
    "            segs = subdivide_if_gap(g, max_theta_gap)\n",
    "            final_groups.extend(segs)\n",
    "    else:\n",
    "        final_groups = groups\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    # H) Filtro final por rango de PA\n",
    "    # -------------------------------------------\n",
    "    valid_groups = []\n",
    "    for grp in final_groups:\n",
    "        if grp['slope'] is not None and grp['intercept'] is not None:\n",
    "            pa_val = grp['pa']\n",
    "            if pa_val is not None and flexible_pa_range[0] <= abs(pa_val) <= flexible_pa_range[1]:\n",
    "                valid_groups.append(grp)\n",
    "    \n",
    "    # Último snapshot\n",
    "    states.append({\n",
    "        'iteration': iteration,\n",
    "        'groups': copy.deepcopy(valid_groups),\n",
    "        'temp_groups': copy.deepcopy(temp_groups),\n",
    "        'remaining': df_non_seed.copy()\n",
    "    })\n",
    "    \n",
    "    return states, valid_groups, df  # df es el dataframe con in_seed=True/False\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "# 4. Función de visualización (opcional)\n",
    "# ===================================================\n",
    "\n",
    "def plot_intermediate_state_cartesian(state, df_all):\n",
    "    \"\"\"\n",
    "    Similar a plot_intermediate_state de \"código 1\",\n",
    "    pero adaptado al pipeline integrado.\n",
    "    Muestra en (theta, r).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=1, alpha=0.5, label='Datos completos')\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan']\n",
    "    for idx, g in enumerate(state['groups']):\n",
    "        pts = g['points']\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, label=f\"Grupo {idx+1}\")\n",
    "        \n",
    "        # Plot de la recta ajustada\n",
    "        if g.get('slope') is not None:\n",
    "            theta_line = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_line = g['slope'] * theta_line + g['intercept']\n",
    "            plt.plot(theta_line, r_line, '--', linewidth=1.5)\n",
    "    \n",
    "    if state.get('temp_groups'):\n",
    "        for idx_t, tg in enumerate(state['temp_groups']):\n",
    "            pts = tg['points']\n",
    "            plt.scatter(pts['theta'], pts['r'], s=10, marker='x', color='black', label=f\"Temp {idx_t+1}\")\n",
    "    \n",
    "    plt.xlabel(r\"$\\theta$ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Estado en iteración {state['iteration']} (Plano r, θ)\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b41f087-4364-4cd6-91e8-184de157f768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "states, final_groups, df_all = process_spiral_arms_integrated(\n",
    "    id_halo=\"17\",     # Cambia si es otro halo\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    max_theta_gap=15.0,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=10.0,\n",
    "    fusion_theta_threshold=7.0,\n",
    "    fusion_r_threshold=7.0,\n",
    "    snapshot_interval=10,\n",
    "    do_subdivide=True,\n",
    "    file_prefix='data_rho'\n",
    ")\n",
    "\n",
    "print(\"Número de estados:\", len(states))\n",
    "print(\"Número de grupos finales:\", len(final_groups))\n",
    "\n",
    "# Graficamos el último estado\n",
    "plot_intermediate_state_cartesian(states[-1], df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed4c5ce-8996-4c9f-a09b-45b9cb8d6a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state_cartesian(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c1fa0a-8137-46ae-9307-d2e29fa1717b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "states, final_groups, df_all = process_spiral_arms_integrated(\n",
    "    id_halo=\"11\",     # Cambia si es otro halo\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.45,\n",
    "    gap_threshold_theta=1.80,\n",
    "    gap_threshold_r=2.0,\n",
    "    radius_tolerance=0.45,\n",
    "    slope_variation_threshold=0.20,\n",
    "    flexible_pa_range=(4.0, 30.0),\n",
    "    max_theta_gap=9.0,\n",
    "    theta_fit_window=30.0,\n",
    "    r_fit_window=6.0,\n",
    "    fusion_theta_threshold=6.0,\n",
    "    fusion_r_threshold=6.0,\n",
    "    snapshot_interval=10,\n",
    "    do_subdivide=True,\n",
    "    file_prefix='data_rho'\n",
    ")\n",
    "\n",
    "print(\"Número de estados:\", len(states))\n",
    "print(\"Número de grupos finales:\", len(final_groups))\n",
    "\n",
    "# Graficamos el último estado\n",
    "plot_intermediate_state_cartesian(states[-1], df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac53336e-2ebd-4cec-8d6e-e249a0bddb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def view_state(state_index):\n",
    "    state = states[state_index]\n",
    "    plot_intermediate_state_cartesian(state, df_filtered)\n",
    "\n",
    "interact(view_state, state_index=IntSlider(min=0, max=len(states)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0341f1-1eba-4afa-9f67-6ce5ba015dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add95f86-4acd-43b2-a76e-1c4d043501e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# =========================================\n",
    "# Funciones Auxiliares: Carga, BFS, Subdivisión\n",
    "# =========================================\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Carga los datos desde CSV, calcula (r, theta) y filtra \n",
    "    en el rango de theta [theta_min, theta_max].\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    # Asegurarnos de que sean float\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un \"grafo\" (lista de adyacencia) donde cada punto se conecta\n",
    "    con sus vecinos si la diferencia en θ y en r es <= a los umbrales dados.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors = np.where(mask_neighbors)[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Recorre el 'graph' con BFS y obtiene componentes conexas. \n",
    "    Cada componente devuelta es un DataFrame con esos puntos.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide un DataFrame si hay 'gaps' mayores que 'gap_threshold' \n",
    "    en la secuencia de theta (o r) ordenada.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "# =========================================\n",
    "# Función Principal: Genera SOLO las Semillas (BFS) y las Visualiza\n",
    "# =========================================\n",
    "\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Carga y filtra datos en [theta_min, theta_max].\n",
    "    2) Filtra además por 'rho_resta_final_exp' > percentil (quartile_threshold).\n",
    "    3) Construye un grafo BFS con vecindad rectangular (theta_diff, r_diff).\n",
    "    4) Extrae componentes conexas.\n",
    "    5) Subdivide cada cluster por gap en theta y r.\n",
    "    6) Retorna la lista final de 'semillas' (clusters BFS) y el DataFrame filtrado.\n",
    "\n",
    "    NOTA: No hay iteración ni asignación adicional de puntos.\n",
    "    \"\"\"\n",
    "    # A) Cargar y filtrar\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix=file_prefix)\n",
    "    \n",
    "    # B) Filtrar por percentil\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    # Para BFS necesitaremos indices consecutivos:\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # C) Grafo BFS\n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    # D) Subdividir por gaps en θ y r\n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        # subdividir en theta\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            # subdividir en r\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "def plot_bfs_seeds_polar(bfs_clusters, df_all):\n",
    "    \"\"\"\n",
    "    Muestra en el plano (theta, r) los puntos totales 'df_all' en gris,\n",
    "    y colorea cada cluster BFS diferente.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Fondo: todos los puntos del df_all\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=5, alpha=0.2, label='Datos filtrados')\n",
    "    \n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for idx, cluster_df in enumerate(bfs_clusters):\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.scatter(cluster_df['theta'], cluster_df['r'], s=10, label=f\"Semilla {idx+1}\", color=color)\n",
    "    \n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Semillas (BFS) antes de iterar\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "    id_halo=\"17\",\n",
    "    theta_min=0,\n",
    "    theta_max=390,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    ")\n",
    "print(f\"Encontré {len(bfs_clusters)} grupos BFS (posibles semillas).\")\n",
    "plot_bfs_seeds_polar(bfs_clusters, df_filtrado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad8774f-2060-46bc-b7af-ebe0e6fd9dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def calculate_pa(slope):\n",
    "    \"\"\"Calcula el 'position angle' (en grados) a partir de la pendiente.\"\"\"\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    \"\"\"\n",
    "    Ajusta una recta a df_cluster (si >=2 puntos).\n",
    "    Retorna slope, intercept, pa, bounding box (theta_min, theta_max, r_min, r_max).\n",
    "    Si <2 puntos, retorna None en slope/intercept.\n",
    "    \"\"\"\n",
    "    if len(df_cluster) < 2:\n",
    "        return None, None, None, None, None, None, None, None\n",
    "    \n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    slope_ = model.coef_[0]\n",
    "    intercept_ = model.intercept_\n",
    "    pa_ = calculate_pa(slope_)\n",
    "    \n",
    "    theta_min_ = df_cluster['theta'].min()\n",
    "    theta_max_ = df_cluster['theta'].max()\n",
    "    r_min_ = df_cluster['r'].min()\n",
    "    r_max_ = df_cluster['r'].max()\n",
    "    \n",
    "    return slope_, intercept_, pa_, theta_min_, theta_max_, r_min_, r_max_\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.40,  # 40% variación de pendiente\n",
    "    bounding_extrap=0.30            # 30% de extrapolación de bounding box\n",
    "):\n",
    "    \"\"\"\n",
    "    Toma los clusters BFS (semillas), ajusta una recta a cada uno \n",
    "    (si >=2 puntos) y luego realiza fusiones recíprocas mientras:\n",
    "      - Haya superposición en bounding boxes (extrapoladas 30%)\n",
    "      - Fusionar no dispare la variación de pendiente más allá del 40%\n",
    "\n",
    "    Retorna una lista final de grupos (dict) con:\n",
    "      - 'points': DataFrame del grupo\n",
    "      - 'slope'\n",
    "      - 'intercept'\n",
    "      - 'pa'\n",
    "      - bounding box final (theta_min, theta_max, r_min, r_max)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Crear lista de \"grupos\" con slope/intercept y bounding box\n",
    "    groups = []\n",
    "    for clust_df in bfs_clusters:\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(clust_df)\n",
    "        grp = {\n",
    "            'points': clust_df.copy(),\n",
    "            'slope': slope_,\n",
    "            'intercept': intercept_,\n",
    "            'pa': pa_,\n",
    "            'theta_min': tmin,\n",
    "            'theta_max': tmax,\n",
    "            'r_min': rmin,\n",
    "            'r_max': rmax\n",
    "        }\n",
    "        groups.append(grp)\n",
    "    \n",
    "    # 2) Función para recalcular slope/intercept y bounding box luego de fusionar\n",
    "    def recalc_properties(group):\n",
    "        dfp = group['points']\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(dfp)\n",
    "        group['slope'] = slope_\n",
    "        group['intercept'] = intercept_\n",
    "        group['pa'] = pa_\n",
    "        group['theta_min'] = tmin\n",
    "        group['theta_max'] = tmax\n",
    "        group['r_min'] = rmin\n",
    "        group['r_max'] = rmax\n",
    "\n",
    "    # 3) Función para verificar superposición de bounding box con extrapolación\n",
    "    def boxes_overlap(g1, g2, extrap=0.30):\n",
    "        \"\"\"\n",
    "        Devuelve True si los bounding boxes (theta_min, theta_max, r_min, r_max)\n",
    "        se superponen cuando se les amplía un 30% (por ejemplo).\n",
    "        \"\"\"\n",
    "        # Extraer bounding box de g1\n",
    "        tmin1, tmax1 = g1['theta_min'], g1['theta_max']\n",
    "        rmin1, rmax1 = g1['r_min'], g1['r_max']\n",
    "        # Extraer bounding box de g2\n",
    "        tmin2, tmax2 = g2['theta_min'], g2['theta_max']\n",
    "        rmin2, rmax2 = g2['r_min'], g2['r_max']\n",
    "        \n",
    "        if (tmin1 is None or tmin2 is None):\n",
    "            return False\n",
    "        \n",
    "        # Expandir c/ bounding en 30%\n",
    "        # 1) Calculamos los anchos\n",
    "        dt1 = (tmax1 - tmin1)\n",
    "        dr1 = (rmax1 - rmin1)\n",
    "        dt2 = (tmax2 - tmin2)\n",
    "        dr2 = (rmax2 - rmin2)\n",
    "        \n",
    "        # 2) Sumamos un 0.30 * dt, etc., arriba y abajo\n",
    "        #    Por ejemplo:\n",
    "        extr_dt1 = dt1 * extrap\n",
    "        extr_dr1 = dr1 * extrap\n",
    "        extr_dt2 = dt2 * extrap\n",
    "        extr_dr2 = dr2 * extrap\n",
    "        \n",
    "        # \"caja\" g1 extrapolada\n",
    "        tmin1e = tmin1 - extr_dt1\n",
    "        tmax1e = tmax1 + extr_dt1\n",
    "        rmin1e = rmin1 - extr_dr1\n",
    "        rmax1e = rmax1 + extr_dr1\n",
    "        \n",
    "        # \"caja\" g2 extrapolada\n",
    "        tmin2e = tmin2 - extr_dt2\n",
    "        tmax2e = tmax2 + extr_dt2\n",
    "        rmin2e = rmin2 - extr_dr2\n",
    "        rmax2e = rmax2 + extr_dr2\n",
    "        \n",
    "        # Chequeo de superposición 1D en theta\n",
    "        overlap_theta = not (tmax1e < tmin2e or tmax2e < tmin1e)\n",
    "        # Chequeo de superposición 1D en r\n",
    "        overlap_r = not (rmax1e < rmin2e or rmax2e < rmin1e)\n",
    "        \n",
    "        return overlap_theta and overlap_r\n",
    "    \n",
    "    # 4) Iterar fusiones\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        new_groups = []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1 = groups[i]\n",
    "            j = i+1\n",
    "            fused_any = False\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                # a) si no tiene slope, no fusionamos nada\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                # b) bounding boxes se superponen (con extrapolación 30%)\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    # c) Probamos a fusionar: combinamos puntos, recalculamos slope\n",
    "                    combined_points = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    \n",
    "                    # Ajuste con lineal\n",
    "                    slope_, intercept_, _, _, _, _, _ = fit_line_to_cluster(combined_points)\n",
    "                    if slope_ is not None:\n",
    "                        # Revisar variación vs la slope de g1\n",
    "                        slope_var1 = abs(slope_ - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        # Revisar también vs la slope de g2\n",
    "                        slope_var2 = abs(slope_ - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        # Si ambas variaciones < 40%, permitimos la fusión\n",
    "                        if slope_var1 < slope_variation_threshold and slope_var2 < slope_variation_threshold:\n",
    "                            # Hacemos la fusión\n",
    "                            g1['points'] = combined_points\n",
    "                            # Recalcular su slope/bbox\n",
    "                            recalc_properties(g1)\n",
    "                            # Eliminamos g2\n",
    "                            groups.pop(j)\n",
    "                            fused_any = True\n",
    "                            merged_flag = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new_groups.append(g1)\n",
    "            i += 1\n",
    "        groups = new_groups\n",
    "    \n",
    "    # 5) Limpieza final (por ejemplo, quitar grupos con <2 puntos)\n",
    "    final_groups = []\n",
    "    for gg in groups:\n",
    "        if len(gg['points']) >= 2:\n",
    "            final_groups.append(gg)\n",
    "    \n",
    "    return final_groups\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# EJEMPLO DE USO\n",
    "# ===========================\n",
    "\n",
    "def example_adjust_and_merge_seeds():\n",
    "    # Imagina que ya tienes \"bfs_clusters\" y \"df_filtrado\" de tu BFS:\n",
    "    #   bfs_clusters, df_filtrado = generate_bfs_seeds(...)\n",
    "\n",
    "    # Supongamos que 'bfs_clusters' es una lista de DataFrames con columns 'theta','r', etc.\n",
    "    # (aquí no pongo el BFS code para no repetir, pero se asume que lo tienes)\n",
    "\n",
    "    # EJEMPLO: ajustamos y fusionamos\n",
    "    final = adjust_and_merge_seeds(\n",
    "        bfs_clusters,                 # tu lista BFS\n",
    "        slope_variation_threshold=0.40,  # 40%\n",
    "        bounding_extrap=0.30            # 30%\n",
    "    )\n",
    "    print(\"Número de grupos finales:\", len(final))\n",
    "    for idx, fg in enumerate(final, 1):\n",
    "        print(f\"Grupo {idx}: slope={fg['slope']:.3f}, intercept={fg['intercept']:.3f}, #pts={len(fg['points'])}\")\n",
    "    \n",
    "    # Si quisieras graficar en (theta,r), algo así:\n",
    "    # (asumiendo que df_filtrado son todos los puntos BFS previos)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.scatter(df_filtrado['theta'], df_filtrado['r'], s=3, alpha=0.3, label='Datos BFS')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, group in enumerate(final):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = group['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"Grupo {i+1}\")\n",
    "        if group['slope'] is not None:\n",
    "            # Graficamos la recta\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 50)\n",
    "            r_lin = group['slope']*t_lin + group['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "    plt.xlabel(\"theta (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a634775-7046-4bdd-80b8-7a264563274e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "    id_halo=\"17\",\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    ")\n",
    "print(f\"Generadas {len(bfs_clusters)} semillas BFS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96883236-fd51-461b-94d9-bfa68e7ec73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ====================================\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ====================================\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Carga datos desde CSV: data_rho_<id_halo>_filtered.csv\n",
    "    Calcula r, theta, y filtra en [theta_min, theta_max].\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    # Filtrar en [theta_min, theta_max]\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    # Asegurarnos de que sean float\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "# ====================================\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ====================================\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Crea un \"grafo\" (lista de adyacencia) donde cada punto se conecta\n",
    "    a los vecinos si |dtheta| <= theta_diff y |dr| <= r_diff.\n",
    "    \"\"\"\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors = np.where(mask_neighbors)[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Recorre el 'graph' con BFS y produce clusters (componentes conexas).\n",
    "    Cada cluster se devuelve como un DataFrame con los puntos correspondientes.\n",
    "    \"\"\"\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "# ====================================\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ====================================\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Ordena df_cluster por 'mode' (theta/r), \n",
    "    y si encuentra saltos > gap_threshold, parte en sub-grupos.\n",
    "    \"\"\"\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "# ====================================\n",
    "# 4) Generar las Semillas BFS\n",
    "# ====================================\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Carga y filtra datos en [theta_min, theta_max].\n",
    "    2) Filtra por 'rho_resta_final_exp' > percentile (quartile_threshold).\n",
    "    3) Construe BFS rectangular (theta_diff, r_diff).\n",
    "    4) Extrae clusters y los subdivide por gaps en theta y r.\n",
    "    5) Retorna (bfs_clusters, df_filtrado).\n",
    "    \"\"\"\n",
    "    # A) Carga y filtra\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    # B) Filtrar por percentil\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # C) Grafo BFS\n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    # D) Subdividir por gaps\n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "# ====================================\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ====================================\n",
    "def calculate_pa(slope):\n",
    "    \"\"\"Calcula el 'position angle' (grados) a partir de la pendiente.\"\"\"\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    \"\"\"\n",
    "    Ajusta recta a df_cluster (si >=2 puntos).\n",
    "    Retorna slope, intercept, pa, bounding box (t_min,t_max,r_min,r_max).\n",
    "    \"\"\"\n",
    "    if len(df_cluster) < 2:\n",
    "        return None, None, None, None, None, None, None\n",
    "    \n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    slope_ = model.coef_[0]\n",
    "    intercept_ = model.intercept_\n",
    "    pa_ = calculate_pa(slope_)\n",
    "    \n",
    "    t_min = df_cluster['theta'].min()\n",
    "    t_max = df_cluster['theta'].max()\n",
    "    r_min = df_cluster['r'].min()\n",
    "    r_max = df_cluster['r'].max()\n",
    "    \n",
    "    return slope_, intercept_, pa_, t_min, t_max, r_min, r_max\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.40,  # 40% máx variación de pendiente\n",
    "    bounding_extrap=0.30            # 30% extrapolación en bounding box\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Ajusta recta a cada cluster BFS (>=2 puntos).\n",
    "    2) Fusiona recíprocamente mientras:\n",
    "       - Las bounding boxes extrapoladas se solapen\n",
    "       - La pendiente combinada no difiera > 'slope_variation_threshold' (40%) \n",
    "         de la pendiente de cada uno.\n",
    "\n",
    "    Retorna lista final de grupos con:\n",
    "      {\n",
    "        'points': DataFrame,\n",
    "        'slope',\n",
    "        'intercept',\n",
    "        'pa',\n",
    "        'theta_min',\n",
    "        'theta_max',\n",
    "        'r_min',\n",
    "        'r_max'\n",
    "      }\n",
    "    \"\"\"\n",
    "    # 1) Crear lista de grupos iniciales\n",
    "    groups = []\n",
    "    for clust_df in bfs_clusters:\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(clust_df)\n",
    "        grp = {\n",
    "            'points': clust_df.copy(),\n",
    "            'slope': slope_,\n",
    "            'intercept': intercept_,\n",
    "            'pa': pa_,\n",
    "            'theta_min': tmin,\n",
    "            'theta_max': tmax,\n",
    "            'r_min': rmin,\n",
    "            'r_max': rmax\n",
    "        }\n",
    "        groups.append(grp)\n",
    "    \n",
    "    # 2) Función para recalcular recta + bounding box\n",
    "    def recalc_properties(group):\n",
    "        dfp = group['points']\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(dfp)\n",
    "        group['slope'] = slope_\n",
    "        group['intercept'] = intercept_\n",
    "        group['pa'] = pa_\n",
    "        group['theta_min'] = tmin\n",
    "        group['theta_max'] = tmax\n",
    "        group['r_min'] = rmin\n",
    "        group['r_max'] = rmax\n",
    "\n",
    "    # 3) Chequeo de solapamiento en bounding box extrapolado\n",
    "    def boxes_overlap(g1, g2, extrap=0.30):\n",
    "        tmin1, tmax1 = g1['theta_min'], g1['theta_max']\n",
    "        rmin1, rmax1 = g1['r_min'], g1['r_max']\n",
    "        tmin2, tmax2 = g2['theta_min'], g2['theta_max']\n",
    "        rmin2, rmax2 = g2['r_min'], g2['r_max']\n",
    "        \n",
    "        if tmin1 is None or tmin2 is None:\n",
    "            return False\n",
    "        \n",
    "        dt1 = (tmax1 - tmin1)\n",
    "        dr1 = (rmax1 - rmin1)\n",
    "        dt2 = (tmax2 - tmin2)\n",
    "        dr2 = (rmax2 - rmin2)\n",
    "        \n",
    "        extr_dt1 = dt1*extrap\n",
    "        extr_dr1 = dr1*extrap\n",
    "        extr_dt2 = dt2*extrap\n",
    "        extr_dr2 = dr2*extrap\n",
    "        \n",
    "        # bounding box expandido g1\n",
    "        tmin1e = tmin1 - extr_dt1\n",
    "        tmax1e = tmax1 + extr_dt1\n",
    "        rmin1e = rmin1 - extr_dr1\n",
    "        rmax1e = rmax1 + extr_dr1\n",
    "        \n",
    "        # bounding box expandido g2\n",
    "        tmin2e = tmin2 - extr_dt2\n",
    "        tmax2e = tmax2 + extr_dt2\n",
    "        rmin2e = rmin2 - extr_dr2\n",
    "        rmax2e = rmax2 + extr_dr2\n",
    "        \n",
    "        overlap_theta = not (tmax1e < tmin2e or tmax2e < tmin1e)\n",
    "        overlap_r = not (rmax1e < rmin2e or rmax2e < rmin1e)\n",
    "        \n",
    "        return overlap_theta and overlap_r\n",
    "    \n",
    "    # 4) Iterar fusiones\n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        new_groups = []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1 = groups[i]\n",
    "            j = i+1\n",
    "            fused_any = False\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                # Chequea si ambos tienen slope (>=2 puntos)\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                # Verificar bounding box extrapolado\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    # Combinar puntos\n",
    "                    combined = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    # Ajustar recta nueva\n",
    "                    slope_, intercept_, _, _, _, _, _ = fit_line_to_cluster(combined)\n",
    "                    if slope_ is not None:\n",
    "                        # Checar variación vs pend. g1 y g2\n",
    "                        var1 = abs(slope_ - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        var2 = abs(slope_ - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if var1 < slope_variation_threshold and var2 < slope_variation_threshold:\n",
    "                            # Fusionar\n",
    "                            g1['points'] = combined\n",
    "                            recalc_properties(g1)\n",
    "                            groups.pop(j)\n",
    "                            fused_any = True\n",
    "                            merged_flag = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new_groups.append(g1)\n",
    "            i += 1\n",
    "        groups = new_groups\n",
    "    \n",
    "    # 5) Filtrar grupos con <2 puntos\n",
    "    final_groups = []\n",
    "    for gg in groups:\n",
    "        if len(gg['points']) >= 2:\n",
    "            final_groups.append(gg)\n",
    "    \n",
    "    return final_groups\n",
    "\n",
    "# ====================================\n",
    "# 6) Función de ejemplo para ejecutarlo TODO\n",
    "# ====================================\n",
    "def main_example():\n",
    "    # 1) Generar BFS\n",
    "    bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "        id_halo=\"17\",           # <--- Cambia a tu halo\n",
    "        theta_min=20,\n",
    "        theta_max=360,\n",
    "        quartile_threshold=0.55,\n",
    "        theta_diff=3.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=2.0,\n",
    "        gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'  # Ajusta si tu CSV se llama distinto\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs_clusters)} grupos\")\n",
    "\n",
    "    # 2) Ajustar & Fusionar\n",
    "    final_groups = adjust_and_merge_seeds(\n",
    "        bfs_clusters,\n",
    "        slope_variation_threshold=0.40,  # 40% máx variación de pendiente\n",
    "        bounding_extrap=0.30            # 30% de extrapolación bounding box\n",
    "    )\n",
    "    print(f\"Grupos finales tras fusiones: {len(final_groups)}\")\n",
    "    \n",
    "    # 3) Graficar en (theta, r)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # Todos los puntos BFS en gris\n",
    "    plt.scatter(df_filtrado['theta'], df_filtrado['r'], s=3, alpha=0.3, label='Datos BFS')\n",
    "    \n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, group in enumerate(final_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = group['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"Grupo {i+1}\")\n",
    "        if group['slope'] is not None:\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 50)\n",
    "            r_lin = group['slope']*t_lin + group['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "    plt.xlabel(\"theta (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"BFS + Ajuste y Fusión de Semillas\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ====================================\n",
    "# Para ejecutar, haz:\n",
    "# main_example()\n",
    "# ====================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291034ed-c18d-4142-a280-e61b97b27527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738aac53-1510-4adb-9ef9-55240ec4c3be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ====================================\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ====================================\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "# ====================================\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ====================================\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors = np.where(mask_neighbors)[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    from collections import deque\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "# ====================================\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ====================================\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "# ====================================\n",
    "# 4) Generar Semillas BFS\n",
    "# ====================================\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "# ====================================\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ====================================\n",
    "def calculate_pa(slope):\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return None, None, None, None, None, None, None\n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    slope_ = model.coef_[0]\n",
    "    intercept_ = model.intercept_\n",
    "    pa_ = calculate_pa(slope_)\n",
    "    t_min = df_cluster['theta'].min()\n",
    "    t_max = df_cluster['theta'].max()\n",
    "    r_min = df_cluster['r'].min()\n",
    "    r_max = df_cluster['r'].max()\n",
    "    return slope_, intercept_, pa_, t_min, t_max, r_min, r_max\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.40,\n",
    "    bounding_extrap=0.30\n",
    "):\n",
    "    groups = []\n",
    "    for clust_df in bfs_clusters:\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(clust_df)\n",
    "        grp = {\n",
    "            'points': clust_df.copy(),\n",
    "            'slope': slope_,\n",
    "            'intercept': intercept_,\n",
    "            'pa': pa_,\n",
    "            'theta_min': tmin,\n",
    "            'theta_max': tmax,\n",
    "            'r_min': rmin,\n",
    "            'r_max': rmax\n",
    "        }\n",
    "        groups.append(grp)\n",
    "    \n",
    "    def recalc_properties(group):\n",
    "        dfp = group['points']\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(dfp)\n",
    "        group['slope'] = slope_\n",
    "        group['intercept'] = intercept_\n",
    "        group['pa'] = pa_\n",
    "        group['theta_min'] = tmin\n",
    "        group['theta_max'] = tmax\n",
    "        group['r_min'] = rmin\n",
    "        group['r_max'] = rmax\n",
    "\n",
    "    def boxes_overlap(g1, g2, extrap=0.30):\n",
    "        tmin1, tmax1 = g1['theta_min'], g1['theta_max']\n",
    "        rmin1, rmax1 = g1['r_min'], g1['r_max']\n",
    "        tmin2, tmax2 = g2['theta_min'], g2['theta_max']\n",
    "        rmin2, rmax2 = g2['r_min'], g2['r_max']\n",
    "        if tmin1 is None or tmin2 is None:\n",
    "            return False\n",
    "        dt1 = (tmax1 - tmin1)\n",
    "        dr1 = (rmax1 - rmin1)\n",
    "        dt2 = (tmax2 - tmin2)\n",
    "        dr2 = (rmax2 - rmin2)\n",
    "        extr_dt1 = dt1*extrap\n",
    "        extr_dr1 = dr1*extrap\n",
    "        extr_dt2 = dt2*extrap\n",
    "        extr_dr2 = dr2*extrap\n",
    "        \n",
    "        tmin1e = tmin1 - extr_dt1\n",
    "        tmax1e = tmax1 + extr_dt1\n",
    "        rmin1e = rmin1 - extr_dr1\n",
    "        rmax1e = rmax1 + extr_dr1\n",
    "        \n",
    "        tmin2e = tmin2 - extr_dt2\n",
    "        tmax2e = tmax2 + extr_dt2\n",
    "        rmin2e = rmin2 - extr_dr2\n",
    "        rmax2e = rmax2 + extr_dr2\n",
    "        \n",
    "        overlap_theta = not (tmax1e < tmin2e or tmax2e < tmin1e)\n",
    "        overlap_r = not (rmax1e < rmin2e or rmax2e < rmin1e)\n",
    "        return overlap_theta and overlap_r\n",
    "    \n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        new_groups = []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1 = groups[i]\n",
    "            j = i+1\n",
    "            fused_any = False\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    combined = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    slope_, intercept_, _, _, _, _, _ = fit_line_to_cluster(combined)\n",
    "                    if slope_ is not None:\n",
    "                        var1 = abs(slope_ - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        var2 = abs(slope_ - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if var1 < slope_variation_threshold and var2 < slope_variation_threshold:\n",
    "                            g1['points'] = combined\n",
    "                            recalc_properties(g1)\n",
    "                            groups.pop(j)\n",
    "                            fused_any = True\n",
    "                            merged_flag = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new_groups.append(g1)\n",
    "            i += 1\n",
    "        groups = new_groups\n",
    "    \n",
    "    final_groups = []\n",
    "    for gg in groups:\n",
    "        if len(gg['points']) >= 2:\n",
    "            final_groups.append(gg)\n",
    "    return final_groups\n",
    "\n",
    "# ====================================\n",
    "# 6) [Nuevo] Validar dispersión y re-procesar\n",
    "# ====================================\n",
    "def validate_dispersion_and_reprocess(\n",
    "    groups,\n",
    "    dispersion_threshold=2.0,   # umbral de desviación std (ejemplo)\n",
    "    reproc_theta_diff=2.0,      # BFS para reprocesar\n",
    "    reproc_r_diff=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    - Verifica la 'dispersión' (std de los residuos de cada grupo).\n",
    "    - Si un grupo excede 'dispersion_threshold', se 'rompe':\n",
    "       * Tomamos sus puntos\n",
    "       * Reaplicamos BFS (o un mini-proceso) a ese subconjunto\n",
    "       * Ajustamos rectas otra vez\n",
    "    - Devuelve la lista final de grupos (los que no se rompieron + \n",
    "      los subgrupos reagrupados).\n",
    "    \"\"\"\n",
    "    # 1) Separar grupos \"buenos\" de grupos \"a re-procesar\"\n",
    "    groups_ok = []\n",
    "    groups_toreprocess = []\n",
    "    \n",
    "    for grp in groups:\n",
    "        slope = grp['slope']\n",
    "        intercept = grp['intercept']\n",
    "        pts = grp['points']\n",
    "        if slope is None or len(pts) < 2:\n",
    "            # Grupo muy pequeño -> considerarlo \"ok\" sin dispersión\n",
    "            groups_ok.append(grp)\n",
    "            continue\n",
    "        \n",
    "        # Calcular residuos\n",
    "        predicted_r = slope*pts['theta'] + intercept\n",
    "        residuals = pts['r'] - predicted_r\n",
    "        std_resid = residuals.std()  # desviación estándar\n",
    "        # Checar si std_resid > threshold\n",
    "        if std_resid > dispersion_threshold:\n",
    "            groups_toreprocess.append(grp)\n",
    "        else:\n",
    "            groups_ok.append(grp)\n",
    "    \n",
    "    # 2) Re-procesar los que superaron la dispersión\n",
    "    #    Por ejemplo, usando BFS de nuevo\n",
    "    new_groups = []\n",
    "    for grp in groups_toreprocess:\n",
    "        # Tomar sus puntos:\n",
    "        subdf = grp['points'].copy()\n",
    "        subdf.reset_index(drop=True, inplace=True)\n",
    "        # BFS con reproc_theta_diff, reproc_r_diff\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        clusters_sub = bfs_components(graph, subdf)\n",
    "        \n",
    "        # A) Ajustamos recta a cada mini-cluster (sin fusión en este ejemplo)\n",
    "        #    O podrías re-llamar a adjust_and_merge_seeds con slope_variation_threshold,\n",
    "        #    si quieres un pipeline más largo.\n",
    "        \n",
    "        for csub in clusters_sub:\n",
    "            slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(csub)\n",
    "            new_groups.append({\n",
    "                'points': csub,\n",
    "                'slope': slope_,\n",
    "                'intercept': intercept_,\n",
    "                'pa': pa_,\n",
    "                'theta_min': tmin,\n",
    "                'theta_max': tmax,\n",
    "                'r_min': rmin,\n",
    "                'r_max': rmax\n",
    "            })\n",
    "    \n",
    "    # 3) Unir \"groups_ok\" + \"new_groups\" en la lista final\n",
    "    final_res = groups_ok + new_groups\n",
    "    \n",
    "    return final_res\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# 7) Función principal que muestra TODO\n",
    "# ====================================\n",
    "def main_example():\n",
    "    # 1) Generar BFS\n",
    "    bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=20,\n",
    "        theta_max=350,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=2.0,\n",
    "        gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs_clusters)} grupos\")\n",
    "    \n",
    "    # 2) Ajustar y fusionar\n",
    "    merged_groups = adjust_and_merge_seeds(\n",
    "        bfs_clusters,\n",
    "        slope_variation_threshold=0.30,\n",
    "        bounding_extrap=0.35\n",
    "    )\n",
    "    print(f\"Grupos tras fusión inicial: {len(merged_groups)}\")\n",
    "    \n",
    "    # 3) Validar dispersión y re-procesar grupos con alta dispersión\n",
    "    #    Por ejemplo, dispersion_threshold=2.0\n",
    "    validated_groups = validate_dispersion_and_reprocess(\n",
    "        merged_groups,\n",
    "        dispersion_threshold=1.80, # si la std del residuo >2 -> re-proceso BFS\n",
    "        reproc_theta_diff=1.25,\n",
    "        reproc_r_diff=0.4\n",
    "    )\n",
    "    print(f\"Grupos tras validación dispersión: {len(validated_groups)}\")\n",
    "    \n",
    "    # 4) Graficar\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_filtrado['theta'], df_filtrado['r'], s=3, alpha=0.3, label='Datos BFS')\n",
    "    \n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, group in enumerate(validated_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = group['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1}\")\n",
    "        if group['slope'] is not None:\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 50)\n",
    "            r_lin = group['slope']*t_lin + group['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "    plt.xlabel(\"theta (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"BFS + Fusión + Validación de Dispersión (reprocesado)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Ejecución:\n",
    "# main_example()\n",
    "# ====================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94094ed8-99a3-4e45-8806-577aec87dbc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbcc72e6-011a-417e-8baa-49e8a2b8ba9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b64a771c-de61-45c2-a0d3-c3b9084ac04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Halo 17, procesamiendo y agregacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a699facc-3af5-4063-8194-f93d929046a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ====================================\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ====================================\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "# ====================================\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ====================================\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors = np.where(mask_neighbors)[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "# ====================================\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ====================================\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "# ====================================\n",
    "# 4) Generar Semillas BFS\n",
    "# ====================================\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "# ====================================\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ====================================\n",
    "def calculate_pa(slope):\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return None, None, None, None, None, None, None\n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    slope_ = model.coef_[0]\n",
    "    intercept_ = model.intercept_\n",
    "    pa_ = calculate_pa(slope_)\n",
    "    t_min = df_cluster['theta'].min()\n",
    "    t_max = df_cluster['theta'].max()\n",
    "    r_min = df_cluster['r'].min()\n",
    "    r_max = df_cluster['r'].max()\n",
    "    return slope_, intercept_, pa_, t_min, t_max, r_min, r_max\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.40,\n",
    "    bounding_extrap=0.30\n",
    "):\n",
    "    groups = []\n",
    "    for clust_df in bfs_clusters:\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(clust_df)\n",
    "        grp = {\n",
    "            'points': clust_df.copy(),\n",
    "            'slope': slope_,\n",
    "            'intercept': intercept_,\n",
    "            'pa': pa_,\n",
    "            'theta_min': tmin,\n",
    "            'theta_max': tmax,\n",
    "            'r_min': rmin,\n",
    "            'r_max': rmax\n",
    "        }\n",
    "        groups.append(grp)\n",
    "    \n",
    "    def recalc_properties(group):\n",
    "        dfp = group['points']\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(dfp)\n",
    "        group['slope'] = slope_\n",
    "        group['intercept'] = intercept_\n",
    "        group['pa'] = pa_\n",
    "        group['theta_min'] = tmin\n",
    "        group['theta_max'] = tmax\n",
    "        group['r_min'] = rmin\n",
    "        group['r_max'] = rmax\n",
    "\n",
    "    def boxes_overlap(g1, g2, extrap=0.30):\n",
    "        tmin1, tmax1 = g1['theta_min'], g1['theta_max']\n",
    "        rmin1, rmax1 = g1['r_min'], g1['r_max']\n",
    "        tmin2, tmax2 = g2['theta_min'], g2['theta_max']\n",
    "        rmin2, rmax2 = g2['r_min'], g2['r_max']\n",
    "        if tmin1 is None or tmin2 is None:\n",
    "            return False\n",
    "        dt1 = (tmax1 - tmin1)\n",
    "        dr1 = (rmax1 - rmin1)\n",
    "        dt2 = (tmax2 - tmin2)\n",
    "        dr2 = (rmax2 - rmin2)\n",
    "        extr_dt1 = dt1*extrap\n",
    "        extr_dr1 = dr1*extrap\n",
    "        extr_dt2 = dt2*extrap\n",
    "        extr_dr2 = dr2*extrap\n",
    "        \n",
    "        tmin1e = tmin1 - extr_dt1\n",
    "        tmax1e = tmax1 + extr_dt1\n",
    "        rmin1e = rmin1 - extr_dr1\n",
    "        rmax1e = rmax1 + extr_dr1\n",
    "        \n",
    "        tmin2e = tmin2 - extr_dt2\n",
    "        tmax2e = tmax2 + extr_dt2\n",
    "        rmin2e = rmin2 - extr_dr2\n",
    "        rmax2e = rmax2 + extr_dr2\n",
    "        \n",
    "        overlap_theta = not (tmax1e < tmin2e or tmax2e < tmin1e)\n",
    "        overlap_r = not (rmax1e < rmin2e or rmax2e < rmin1e)\n",
    "        return overlap_theta and overlap_r\n",
    "    \n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        new_groups = []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1 = groups[i]\n",
    "            j = i+1\n",
    "            fused_any = False\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    combined = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    slope_, intercept_, _, _, _, _, _ = fit_line_to_cluster(combined)\n",
    "                    if slope_ is not None:\n",
    "                        var1 = abs(slope_ - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        var2 = abs(slope_ - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if var1 < slope_variation_threshold and var2 < slope_variation_threshold:\n",
    "                            g1['points'] = combined\n",
    "                            recalc_properties(g1)\n",
    "                            groups.pop(j)\n",
    "                            fused_any = True\n",
    "                            merged_flag = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new_groups.append(g1)\n",
    "            i += 1\n",
    "        groups = new_groups\n",
    "    \n",
    "    final_groups = []\n",
    "    for gg in groups:\n",
    "        if len(gg['points']) >= 2:\n",
    "            final_groups.append(gg)\n",
    "    return final_groups\n",
    "\n",
    "# ====================================\n",
    "# 6) Validar dispersión y re-procesar (opcional)\n",
    "# ====================================\n",
    "def validate_dispersion_and_reprocess(\n",
    "    groups,\n",
    "    dispersion_threshold=2.0,   # umbral de desviación std (ejemplo)\n",
    "    reproc_theta_diff=2.0,      # BFS para reprocesar\n",
    "    reproc_r_diff=0.5\n",
    "):\n",
    "    groups_ok = []\n",
    "    groups_toreprocess = []\n",
    "    \n",
    "    for grp in groups:\n",
    "        slope = grp['slope']\n",
    "        intercept = grp['intercept']\n",
    "        pts = grp['points']\n",
    "        if slope is None or len(pts) < 2:\n",
    "            groups_ok.append(grp)\n",
    "            continue\n",
    "        \n",
    "        # Calcular std de residuo\n",
    "        predicted_r = slope*pts['theta'] + intercept\n",
    "        residuals = pts['r'] - predicted_r\n",
    "        std_resid = residuals.std()\n",
    "        if std_resid > dispersion_threshold:\n",
    "            groups_toreprocess.append(grp)\n",
    "        else:\n",
    "            groups_ok.append(grp)\n",
    "    \n",
    "    # Reprocesar los grupos con alta dispersión\n",
    "    new_groups = []\n",
    "    for grp in groups_toreprocess:\n",
    "        subdf = grp['points'].copy()\n",
    "        subdf.reset_index(drop=True, inplace=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        clusters_sub = bfs_components(graph, subdf)\n",
    "        \n",
    "        for csub in clusters_sub:\n",
    "            slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(csub)\n",
    "            new_groups.append({\n",
    "                'points': csub,\n",
    "                'slope': slope_,\n",
    "                'intercept': intercept_,\n",
    "                'pa': pa_,\n",
    "                'theta_min': tmin,\n",
    "                'theta_max': tmax,\n",
    "                'r_min': rmin,\n",
    "                'r_max': rmax\n",
    "            })\n",
    "    \n",
    "    final_res = groups_ok + new_groups\n",
    "    return final_res\n",
    "\n",
    "# =========================================\n",
    "# 7) Nueva fase: Elegir grupos con >= 60 puntos y graficar con bounding box extrapolado\n",
    "# =========================================\n",
    "def plot_final_segments_over_60(groups, df_all, bounding_extrap=0.30):\n",
    "    \"\"\"\n",
    "    - Toma los grupos finales\n",
    "    - Filtra los que tienen >= 60 puntos\n",
    "    - Grafica en un nuevo plot:\n",
    "        * Los puntos de cada grupo\n",
    "        * La recta\n",
    "        * Un rectángulo que muestre el bounding box extrapolado un 30%\n",
    "    \"\"\"\n",
    "    # 1. Filtrar\n",
    "    large_groups = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos con >= 60 puntos: {len(large_groups)}\")\n",
    "    \n",
    "    # 2. Crear figura\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Fondo con df_all en gris\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "    \n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    \n",
    "    for i, grp in enumerate(large_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = grp['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"Grupo_{i+1} ({len(pts)} pts)\")\n",
    "        \n",
    "        slope_ = grp['slope']\n",
    "        intercept_ = grp['intercept']\n",
    "        if slope_ is not None:\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 100)\n",
    "            r_lin = slope_*t_lin + intercept_\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "        \n",
    "        # 3. Dibujar bounding box extrapolado 30%\n",
    "        # bounding box actual\n",
    "        tmin_bb = grp['theta_min']\n",
    "        tmax_bb = grp['theta_max']\n",
    "        rmin_bb = grp['r_min']\n",
    "        rmax_bb = grp['r_max']\n",
    "        \n",
    "        # Calcular ancho/alto\n",
    "        dt = (tmax_bb - tmin_bb)\n",
    "        dr = (rmax_bb - rmin_bb)\n",
    "        dt_ext = dt * bounding_extrap\n",
    "        dr_ext = dr * bounding_extrap\n",
    "        \n",
    "        # bounding box expandido\n",
    "        tmin_e = tmin_bb - dt_ext\n",
    "        tmax_e = tmax_bb + dt_ext\n",
    "        rmin_e = rmin_bb - dr_ext\n",
    "        rmax_e = rmax_bb + dr_ext\n",
    "        \n",
    "        # Dibujar rectángulo con plt.plot (4 lados)\n",
    "        # forma => [(x1, y1), (x1, y2), (x2, y2), (x2, y1), (x1, y1)]\n",
    "        # donde x=theta, y=r\n",
    "        box_x = [tmin_e, tmin_e, tmax_e, tmax_e, tmin_e]\n",
    "        box_y = [rmin_e, rmax_e, rmax_e, rmin_e, rmin_e]\n",
    "        plt.plot(box_x, box_y, color=c, linewidth=1.2)\n",
    "    \n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Grupos con >= 60 puntos y su bounding box extrapolado\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =========================================\n",
    "# 8) Función principal: ejemplo\n",
    "# =========================================\n",
    "def main_example():\n",
    "    # Paso 1: generar BFS\n",
    "    bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=20,\n",
    "        theta_max=350,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=2.0,\n",
    "        gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs_clusters)} grupos\")\n",
    "    \n",
    "    # Paso 2: ajustar/fusionar\n",
    "    merged_groups = adjust_and_merge_seeds(\n",
    "        bfs_clusters,\n",
    "        slope_variation_threshold=0.30,\n",
    "        bounding_extrap=0.35\n",
    "    )\n",
    "    print(f\"Grupos tras fusión inicial: {len(merged_groups)}\")\n",
    "    \n",
    "    # Paso 3: validación de dispersión (opcional)\n",
    "    validated_groups = validate_dispersion_and_reprocess(\n",
    "        merged_groups,\n",
    "        dispersion_threshold=1.80,\n",
    "        reproc_theta_diff=1.25,\n",
    "        reproc_r_diff=0.4\n",
    "    )\n",
    "    print(f\"Grupos tras validación dispersión: {len(validated_groups)}\")\n",
    "    \n",
    "    # Plot de esos grupos en general\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(df_filtrado['theta'], df_filtrado['r'], s=3, alpha=0.3, label='Datos BFS')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, group in enumerate(validated_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = group['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)}pts)\")\n",
    "        if group['slope'] is not None:\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 100)\n",
    "            r_lin = group['slope']*t_lin + group['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    #plt.legend(loc='best')\n",
    "    plt.title(\"Grupos finales (validados)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Paso 4: ahora filtrar los grupos con >=60 puntos y graficarlos aparte\n",
    "    plot_final_segments_over_60(validated_groups, df_filtrado, bounding_extrap=0.10)\n",
    "\n",
    "\n",
    "main_example()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51480617-198c-47ce-8c6c-ac034bc55698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ====================================\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ====================================\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "# ====================================\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ====================================\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors = np.where(mask_neighbors)[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "# ====================================\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ====================================\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "# ====================================\n",
    "# 4) Generar Semillas BFS\n",
    "# ====================================\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "# ====================================\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ====================================\n",
    "def calculate_pa(slope):\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return None, None, None, None, None, None, None\n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    slope_ = model.coef_[0]\n",
    "    intercept_ = model.intercept_\n",
    "    pa_ = calculate_pa(slope_)\n",
    "    t_min = df_cluster['theta'].min()\n",
    "    t_max = df_cluster['theta'].max()\n",
    "    r_min = df_cluster['r'].min()\n",
    "    r_max = df_cluster['r'].max()\n",
    "    return slope_, intercept_, pa_, t_min, t_max, r_min, r_max\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.40,\n",
    "    bounding_extrap=0.30\n",
    "):\n",
    "    groups = []\n",
    "    for clust_df in bfs_clusters:\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(clust_df)\n",
    "        grp = {\n",
    "            'points': clust_df.copy(),\n",
    "            'slope': slope_,\n",
    "            'intercept': intercept_,\n",
    "            'pa': pa_,\n",
    "            'theta_min': tmin,\n",
    "            'theta_max': tmax,\n",
    "            'r_min': rmin,\n",
    "            'r_max': rmax\n",
    "        }\n",
    "        groups.append(grp)\n",
    "    \n",
    "    def recalc_properties(group):\n",
    "        dfp = group['points']\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(dfp)\n",
    "        group['slope'] = slope_\n",
    "        group['intercept'] = intercept_\n",
    "        group['pa'] = pa_\n",
    "        group['theta_min'] = tmin\n",
    "        group['theta_max'] = tmax\n",
    "        group['r_min'] = rmin\n",
    "        group['r_max'] = rmax\n",
    "\n",
    "    def boxes_overlap(g1, g2, extrap=0.30):\n",
    "        tmin1, tmax1 = g1['theta_min'], g1['theta_max']\n",
    "        rmin1, rmax1 = g1['r_min'], g1['r_max']\n",
    "        tmin2, tmax2 = g2['theta_min'], g2['theta_max']\n",
    "        rmin2, rmax2 = g2['r_min'], g2['r_max']\n",
    "        if tmin1 is None or tmin2 is None:\n",
    "            return False\n",
    "        dt1 = (tmax1 - tmin1)\n",
    "        dr1 = (rmax1 - rmin1)\n",
    "        dt2 = (tmax2 - tmin2)\n",
    "        dr2 = (rmax2 - rmin2)\n",
    "        extr_dt1 = dt1*extrap\n",
    "        extr_dr1 = dr1*extrap\n",
    "        extr_dt2 = dt2*extrap\n",
    "        extr_dr2 = dr2*extrap\n",
    "        \n",
    "        tmin1e = tmin1 - extr_dt1\n",
    "        tmax1e = tmax1 + extr_dt1\n",
    "        rmin1e = rmin1 - extr_dr1\n",
    "        rmax1e = rmax1 + extr_dr1\n",
    "        \n",
    "        tmin2e = tmin2 - extr_dt2\n",
    "        tmax2e = tmax2 + extr_dt2\n",
    "        rmin2e = rmin2 - extr_dr2\n",
    "        rmax2e = rmax2 + extr_dr2\n",
    "        \n",
    "        overlap_theta = not (tmax1e < tmin2e or tmax2e < tmin1e)\n",
    "        overlap_r = not (rmax1e < rmin2e or rmax2e < rmin1e)\n",
    "        return overlap_theta and overlap_r\n",
    "    \n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        new_groups = []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1 = groups[i]\n",
    "            j = i+1\n",
    "            fused_any = False\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    combined = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    slope_, intercept_, _, _, _, _, _ = fit_line_to_cluster(combined)\n",
    "                    if slope_ is not None:\n",
    "                        var1 = abs(slope_ - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        var2 = abs(slope_ - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if var1 < slope_variation_threshold and var2 < slope_variation_threshold:\n",
    "                            g1['points'] = combined\n",
    "                            recalc_properties(g1)\n",
    "                            groups.pop(j)\n",
    "                            fused_any = True\n",
    "                            merged_flag = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new_groups.append(g1)\n",
    "            i += 1\n",
    "        groups = new_groups\n",
    "    \n",
    "    final_groups = []\n",
    "    for gg in groups:\n",
    "        if len(gg['points']) >= 2:\n",
    "            final_groups.append(gg)\n",
    "    return final_groups\n",
    "\n",
    "# ====================================\n",
    "# 6) Validar dispersión y re-procesar (opcional)\n",
    "# ====================================\n",
    "def validate_dispersion_and_reprocess(\n",
    "    groups,\n",
    "    dispersion_threshold=2.0,   # umbral de desviación std (ejemplo)\n",
    "    reproc_theta_diff=2.0,      # BFS para reprocesar\n",
    "    reproc_r_diff=0.5\n",
    "):\n",
    "    groups_ok = []\n",
    "    groups_toreprocess = []\n",
    "    \n",
    "    for grp in groups:\n",
    "        slope = grp['slope']\n",
    "        intercept = grp['intercept']\n",
    "        pts = grp['points']\n",
    "        if slope is None or len(pts) < 2:\n",
    "            groups_ok.append(grp)\n",
    "            continue\n",
    "        \n",
    "        predicted_r = slope*pts['theta'] + intercept\n",
    "        residuals = pts['r'] - predicted_r\n",
    "        std_resid = residuals.std()\n",
    "        if std_resid > dispersion_threshold:\n",
    "            groups_toreprocess.append(grp)\n",
    "        else:\n",
    "            groups_ok.append(grp)\n",
    "    \n",
    "    new_groups = []\n",
    "    for grp in groups_toreprocess:\n",
    "        subdf = grp['points'].copy()\n",
    "        subdf.reset_index(drop=True, inplace=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        clusters_sub = bfs_components(graph, subdf)\n",
    "        \n",
    "        for csub in clusters_sub:\n",
    "            slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(csub)\n",
    "            new_groups.append({\n",
    "                'points': csub,\n",
    "                'slope': slope_,\n",
    "                'intercept': intercept_,\n",
    "                'pa': pa_,\n",
    "                'theta_min': tmin,\n",
    "                'theta_max': tmax,\n",
    "                'r_min': rmin,\n",
    "                'r_max': rmax\n",
    "            })\n",
    "    \n",
    "    final_res = groups_ok + new_groups\n",
    "    return final_res\n",
    "\n",
    "# =========================================\n",
    "# 7) Nueva función: Graficar grupos >= 60 puntos con la recta extrapolada un 30%\n",
    "# =========================================\n",
    "def plot_final_segments_over_60_with_line_extrapolation(groups, df_all, line_extrap=0.3):\n",
    "    \"\"\"\n",
    "    - Filtra grupos con >= 60 puntos\n",
    "    - Grafica cada uno en un nuevo plot (theta vs r)\n",
    "    - En lugar de dibujar bounding box, extiende la recta un 'line_extrap'% \n",
    "      por debajo y por encima del rango de theta.\n",
    "    \"\"\"\n",
    "    # 1) Filtrar los grupos grandes\n",
    "    large_groups = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos con >= 60 puntos: {len(large_groups)}\")\n",
    "\n",
    "    # 2) Crear figura\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Fondo con df_all en gris\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "\n",
    "    for i, grp in enumerate(large_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = grp['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"Grupo_{i+1} ({len(pts)} pts)\")\n",
    "\n",
    "        slope_ = grp['slope']\n",
    "        intercept_ = grp['intercept']\n",
    "        if slope_ is not None and grp['theta_min'] is not None and grp['theta_max'] is not None:\n",
    "            # rango de theta del grupo\n",
    "            theta_min = grp['theta_min']\n",
    "            theta_max = grp['theta_max']\n",
    "            dtheta = (theta_max - theta_min)\n",
    "            # Extender un 30%\n",
    "            theta_min_ext = theta_min - line_extrap*dtheta\n",
    "            theta_max_ext = theta_max + line_extrap*dtheta\n",
    "\n",
    "            # Graficar la línea en el rango extendido\n",
    "            t_lin = np.linspace(theta_min_ext, theta_max_ext, 200)\n",
    "            r_lin = slope_*t_lin + intercept_\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "            plt.plot(t_lin, r_lin, '-.', color='k', alpha=0.3)\n",
    "\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Grupos >= 60 puntos con recta extrapolada {int(line_extrap*100)}%\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =========================================\n",
    "# 8) Función principal: ejemplo\n",
    "# =========================================\n",
    "def main_example():\n",
    "    # A) Generar BFS\n",
    "    bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0,\n",
    "        theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=2.0,\n",
    "        gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs_clusters)} grupos\")\n",
    "\n",
    "    # B) Ajustar y fusionar\n",
    "    merged_groups = adjust_and_merge_seeds(\n",
    "        bfs_clusters,\n",
    "        slope_variation_threshold=0.30,\n",
    "        bounding_extrap=0.35\n",
    "    )\n",
    "    print(f\"Grupos tras fusión inicial: {len(merged_groups)}\")\n",
    "\n",
    "    # C) Validar dispersión y re-procesar\n",
    "    validated_groups = validate_dispersion_and_reprocess(\n",
    "        merged_groups,\n",
    "        dispersion_threshold=1.80,\n",
    "        reproc_theta_diff=1.25,\n",
    "        reproc_r_diff=0.4\n",
    "    )\n",
    "    print(f\"Grupos tras validación dispersión: {len(validated_groups)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # D) Grafico global (todos los grupos) en (theta, r)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_filtrado['theta'], df_filtrado['r'], s=3, alpha=0.3, label='Datos BFS')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, group in enumerate(validated_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = group['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)}pts)\")\n",
    "        if group['slope'] is not None:\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 100)\n",
    "            r_lin = group['slope']*t_lin + group['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    #plt.legend(loc='best')\n",
    "    plt.title(\"Grupos finales (validados)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # E) Ahora, filtrar los grupos con >=60 puntos y graficar su recta extrapolada\n",
    "    plot_final_segments_over_60_with_line_extrapolation(\n",
    "        validated_groups,\n",
    "        df_filtrado,\n",
    "        line_extrap=0.15  # 30% de extrapolación\n",
    "    )\n",
    "\n",
    "\n",
    "    def plot_final_segments_over_60_polar(groups, df_all, line_extrap=0.15):\n",
    "        \"\"\"\n",
    "        Grafica en coordenadas polares (r, theta):\n",
    "        - Filtra los grupos con >= 60 puntos.\n",
    "        - Para cada grupo, crea una curva r = slope*theta_deg + intercept,\n",
    "        donde theta_deg se extrapola un 'line_extrap'% a cada lado\n",
    "        y se convierte a radianes para dibujar.\n",
    "\n",
    "        Param:\n",
    "        groups: lista de dicts con campos:\n",
    "                'points' (DataFrame con 'theta','r'),\n",
    "                'slope','intercept','theta_min','theta_max', ...\n",
    "        df_all: DataFrame con las columnas 'theta','r' (theta en grados).\n",
    "        line_extrap: factor de extrapolación (e.g. 0.3 = 30%)\n",
    "        \"\"\"\n",
    "        # 1) Filtrar grupos con >=60 puntos\n",
    "        large_groups = [g for g in groups if len(g['points']) >= 60]\n",
    "        print(f\"Grupos con >= 60 puntos: {len(large_groups)}\")\n",
    "\n",
    "        # 2) Crear figura con proyección polar\n",
    "        fig = plt.figure(figsize=(9, 8))\n",
    "        ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "        # 3) Fondo con df_all en gris (convirtiendo theta a radianes)\n",
    "        theta_all_rad = np.radians(df_all['theta'].values)\n",
    "        r_all = df_all['r'].values\n",
    "        ax.scatter(theta_all_rad, r_all, s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "        # 4) Graficar cada grupo\n",
    "        colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "        for i, grp in enumerate(large_groups):\n",
    "            c = colors[i % len(colors)]\n",
    "            pts = grp['points']\n",
    "\n",
    "            # a) scatter de puntos del grupo (convertir theta->rad)\n",
    "            theta_pts_rad = np.radians(pts['theta'].values)\n",
    "            r_pts = pts['r'].values\n",
    "            label_txt = f\"Grupo_{i+1} ({len(pts)} pts)\"\n",
    "            ax.scatter(theta_pts_rad, r_pts, s=10, color=c, label=label_txt)\n",
    "\n",
    "            # b) línea extrapolada\n",
    "            slope_ = grp['slope']\n",
    "            intercept_ = grp['intercept']\n",
    "            tmin_deg = grp.get('theta_min', None)\n",
    "            tmax_deg = grp.get('theta_max', None)\n",
    "            if slope_ is not None and tmin_deg is not None and tmax_deg is not None:\n",
    "                dt = tmax_deg - tmin_deg\n",
    "                tmin_ext = tmin_deg - line_extrap*dt\n",
    "                tmax_ext = tmax_deg + line_extrap*dt\n",
    "\n",
    "                # Generamos 200 puntos en grados\n",
    "                theta_deg_line = np.linspace(tmin_ext, tmax_ext, 200)\n",
    "                # Calculamos r en base a slope*(theta_deg) + intercept\n",
    "                r_line = slope_*theta_deg_line + intercept_\n",
    "                # Convertimos a radianes para plot\n",
    "                theta_rad_line = np.radians(theta_deg_line)\n",
    "\n",
    "                ax.plot(theta_rad_line, r_line, '--', color=c)\n",
    "\n",
    "        # 5) Ajustes finales en polar\n",
    "        ax.set_theta_zero_location(\"E\")   # opcional: 0° a la derecha (eje x)\n",
    "        ax.set_theta_direction(-1)        # opcional: ángulos crecen en sentido horario\n",
    "        ax.set_rlabel_position(0)         # etiquetas de r en 0 grados\n",
    "        ax.set_title(\"Grupos >= 60 puntos (coordenadas polares, 30% extrapolación)\", y=1.05)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "        plt.show()\n",
    "\n",
    "    plot_final_segments_over_60_polar(\n",
    "    validated_groups,\n",
    "    df_filtrado,\n",
    "    line_extrap=0.15  # 30% de extrapolación\n",
    ")\n",
    "\n",
    "main_example()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e34063b7-853d-4976-bc96-65d1357f8239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5241a111-b5e7-4e49-8bc9-e87d272b41cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ====================================\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ====================================\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "# ====================================\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ====================================\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors = np.where(mask_neighbors)[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "# ====================================\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ====================================\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "# ====================================\n",
    "# 4) Generar Semillas BFS\n",
    "# ====================================\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=0,\n",
    "    theta_max=450,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "# ====================================\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ====================================\n",
    "def calculate_pa(slope):\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return None, None, None, None, None, None, None\n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    slope_ = model.coef_[0]\n",
    "    intercept_ = model.intercept_\n",
    "    pa_ = calculate_pa(slope_)\n",
    "    t_min = df_cluster['theta'].min()\n",
    "    t_max = df_cluster['theta'].max()\n",
    "    r_min = df_cluster['r'].min()\n",
    "    r_max = df_cluster['r'].max()\n",
    "    return slope_, intercept_, pa_, t_min, t_max, r_min, r_max\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.40,\n",
    "    bounding_extrap=0.30\n",
    "):\n",
    "    groups = []\n",
    "    for clust_df in bfs_clusters:\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(clust_df)\n",
    "        grp = {\n",
    "            'points': clust_df.copy(),\n",
    "            'slope': slope_,\n",
    "            'intercept': intercept_,\n",
    "            'pa': pa_,\n",
    "            'theta_min': tmin,\n",
    "            'theta_max': tmax,\n",
    "            'r_min': rmin,\n",
    "            'r_max': rmax\n",
    "        }\n",
    "        groups.append(grp)\n",
    "    \n",
    "    def recalc_properties(group):\n",
    "        dfp = group['points']\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(dfp)\n",
    "        group['slope'] = slope_\n",
    "        group['intercept'] = intercept_\n",
    "        group['pa'] = pa_\n",
    "        group['theta_min'] = tmin\n",
    "        group['theta_max'] = tmax\n",
    "        group['r_min'] = rmin\n",
    "        group['r_max'] = rmax\n",
    "\n",
    "    def boxes_overlap(g1, g2, extrap=0.30):\n",
    "        tmin1, tmax1 = g1['theta_min'], g1['theta_max']\n",
    "        rmin1, rmax1 = g1['r_min'], g1['r_max']\n",
    "        tmin2, tmax2 = g2['theta_min'], g2['theta_max']\n",
    "        rmin2, rmax2 = g2['r_min'], g2['r_max']\n",
    "        if tmin1 is None or tmin2 is None:\n",
    "            return False\n",
    "        dt1 = (tmax1 - tmin1)\n",
    "        dr1 = (rmax1 - rmin1)\n",
    "        dt2 = (tmax2 - tmin2)\n",
    "        dr2 = (rmax2 - rmin2)\n",
    "        extr_dt1 = dt1*extrap\n",
    "        extr_dr1 = dr1*extrap\n",
    "        extr_dt2 = dt2*extrap\n",
    "        extr_dr2 = dr2*extrap\n",
    "        \n",
    "        tmin1e = tmin1 - extr_dt1\n",
    "        tmax1e = tmax1 + extr_dt1\n",
    "        rmin1e = rmin1 - extr_dr1\n",
    "        rmax1e = rmax1 + extr_dr1\n",
    "        \n",
    "        tmin2e = tmin2 - extr_dt2\n",
    "        tmax2e = tmax2 + extr_dt2\n",
    "        rmin2e = rmin2 - extr_dr2\n",
    "        rmax2e = rmax2 + extr_dr2\n",
    "        \n",
    "        overlap_theta = not (tmax1e < tmin2e or tmax2e < tmin1e)\n",
    "        overlap_r = not (rmax1e < rmin2e or rmax2e < rmin1e)\n",
    "        return overlap_theta and overlap_r\n",
    "    \n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        new_groups = []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1 = groups[i]\n",
    "            j = i+1\n",
    "            fused_any = False\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    combined = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    slope_, intercept_, _, _, _, _, _ = fit_line_to_cluster(combined)\n",
    "                    if slope_ is not None:\n",
    "                        var1 = abs(slope_ - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        var2 = abs(slope_ - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if var1 < slope_variation_threshold and var2 < slope_variation_threshold:\n",
    "                            g1['points'] = combined\n",
    "                            recalc_properties(g1)\n",
    "                            groups.pop(j)\n",
    "                            fused_any = True\n",
    "                            merged_flag = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new_groups.append(g1)\n",
    "            i += 1\n",
    "        groups = new_groups\n",
    "    \n",
    "    final_groups = []\n",
    "    for gg in groups:\n",
    "        if len(gg['points']) >= 2:\n",
    "            final_groups.append(gg)\n",
    "    return final_groups\n",
    "\n",
    "# ====================================\n",
    "# 6) Validar dispersión y re-procesar (opcional)\n",
    "# ====================================\n",
    "def validate_dispersion_and_reprocess(\n",
    "    groups,\n",
    "    dispersion_threshold=2.0,   # umbral de desviación std (ejemplo)\n",
    "    reproc_theta_diff=2.0,      # BFS para reprocesar\n",
    "    reproc_r_diff=0.5\n",
    "):\n",
    "    groups_ok = []\n",
    "    groups_toreprocess = []\n",
    "    \n",
    "    for grp in groups:\n",
    "        slope = grp['slope']\n",
    "        intercept = grp['intercept']\n",
    "        pts = grp['points']\n",
    "        if slope is None or len(pts) < 2:\n",
    "            groups_ok.append(grp)\n",
    "            continue\n",
    "        \n",
    "        predicted_r = slope*pts['theta'] + intercept\n",
    "        residuals = pts['r'] - predicted_r\n",
    "        std_resid = residuals.std()\n",
    "        if std_resid > dispersion_threshold:\n",
    "            groups_toreprocess.append(grp)\n",
    "        else:\n",
    "            groups_ok.append(grp)\n",
    "    \n",
    "    new_groups = []\n",
    "    for grp in groups_toreprocess:\n",
    "        subdf = grp['points'].copy()\n",
    "        subdf.reset_index(drop=True, inplace=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        clusters_sub = bfs_components(graph, subdf)\n",
    "        \n",
    "        for csub in clusters_sub:\n",
    "            slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(csub)\n",
    "            new_groups.append({\n",
    "                'points': csub,\n",
    "                'slope': slope_,\n",
    "                'intercept': intercept_,\n",
    "                'pa': pa_,\n",
    "                'theta_min': tmin,\n",
    "                'theta_max': tmax,\n",
    "                'r_min': rmin,\n",
    "                'r_max': rmax\n",
    "            })\n",
    "    \n",
    "    final_res = groups_ok + new_groups\n",
    "    return final_res\n",
    "\n",
    "# =========================================\n",
    "# 7) Nueva función: Graficar grupos >= 60 puntos con la recta extrapolada un 30%\n",
    "# =========================================\n",
    "def plot_final_segments_over_60_with_line_extrapolation(groups, df_all, line_extrap=0.3):\n",
    "    \"\"\"\n",
    "    - Filtra grupos con >= 60 puntos\n",
    "    - Grafica cada uno en un nuevo plot (theta vs r)\n",
    "    - En lugar de dibujar bounding box, extiende la recta un 'line_extrap'% \n",
    "      por debajo y por encima del rango de theta.\n",
    "    \"\"\"\n",
    "    # 1) Filtrar los grupos grandes\n",
    "    large_groups = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos con >= 60 puntos: {len(large_groups)}\")\n",
    "\n",
    "    # 2) Crear figura\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Fondo con df_all en gris\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "\n",
    "    for i, grp in enumerate(large_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = grp['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"Grupo_{i+1} ({len(pts)} pts)\")\n",
    "\n",
    "        slope_ = grp['slope']\n",
    "        intercept_ = grp['intercept']\n",
    "        if slope_ is not None and grp['theta_min'] is not None and grp['theta_max'] is not None:\n",
    "            # rango de theta del grupo\n",
    "            theta_min = grp['theta_min']\n",
    "            theta_max = grp['theta_max']\n",
    "            dtheta = (theta_max - theta_min)\n",
    "            # Extender un 30%\n",
    "            theta_min_ext = theta_min - line_extrap*dtheta\n",
    "            theta_max_ext = theta_max + line_extrap*dtheta\n",
    "\n",
    "            # Graficar la línea en el rango extendido\n",
    "            t_lin = np.linspace(theta_min_ext, theta_max_ext, 200)\n",
    "            r_lin = slope_*t_lin + intercept_\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "            plt.plot(t_lin, r_lin, '-.', color='k', alpha=0.3)\n",
    "\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Grupos >= 60 puntos con recta extrapolada {int(line_extrap*100)}%\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =========================================\n",
    "# 8) Función principal: ejemplo\n",
    "# =========================================\n",
    "def main_example():\n",
    "    # A) Generar BFS\n",
    "    bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0,\n",
    "        theta_max=460,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=2.0,\n",
    "        gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs_clusters)} grupos\")\n",
    "\n",
    "    # B) Ajustar y fusionar\n",
    "    merged_groups = adjust_and_merge_seeds(\n",
    "        bfs_clusters,\n",
    "        slope_variation_threshold=0.20,\n",
    "        bounding_extrap=0.35\n",
    "    )\n",
    "    print(f\"Grupos tras fusión inicial: {len(merged_groups)}\")\n",
    "\n",
    "    # C) Validar dispersión y re-procesar\n",
    "    validated_groups = validate_dispersion_and_reprocess(\n",
    "        merged_groups,\n",
    "        dispersion_threshold=1.30,\n",
    "        reproc_theta_diff=1.05,\n",
    "        reproc_r_diff=0.4\n",
    "    )\n",
    "    print(f\"Grupos tras validación dispersión: {len(validated_groups)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # D) Grafico global (todos los grupos) en (theta, r)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_filtrado['theta'], df_filtrado['r'], s=3, alpha=0.3, label='Datos BFS')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, group in enumerate(validated_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = group['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)}pts)\")\n",
    "        if group['slope'] is not None:\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 100)\n",
    "            r_lin = group['slope']*t_lin + group['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    #plt.legend(loc='best')\n",
    "    plt.title(\"Grupos finales (validados)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # E) Ahora, filtrar los grupos con >=60 puntos y graficar su recta extrapolada\n",
    "    plot_final_segments_over_60_with_line_extrapolation(\n",
    "        validated_groups,\n",
    "        df_filtrado,\n",
    "        line_extrap=0.15  # 30% de extrapolación\n",
    "    )\n",
    "\n",
    "\n",
    "    def plot_final_segments_over_60_polar(groups, df_all, line_extrap=0.15):\n",
    "        \"\"\"\n",
    "        Grafica en coordenadas polares (r, theta):\n",
    "        - Filtra los grupos con >= 60 puntos.\n",
    "        - Para cada grupo, crea una curva r = slope*theta_deg + intercept,\n",
    "        donde theta_deg se extrapola un 'line_extrap'% a cada lado\n",
    "        y se convierte a radianes para dibujar.\n",
    "\n",
    "        Param:\n",
    "        groups: lista de dicts con campos:\n",
    "                'points' (DataFrame con 'theta','r'),\n",
    "                'slope','intercept','theta_min','theta_max', ...\n",
    "        df_all: DataFrame con las columnas 'theta','r' (theta en grados).\n",
    "        line_extrap: factor de extrapolación (e.g. 0.3 = 30%)\n",
    "        \"\"\"\n",
    "        # 1) Filtrar grupos con >=60 puntos\n",
    "        large_groups = [g for g in groups if len(g['points']) >= 60]\n",
    "        print(f\"Grupos con >= 60 puntos: {len(large_groups)}\")\n",
    "\n",
    "        # 2) Crear figura con proyección polar\n",
    "        fig = plt.figure(figsize=(9, 8))\n",
    "        ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "        # 3) Fondo con df_all en gris (convirtiendo theta a radianes)\n",
    "        theta_all_rad = np.radians(df_all['theta'].values)\n",
    "        r_all = df_all['r'].values\n",
    "        ax.scatter(theta_all_rad, r_all, s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "        # 4) Graficar cada grupo\n",
    "        colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "        for i, grp in enumerate(large_groups):\n",
    "            c = colors[i % len(colors)]\n",
    "            pts = grp['points']\n",
    "\n",
    "            # a) scatter de puntos del grupo (convertir theta->rad)\n",
    "            theta_pts_rad = np.radians(pts['theta'].values)\n",
    "            r_pts = pts['r'].values\n",
    "            label_txt = f\"Grupo_{i+1} ({len(pts)} pts)\"\n",
    "            ax.scatter(theta_pts_rad, r_pts, s=10, color=c, label=label_txt)\n",
    "\n",
    "            # b) línea extrapolada\n",
    "            slope_ = grp['slope']\n",
    "            intercept_ = grp['intercept']\n",
    "            tmin_deg = grp.get('theta_min', None)\n",
    "            tmax_deg = grp.get('theta_max', None)\n",
    "            if slope_ is not None and tmin_deg is not None and tmax_deg is not None:\n",
    "                dt = tmax_deg - tmin_deg\n",
    "                tmin_ext = tmin_deg - line_extrap*dt\n",
    "                tmax_ext = tmax_deg + line_extrap*dt\n",
    "\n",
    "                # Generamos 200 puntos en grados\n",
    "                theta_deg_line = np.linspace(tmin_ext, tmax_ext, 200)\n",
    "                # Calculamos r en base a slope*(theta_deg) + intercept\n",
    "                r_line = slope_*theta_deg_line + intercept_\n",
    "                # Convertimos a radianes para plot\n",
    "                theta_rad_line = np.radians(theta_deg_line)\n",
    "\n",
    "                ax.plot(theta_rad_line, r_line, '--', color=c)\n",
    "\n",
    "        # 5) Ajustes finales en polar\n",
    "        ax.set_theta_zero_location(\"E\")   # opcional: 0° a la derecha (eje x)\n",
    "        ax.set_theta_direction(-1)        # opcional: ángulos crecen en sentido horario\n",
    "        ax.set_rlabel_position(0)         # etiquetas de r en 0 grados\n",
    "        ax.set_title(\"Grupos >= 60 puntos (coordenadas polares, 30% extrapolación)\", y=1.05)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "        plt.show()\n",
    "\n",
    "    plot_final_segments_over_60_polar(\n",
    "    validated_groups,\n",
    "    df_filtrado,\n",
    "    line_extrap=0.15  # 30% de extrapolación\n",
    ")\n",
    "\n",
    "main_example()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0875b8-366b-4094-b95c-b4529efc4875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Algoritmo mejorado para agrupar partículas (θ, r) y ajustar segmentos\n",
    "lineales.  Incluye validación adaptativa cuando el dataset > 2000 puntos.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r']   = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr     = df_points['r'].values\n",
    "    n         = len(df_points)\n",
    "\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr     = np.abs(r_arr[i]     - r_arr)\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors      = np.where(mask_neighbors)[0]\n",
    "        graph[i]       = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n        = len(graph)\n",
    "    visited  = [False]*n\n",
    "    clusters = []\n",
    "\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue        = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "\n",
    "            clusters.append(df_points.iloc[comp_indices].copy())\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)\n",
    "    arr   = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 4) Generar Semillas BFS  (con validación adaptativa > 2000 pts)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def _adaptive_factor(n_pts, reference=2000):\n",
    "    \"\"\"\n",
    "    Devuelve √(n_pts / reference).  Si n_pts <= reference, devuelve 1.\n",
    "    \"\"\"\n",
    "    return np.sqrt(max(n_pts / reference, 1.0))\n",
    "\n",
    "\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    # 4.1) Cargar y filtrar por rango angular\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "\n",
    "    # 4.2) Filtrar por densidad (rho_resta_final_exp > percentil)\n",
    "    threshold    = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered  = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # ─── NEW: Validación adaptativa para datasets grandes ───────────────\n",
    "    n_pts = len(df_filtered)\n",
    "    adapt = _adaptive_factor(n_pts)   # = 1 si n_pts ≤ 2000\n",
    "\n",
    "    theta_diff_eff        = theta_diff        / adapt\n",
    "    r_diff_eff            = r_diff            / adapt\n",
    "    gap_threshold_theta_e = gap_threshold_theta / adapt\n",
    "    gap_threshold_r_e     = gap_threshold_r     / adapt\n",
    "\n",
    "    # 4.3) Construir grafo y obtener componentes\n",
    "    graph, _   = build_graph_rectangular(df_filtered, theta_diff_eff, r_diff_eff)\n",
    "    clusters   = bfs_components(graph, df_filtered)\n",
    "\n",
    "    # 4.4) Subdividir cada componente por \"gaps\"\n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta_e, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r_e, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "\n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def calculate_pa(slope):\n",
    "    return None if slope is None else np.degrees(np.arctan(slope))\n",
    "\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "\n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    model = LinearRegression().fit(X, y)\n",
    "\n",
    "    slope_     = model.coef_[0]\n",
    "    intercept_ = model.intercept_\n",
    "    pa_        = calculate_pa(slope_)\n",
    "    t_min      = df_cluster['theta'].min()\n",
    "    t_max      = df_cluster['theta'].max()\n",
    "    r_min      = df_cluster['r'].min()\n",
    "    r_max      = df_cluster['r'].max()\n",
    "    return slope_, intercept_, pa_, t_min, t_max, r_min, r_max\n",
    "\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.40,\n",
    "    bounding_extrap=0.30\n",
    "):\n",
    "    # 5.1) Convertir cada cluster en dict con propiedades\n",
    "    groups = []\n",
    "    for clust_df in bfs_clusters:\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(clust_df)\n",
    "        groups.append({\n",
    "            'points'     : clust_df.copy(),\n",
    "            'slope'      : slope_,\n",
    "            'intercept'  : intercept_,\n",
    "            'pa'         : pa_,\n",
    "            'theta_min'  : tmin,\n",
    "            'theta_max'  : tmax,\n",
    "            'r_min'      : rmin,\n",
    "            'r_max'      : rmax\n",
    "        })\n",
    "\n",
    "    # 5.2) Funciones auxiliares\n",
    "    def recalc(group):\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(group['points'])\n",
    "        group.update({\n",
    "            'slope'     : slope_,\n",
    "            'intercept' : intercept_,\n",
    "            'pa'        : pa_,\n",
    "            'theta_min' : tmin,\n",
    "            'theta_max' : tmax,\n",
    "            'r_min'     : rmin,\n",
    "            'r_max'     : rmax\n",
    "        })\n",
    "\n",
    "    def boxes_overlap(g1, g2, extrap=0.30):\n",
    "        # Expandir cajas en ambas dimensiones\n",
    "        def expand(tmin, tmax, rmin, rmax):\n",
    "            dt = (tmax - tmin) * extrap\n",
    "            dr = (rmax - rmin) * extrap\n",
    "            return tmin - dt, tmax + dt, rmin - dr, rmax + dr\n",
    "\n",
    "        t1min, t1max, r1min, r1max = expand(g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max'])\n",
    "        t2min, t2max, r2min, r2max = expand(g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max'])\n",
    "\n",
    "        overlap_theta = not (t1max < t2min or t2max < t1min)\n",
    "        overlap_r     = not (r1max < r2min or r2max < r1min)\n",
    "        return overlap_theta and overlap_r\n",
    "\n",
    "    # 5.3) Fusionar recursivamente\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged   = False\n",
    "        new_list = []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1 = groups[i]\n",
    "            j  = i + 1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb_pts = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    slope_c, *_ = fit_line_to_cluster(comb_pts)\n",
    "\n",
    "                    if slope_c is not None:\n",
    "                        var1 = abs(slope_c - g1['slope'])/(abs(g1['slope']) + 1e-12)\n",
    "                        var2 = abs(slope_c - g2['slope'])/(abs(g2['slope']) + 1e-12)\n",
    "                        if var1 < slope_variation_threshold and var2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb_pts\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new_list.append(g1)\n",
    "            i += 1\n",
    "        groups = new_list\n",
    "\n",
    "    # 5.4) Filtrar grupos muy pequeños\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 6) Validar dispersión y re-procesar (opcional)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def validate_dispersion_and_reprocess(\n",
    "    groups,\n",
    "    dispersion_threshold=2.0,\n",
    "    reproc_theta_diff=2.0,\n",
    "    reproc_r_diff=0.5\n",
    "):\n",
    "    ok_groups, to_reproc = [], []\n",
    "\n",
    "    for grp in groups:\n",
    "        if grp['slope'] is None or len(grp['points']) < 2:\n",
    "            ok_groups.append(grp)\n",
    "            continue\n",
    "\n",
    "        residuals = grp['points']['r'] - (grp['slope']*grp['points']['theta'] + grp['intercept'])\n",
    "        if residuals.std() > dispersion_threshold:\n",
    "            to_reproc.append(grp)\n",
    "        else:\n",
    "            ok_groups.append(grp)\n",
    "\n",
    "    # Reprocesar los grupos con alta dispersión\n",
    "    new_groups = []\n",
    "    for grp in to_reproc:\n",
    "        subdf = grp['points'].copy().reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for csub in bfs_components(graph, subdf):\n",
    "            slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(csub)\n",
    "            new_groups.append({\n",
    "                'points'     : csub,\n",
    "                'slope'      : slope_,\n",
    "                'intercept'  : intercept_,\n",
    "                'pa'         : pa_,\n",
    "                'theta_min'  : tmin,\n",
    "                'theta_max'  : tmax,\n",
    "                'r_min'      : rmin,\n",
    "                'r_max'      : rmax\n",
    "            })\n",
    "\n",
    "    return ok_groups + new_groups\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7) Graficar grupos ≥ 60 puntos con recta extrapolada\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_final_segments_over_60_with_line_extrapolation(groups, df_all, line_extrap=0.3):\n",
    "    large_groups = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos con ≥60 puntos: {len(large_groups)}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, grp in enumerate(large_groups):\n",
    "        c   = colors[i % len(colors)]\n",
    "        pts = grp['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)})\")\n",
    "\n",
    "        if grp['slope'] is not None:\n",
    "            dtheta = grp['theta_max'] - grp['theta_min']\n",
    "            t_min  = grp['theta_min'] - line_extrap*dtheta\n",
    "            t_max  = grp['theta_max'] + line_extrap*dtheta\n",
    "            t_lin  = np.linspace(t_min, t_max, 200)\n",
    "            r_lin  = grp['slope']*t_lin + grp['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(f\"Grupos ≥60 puntos – recta extrapolada {int(line_extrap*100)}%\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 8) Ejemplo principal\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def main_example():\n",
    "    # A) Generar BFS + subdivisión (ahora adaptativo)\n",
    "    bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0,\n",
    "        theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=2.0,\n",
    "        gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs_clusters)} clusters\")\n",
    "\n",
    "    # B) Fusionar\n",
    "    merged_groups = adjust_and_merge_seeds(\n",
    "        bfs_clusters,\n",
    "        slope_variation_threshold=0.30,\n",
    "        bounding_extrap=0.35\n",
    "    )\n",
    "    print(f\"Tras fusión: {len(merged_groups)} grupos\")\n",
    "\n",
    "    # C) Validar dispersión\n",
    "    validated_groups = validate_dispersion_and_reprocess(\n",
    "        merged_groups,\n",
    "        dispersion_threshold=1.80,\n",
    "        reproc_theta_diff=1.25,\n",
    "        reproc_r_diff=0.4\n",
    "    )\n",
    "    print(f\"Tras validación: {len(validated_groups)} grupos\")\n",
    "\n",
    "    # D) Gráfico global\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_filtrado['theta'], df_filtrado['r'], s=3, alpha=0.3, label='Datos filtrados')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, g in enumerate(validated_groups):\n",
    "        c   = colors[i % len(colors)]\n",
    "        pts = g['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)})\")\n",
    "        if g['slope'] is not None:\n",
    "            t_lin = np.linspace(pts['theta'].min(), pts['theta'].max(), 100)\n",
    "            r_lin = g['slope']*t_lin + g['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Grupos finales (validados)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # E) Grupos grandes con recta extrapolada\n",
    "    plot_final_segments_over_60_with_line_extrapolation(validated_groups, df_filtrado, line_extrap=0.15)\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 9) Llamada a main\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "if __name__ == \"__main__\":\n",
    "    main_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559d34e0-9dc8-4ff2-bc09-9dfb5cb3ec55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 0)  PITCH‑ANGLE \n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    PA = arctan[(slope·180/π) / intercept]  en grados, si intercept ≠ 0.\n",
    "    Devuelve NaN si intercept == 0.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r']     = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r']     = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    col = 'theta' if mode == 'theta' else 'r'\n",
    "    df  = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 4) Generar Semillas BFS  (validación adaptativa)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50, theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0, r_diff=0.5,\n",
    "    gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "\n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa   = calculate_pa(s, b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.40, bounding_extrap=0.30):\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "\n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0,t1,r0,r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'],g1['theta_max'],g1['r_min'],g1['r_max'])\n",
    "        b = exp(g2['theta_min'],g2['theta_max'],g2['r_min'],g2['r_max'])\n",
    "        return not (a[1]<b[0] or b[1]<a[0]) and not (a[3]<b[2] or b[3]<a[2])\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1; continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb-g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb-g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb; recalc(g1)\n",
    "                            groups.pop(j); merged = True; continue\n",
    "                j += 1\n",
    "            new.append(g1); i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 6) Validar dispersión y re‑procesar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g); continue\n",
    "        res = g['points']['r'] - (g['slope']*g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "\n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph,_ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑A) Plot cartesiano grupos grandes (PA visible)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i%len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            plt.plot(t, g['slope']*t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\"); plt.ylabel(\"r\"); plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – grupos ≥60 pts (cartesiano, PA visible)\")\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑B) Plot POLAR grupos grandes (PA visible)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax  = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i%len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c,\n",
    "                   label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            r_line = g['slope']*t_deg + g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – proyección polar (PA visible)\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20,1.0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 8) MAIN – pipeline completo subhalo 17\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def main_example():\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "\n",
    "    # ─── Visualizaciones ────────────────────────────────────────────────\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 9) Run\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "if __name__ == \"__main__\":\n",
    "    main_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "916889c1-de3f-4e2a-948f-6d76d42e44b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## serpentiando entre puntos ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d8a76d-df3a-4d73-9bf6-00e83333363d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 0)  PITCH‑ANGLE \n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    PA = arctan[(slope·180/π) / intercept]  en grados, si intercept ≠ 0.\n",
    "    Devuelve NaN si intercept == 0.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    # Agregar columna de id para preservar el índice original\n",
    "    df['id'] = df.index\n",
    "    df['r']     = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r']     = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    col = 'theta' if mode == 'theta' else 'r'\n",
    "    df  = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 4) Generar Semillas BFS  (validación adaptativa)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50, theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0, r_diff=0.5,\n",
    "    gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "\n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa   = calculate_pa(s, b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.40, bounding_extrap=0.30):\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "\n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0,t1,r0,r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'],g1['theta_max'],g1['r_min'],g1['r_max'])\n",
    "        b = exp(g2['theta_min'],g2['theta_max'],g2['r_min'],g2['r_max'])\n",
    "        return not (a[1]<b[0] or b[1]<a[0]) and not (a[3]<b[2] or b[3]<a[2])\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new.append(g1)\n",
    "            i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 6) Validar dispersión y re‑procesar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g)\n",
    "            continue\n",
    "        res = g['points']['r'] - (g['slope'] * g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "\n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑A) Plot cartesiano grupos grandes (PA visible)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            plt.plot(t, g['slope'] * t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – grupos ≥60 pts (cartesiano, PA visible)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑B) Plot POLAR grupos grandes (PA visible)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax  = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c,\n",
    "                   label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            r_line = g['slope'] * t_deg + g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – proyección polar (PA visible)\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20, 1.0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 8) MAIN – pipeline completo subhalo 17 (modificado para almacenar grupos y background)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def main_example_modificado():\n",
    "    # Genera las semillas BFS y carga los datos filtrados\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "\n",
    "    # Visualizaciones (si se desea mantener)\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "\n",
    "    # Almacenar grupos finales: solo aquellos con 60 o más puntos\n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "\n",
    "    # Identificar los IDs de los puntos que pertenecen a los grupos finales.\n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        # Se espera que la columna \"id\" esté en los datos originales\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "\n",
    "    # Los puntos \"background\" son aquellos de df_f que no pertenecen a ningún grupo final.\n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "\n",
    "    # Se puede devolver un diccionario con los dos conjuntos de información:\n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 9) Run\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "if __name__ == \"__main__\":\n",
    "    datos_finales = main_example_modificado()\n",
    "    # Ahora, datos_finales['grupos_finales'] contiene los grupos con ≥ 60 pts, \n",
    "    # y datos_finales['background'] contiene la información original de los puntos\n",
    "    # que no fueron asignados a ningún grupo final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb967cc-74be-45e9-a6de-f0bba7dcf65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grupos_finales = datos_finales['grupos_finales']\n",
    "grupos_finales[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f3cce91-d418-412d-bd9d-9d9f77d5bff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segundo paso: Trazado de un camino continuo (de derecha a izquierda) en el background,\n",
    "utilizando la información de las fronteras de los grupos finales.\n",
    "\n",
    "La idea es:\n",
    "    1. Iniciar en el punto con mayor x (más a la derecha) del background.\n",
    "    2. De entre los puntos restantes, sólo considerar aquellos cuyo valor de x es menor que el actual (para asegurar\n",
    "       el recorrido de derecha a izquierda).\n",
    "    3. Seleccionar el candidato más cercano (mínima distancia Euclidiana) entre los que cumplan esa restricción.\n",
    "    4. Evaluar si el candidato está “muy cerca” de la frontera de algún grupo final (distancia < umbral, basada en la\n",
    "       distancia promedio entre puntos vecinos del background). Si es así, se entiende que se ha tocado la frontera y\n",
    "       se finaliza el brazo.\n",
    "    5. En caso contrario, se añade el candidato al camino y se actualiza el punto actual para continuar el recorrido.\n",
    "    6. Se repite hasta que no haya candidatos (o la distancia es mayor al umbral) y se obtiene un brazo espiral.\n",
    "\n",
    "El resultado se grafica en el espacio (θ, r).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Función para convertir de coordenadas polares a cartesianas\n",
    "def polar_to_cartesian(df):\n",
    "    df = df.copy()\n",
    "    rad = np.radians(df['theta'].values)\n",
    "    df['x'] = df['r'] * np.cos(rad)\n",
    "    df['y'] = df['r'] * np.sin(rad)\n",
    "    return df\n",
    "\n",
    "# Función para calcular la distancia promedio al vecino más cercano de un conjunto de puntos\n",
    "def compute_avg_nn_distance(points):\n",
    "    if len(points) < 2:\n",
    "        return np.inf\n",
    "    dists = cdist(points, points)\n",
    "    np.fill_diagonal(dists, np.inf)\n",
    "    return np.mean(np.min(dists, axis=1))\n",
    "\n",
    "# Función para trazar el brazo espiral a partir del background y tomando en cuenta las fronteras de los grupos finales\n",
    "def trace_spiral_arm(background, grupos_finales, tol_factor=1.1):\n",
    "    \"\"\"\n",
    "    background: DataFrame de puntos en coordenadas polares que no pertenecen a ningún grupo final.\n",
    "    grupos_finales: lista de diccionarios (con clave 'points') que contienen los grupos finales ya calculados.\n",
    "                  Se asume que la \"frontera\" se obtiene tomando (por simplicidad) todos los puntos de dichos grupos.\n",
    "    tol_factor: factor de tolerancia para definir el umbral de conexión.\n",
    "    \n",
    "    Retorna un DataFrame con el camino (ordenado) que representa el brazo espiral.\n",
    "    \"\"\"\n",
    "    # Convertir los puntos del background a coordenadas cartesianas.\n",
    "    bg_cart = polar_to_cartesian(background)\n",
    "    pts_bg = bg_cart[['x', 'y']].values\n",
    "\n",
    "    # Preparar los puntos frontera a partir de los grupos finales.\n",
    "    # Aquí se toma como frontera de cada grupo todos sus puntos, pues se entiende que\n",
    "    # el interior ya está definido y no se requiere recorrerlo nuevamente.\n",
    "    boundary_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts = grupo['points']\n",
    "        pts_cart = polar_to_cartesian(pts)\n",
    "        boundary_list.append(pts_cart[['x', 'y']].values)\n",
    "    if boundary_list:\n",
    "        boundary_all = np.vstack(boundary_list)\n",
    "    else:\n",
    "        boundary_all = np.empty((0, 2))\n",
    "    \n",
    "    # Calcular un umbral base usando la distancia promedio entre vecinos en el background.\n",
    "    avg_nn_bg = compute_avg_nn_distance(pts_bg)\n",
    "    threshold = tol_factor * avg_nn_bg\n",
    "\n",
    "    # Preparar el recorrido:\n",
    "    # Partir del punto con el mayor x (más a la derecha).\n",
    "    remaining_indices = list(range(len(pts_bg)))\n",
    "    start_idx = max(remaining_indices, key=lambda i: pts_bg[i, 0])\n",
    "    spiral_indices = [start_idx]\n",
    "    current_idx = start_idx\n",
    "    remaining_indices.remove(start_idx)\n",
    "    \n",
    "    # Búsqueda greedy:\n",
    "    while remaining_indices:\n",
    "        current_pt = pts_bg[current_idx].reshape(1,2)\n",
    "        # Solo se consideran candidatos con x menor que la del punto actual para mantener la dirección (derecha a izquierda)\n",
    "        valid_candidates = [i for i in remaining_indices if pts_bg[i,0] < pts_bg[current_idx,0]]\n",
    "        if not valid_candidates:\n",
    "            break\n",
    "        candidates = pts_bg[valid_candidates]\n",
    "        dists = np.linalg.norm(candidates - current_pt, axis=1)\n",
    "        min_dist = np.min(dists)\n",
    "        candidate_local_idx = np.argmin(dists)\n",
    "        candidate_idx = valid_candidates[candidate_local_idx]\n",
    "\n",
    "        # Verificar conexión con la frontera de algún grupo final:\n",
    "        if boundary_all.shape[0] > 0:\n",
    "            dist_to_boundary = np.min(np.linalg.norm(boundary_all - pts_bg[candidate_idx], axis=1))\n",
    "        else:\n",
    "            dist_to_boundary = np.inf\n",
    "\n",
    "        # Si la distancia al candidato o la distancia a la frontera es menor al umbral,\n",
    "        # se asume que se toca la frontera y se finaliza el recorrido (se añade el candidato y se cierra el brazo).\n",
    "        if min_dist <= threshold or dist_to_boundary <= threshold:\n",
    "            spiral_indices.append(candidate_idx)\n",
    "            # Una vez que se alcanza la frontera, se finaliza el brazo espiral.\n",
    "            break\n",
    "        else:\n",
    "            # Se añade el candidato al camino y se actualiza el punto actual.\n",
    "            spiral_indices.append(candidate_idx)\n",
    "            current_idx = candidate_idx\n",
    "            remaining_indices.remove(candidate_idx)\n",
    "\n",
    "    # Extraer el conjunto de puntos (en orden) que conforman el brazo espiral.\n",
    "    spiral_arm = bg_cart.iloc[spiral_indices].copy()\n",
    "    return spiral_arm\n",
    "\n",
    "# Función para graficar en (θ, r) el brazo espiral obtenido.\n",
    "def plot_spiral_arm(spiral_arm, title=\"Brazo Espiral (θ, r)\"):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(spiral_arm['theta'], spiral_arm['r'], color='blue', s=20, label=\"Puntos del brazo\")\n",
    "    plt.plot(spiral_arm['theta'], spiral_arm['r'], '-o', color='red')\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Ejemplo de uso en la pipeline:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "\n",
    "    if not background.empty and len(grupos_finales) > 0:\n",
    "        brazo_espiral = trace_spiral_arm(background, grupos_finales, tol_factor=1.1)\n",
    "        print(f\"Brazo espiral trazado con {len(brazo_espiral)} puntos.\")\n",
    "        plot_spiral_arm(brazo_espiral)\n",
    "    else:\n",
    "        print(\"No hay datos suficientes en el background o en los grupos finales para trazar el brazo espiral.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45cbd883-a281-4cae-ad44-cd8e600eb687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Prototipo para el segundo paso: trazado de un brazo espiral a partir del background.\n",
    "La idea es recorrer de derecha a izquierda (según coordenada x) de manera greedy; \n",
    "en cada iteración se evalúa si el candidato toca la frontera de algún grupo final. \n",
    "Si ocurre, se añade ese candidato y se lanza una búsqueda complementaria sobre el extremo\n",
    "opuesto para verificar la conexión con otro grupo.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# --- Funciones auxiliares ---\n",
    "\n",
    "def polar_to_cartesian(df):\n",
    "    \"\"\"Convierte de coordenadas polares (theta, r) a cartesianas (x, y).\"\"\"\n",
    "    df = df.copy()\n",
    "    rad = np.radians(df['theta'].values)\n",
    "    df['x'] = df['r'] * np.cos(rad)\n",
    "    df['y'] = df['r'] * np.sin(rad)\n",
    "    return df\n",
    "\n",
    "def compute_avg_nn_distance(points):\n",
    "    \"\"\"\n",
    "    Calcula la distancia promedio al vecino más cercano.\n",
    "    points: arreglo de forma (n,2)\n",
    "    \"\"\"\n",
    "    if len(points) < 2:\n",
    "        return np.inf\n",
    "    dists = cdist(points, points)\n",
    "    np.fill_diagonal(dists, np.inf)\n",
    "    return np.mean(np.min(dists, axis=1))\n",
    "\n",
    "def get_boundary_points(grupos_finales):\n",
    "    \"\"\"\n",
    "    Dado que ya se tienen los grupos finales, se extrae el conjunto de puntos que\n",
    "    se usan para representar la frontera. En este ejemplo se toman todos los puntos.\n",
    "    \"\"\"\n",
    "    boundary_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts_cart = polar_to_cartesian(grupo['points'])\n",
    "        boundary_list.append(pts_cart[['x','y']].values)\n",
    "    if boundary_list:\n",
    "        return np.vstack(boundary_list)\n",
    "    return np.empty((0,2))\n",
    "\n",
    "# --- Funciones para el trazado del brazo espiral ---\n",
    "\n",
    "def trace_spiral_arm(background, grupos_finales, tol_factor=1.1):\n",
    "    \"\"\"\n",
    "    Recorre los puntos del background de derecha a izquierda en forma greedy.\n",
    "    Para cada candidato se comprueba si toca la frontera de algún grupo final,\n",
    "    usando como umbral tol_factor * (distancia promedio entre vecinos del background).\n",
    "    \n",
    "    Si se detecta conexión, se añade el punto y se evalúa el extremo opuesto.\n",
    "    \n",
    "    Retorna un diccionario con:\n",
    "      - 'brazo': DataFrame con el camino trazado (ordenado)\n",
    "      - 'conexiones': información sobre las conexiones con grupos finales (extremos)\n",
    "    \"\"\"\n",
    "    # Convertir background a coordenadas cartesianas.\n",
    "    bg_cart = polar_to_cartesian(background)\n",
    "    pts_bg = bg_cart[['x','y']].values\n",
    "\n",
    "    # Obtener el conjunto de puntos frontera de los grupos finales.\n",
    "    boundary_all = get_boundary_points(grupos_finales)\n",
    "    \n",
    "    # Definir el umbral base usando la distancia promedio de vecinos en el background.\n",
    "    avg_nn = compute_avg_nn_distance(pts_bg)\n",
    "    threshold = tol_factor * avg_nn\n",
    "\n",
    "    # Inicializar el recorrido: partir del punto con máximo x (más a la derecha).\n",
    "    remaining_indices = list(range(len(pts_bg)))\n",
    "    start_idx = max(remaining_indices, key=lambda i: pts_bg[i,0])\n",
    "    brazo_indices = [start_idx]\n",
    "    current_idx = start_idx\n",
    "    remaining_indices.remove(start_idx)\n",
    "\n",
    "    # Búsqueda greedy: recorrer background en dirección de menor x.\n",
    "    while remaining_indices:\n",
    "        current_pt = pts_bg[current_idx].reshape(1,2)\n",
    "        # Filtrar candidatos que tengan x menor al del punto actual.\n",
    "        valid_candidates = [i for i in remaining_indices if pts_bg[i,0] < pts_bg[current_idx,0]]\n",
    "        if not valid_candidates:\n",
    "            break\n",
    "        candidates = pts_bg[valid_candidates]\n",
    "        dists = np.linalg.norm(candidates - current_pt, axis=1)\n",
    "        min_dist = np.min(dists)\n",
    "        candidate_local_idx = np.argmin(dists)\n",
    "        candidate_idx = valid_candidates[candidate_local_idx]\n",
    "        \n",
    "        # Evaluar la distancia del candidato a la frontera de los grupos finales.\n",
    "        if boundary_all.shape[0] > 0:\n",
    "            dist_to_boundary = np.min(np.linalg.norm(boundary_all - pts_bg[candidate_idx], axis=1))\n",
    "        else:\n",
    "            dist_to_boundary = np.inf\n",
    "        \n",
    "        # Si el candidato cumple la condición (candidato muy cerca de la frontera)...\n",
    "        if min_dist <= threshold or dist_to_boundary <= threshold:\n",
    "            brazo_indices.append(candidate_idx)\n",
    "            # Se anota la conexión.\n",
    "            conexion = {\n",
    "                'punto_conexion': bg_cart.iloc[candidate_idx],\n",
    "                'distancia_a_frontera': dist_to_boundary\n",
    "            }\n",
    "            # Se evalúa el extremo opuesto: a partir de este candidato, se lanza un método \n",
    "            # complementario para ver con qué otro grupo se conecta o si se sigue el background.\n",
    "            brazo_opuesto = trace_opposite_extremity(candidate_idx, pts_bg, remaining_indices, boundary_all, threshold)\n",
    "            if brazo_opuesto is not None:\n",
    "                brazo_indices.extend(brazo_opuesto['indices'])\n",
    "                conexion['extremo_opuesto'] = brazo_opuesto['punto_final']\n",
    "            else:\n",
    "                conexion['extremo_opuesto'] = None\n",
    "            # Se finaliza el recorrido.\n",
    "            break\n",
    "        else:\n",
    "            # Si no se conecta, se añade el candidato y se continúa.\n",
    "            brazo_indices.append(candidate_idx)\n",
    "            current_idx = candidate_idx\n",
    "            remaining_indices.remove(candidate_idx)\n",
    "    \n",
    "    # Obtener el DataFrame final del brazo en el orden de recorrido.\n",
    "    brazo = bg_cart.iloc[brazo_indices].copy()\n",
    "    conexiones = None  # Aquí se podría incluir información adicional sobre las conexiones.\n",
    "    return {'brazo': brazo, 'conexiones': conexion if 'conexion' in locals() else None}\n",
    "\n",
    "def trace_opposite_extremity(current_idx, pts_bg, remaining_indices, boundary_all, threshold):\n",
    "    \"\"\"\n",
    "    A partir del punto actual (que acaba de tocar una frontera), se evalúa el \"extremo opuesto\"\n",
    "    buscando entre los puntos del background (de los que aún quedan) si alguno conecta con \n",
    "    la frontera de otro grupo final.\n",
    "    \n",
    "    Retorna:\n",
    "      - un diccionario con 'indices': la secuencia de índices del brazo opuesto,\n",
    "                     'punto_final': la última conexión encontrada.\n",
    "      - None si no se encontró conexión.\n",
    "    \"\"\"\n",
    "    brazo_indices = []\n",
    "    cur_idx = current_idx\n",
    "    # Se hará una búsqueda similar pero ahora se busca extender el brazo (por ejemplo, en la dirección que se aleja)\n",
    "    # De forma simplificada, se puede intentar usar la dirección contraria: candidatos cuyo x sean mayores.\n",
    "    valid_candidates = [i for i in remaining_indices if pts_bg[i,0] > pts_bg[cur_idx,0]]\n",
    "    if not valid_candidates:\n",
    "        return None\n",
    "    # Se selecciona el más cercano en esa dirección.\n",
    "    candidates = pts_bg[valid_candidates]\n",
    "    dists = np.linalg.norm(candidates - pts_bg[cur_idx].reshape(1,2), axis=1)\n",
    "    min_dist = np.min(dists)\n",
    "    candidate_local_idx = np.argmin(dists)\n",
    "    candidate_idx = valid_candidates[candidate_local_idx]\n",
    "    \n",
    "    # Verificar la conexión con la frontera.\n",
    "    if boundary_all.shape[0] > 0:\n",
    "        dist_to_boundary = np.min(np.linalg.norm(boundary_all - pts_bg[candidate_idx], axis=1))\n",
    "    else:\n",
    "        dist_to_boundary = np.inf\n",
    "    \n",
    "    if min_dist <= threshold or dist_to_boundary <= threshold:\n",
    "        brazo_indices.append(candidate_idx)\n",
    "        return {'indices': brazo_indices, 'punto_final': candidate_idx}\n",
    "    return None\n",
    "\n",
    "# --- Función para graficar el brazo espiral en (θ, r) ---\n",
    "def plot_spiral_arm(arm_dict, title=\"Brazo Espiral (θ, r)\"):\n",
    "    brazo = arm_dict['brazo']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(brazo['theta'], brazo['r'], color='blue', s=20, label=\"Puntos del brazo\")\n",
    "    plt.plot(brazo['theta'], brazo['r'], '-o', color='red', label=\"Camino trazado\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# --- Ejemplo de uso ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "\n",
    "    \n",
    "    if not background.empty and len(grupos_finales) > 0:\n",
    "        arm_result = trace_spiral_arm(background, grupos_finales, tol_factor=1.1)\n",
    "        print(f\"Brazo espiral trazado con {len(arm_result['brazo'])} puntos.\")\n",
    "        if arm_result['conexiones']:\n",
    "            print(\"Se estableció conexión con la frontera de un grupo final:\")\n",
    "            print(arm_result['conexiones'])\n",
    "        plot_spiral_arm(arm_result)\n",
    "    else:\n",
    "        print(\"No hay datos suficientes en el background o en los grupos finales para trazar el brazo espiral.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12033dd2-f80c-4d47-96ba-a4c676ec17ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Código completo para:\n",
    "  1. Procesar y agrupar los datos (primer paso).\n",
    "  2. Del background obtener un camino continuo (brazo espiral) mediante recorrido greedy,\n",
    "     evaluando la conexión con la frontera de los grupos finales.\n",
    "  3. Visualizar en el espacio (θ, r) el brazo espiral, superponiendo todos los puntos leídos y los\n",
    "     grupos finales con baja opacidad (alpha=0.1).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# =============================================================================\n",
    "# 0) Funciones básicas de la etapa de agrupación\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el pitch-angle (PA) en grados: PA = arctan[(slope*180/π) / intercept] si intercept≠0.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Carga el archivo CSV, calcula 'r' y 'theta', y filtra según el rango de theta.\n",
    "    Se añade una columna 'id' para preservar el índice original.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['id'] = df.index\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo de adyacencia (BFS) en el espacio (theta, r) considerando una tolerancia.\n",
    "    \"\"\"\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas usando BFS.\n",
    "    \"\"\"\n",
    "    visited, clusters = [False] * len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide un clúster en subgrupos si existen \"gaps\" (saltos en theta o r).\n",
    "    \"\"\"\n",
    "    col = 'theta' if mode == 'theta' else 'r'\n",
    "    df = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "def generate_bfs_seeds(id_halo, theta_min, theta_max,\n",
    "                         quartile_threshold=0.55,\n",
    "                         theta_diff=3.0, r_diff=0.5,\n",
    "                         gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "                         file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Genera las semillas BFS a partir de los datos filtrados.\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff / f, r_diff / f\n",
    "    gap_th, gap_r = gap_threshold_theta / f, gap_threshold_r / f\n",
    "\n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    \"\"\"\n",
    "    Ajusta una línea (regresión lineal) a los puntos del clúster.\n",
    "    \"\"\"\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,) * 7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa = calculate_pa(s, b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.40, bounding_extrap=0.30):\n",
    "    \"\"\"\n",
    "    Fusiona los clústeres que se solapan en la caja definida.\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope', 'intercept', 'pa', 'theta_min', 'theta_max', 'r_min', 'r_max', 'points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "\n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0, t1, r0, r1):\n",
    "            dt, dr = (t1 - t0) * e, (r1 - r0) * e\n",
    "            return t0 - dt, t1 + dt, r0 - dr, r1 + dr\n",
    "        a = exp(g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max'])\n",
    "        b = exp(g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max'])\n",
    "        return not (a[1] < b[0] or b[1] < a[0]) and not (a[3] < b[2] or b[3] < a[2])\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i + 1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb - g1['slope']) / (abs(g1['slope']) + 1e-12)\n",
    "                        v2 = abs(s_comb - g2['slope']) / (abs(g2['slope']) + 1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new.append(g1)\n",
    "            i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    \"\"\"\n",
    "    Valida la dispersión y re-procesa aquellos grupos que no cumplen el criterio.\n",
    "    \"\"\"\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g)\n",
    "            continue\n",
    "        res = g['points']['r'] - (g['slope'] * g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "    \n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope', 'intercept', 'pa', 'theta_min', 'theta_max', 'r_min', 'r_max', 'points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualiza en un gráfico cartesiano los grupos finales con ≥ 60 puntos (PA visible).\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            plt.plot(t, g['slope'] * t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – grupos ≥60 pts (cartesiano, PA visible)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualización en proyección polar de los grupos finales con ≥ 60 puntos.\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c,\n",
    "                   label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            r_line = g['slope'] * t_deg + g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – proyección polar (PA visible)\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20, 1.0))\n",
    "    plt.show()\n",
    "\n",
    "def main_example_modificado():\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline completo para subhalo 17.\n",
    "    Retorna un diccionario con:\n",
    "       - 'grupos_finales': lista de grupos (con ≥ 60 pts)\n",
    "       - 'background': DataFrame con los puntos que no están en ningún grupo final.\n",
    "    \"\"\"\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "    \n",
    "    # Almacenar grupos finales: únicamente aquellos con 60 o más puntos.\n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "    \n",
    "    # Determinar los puntos (IDs) que aparecen en los grupos finales.\n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "    \n",
    "    # Los puntos \"background\" son los que en df_f no están en ningún grupo final.\n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "    \n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Funciones para el trazado del brazo espiral a partir del background\n",
    "# =============================================================================\n",
    "\n",
    "def polar_to_cartesian_simple(df):\n",
    "    \"\"\"\n",
    "    Versión simplificada para convertir a cartesianas (usa 'theta' y 'r').\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    rad = np.radians(df['theta'].values)\n",
    "    df['x'] = df['r'] * np.cos(rad)\n",
    "    df['y'] = df['r'] * np.sin(rad)\n",
    "    return df\n",
    "\n",
    "def compute_avg_nn_distance(points):\n",
    "    if len(points) < 2:\n",
    "        return np.inf\n",
    "    dists = cdist(points, points)\n",
    "    np.fill_diagonal(dists, np.inf)\n",
    "    return np.mean(np.min(dists, axis=1))\n",
    "\n",
    "def get_boundary_points(grupos_finales):\n",
    "    \"\"\"\n",
    "    Extrae, de los grupos finales, el conjunto de puntos a considerar como frontera.\n",
    "    En este ejemplo se toman todos los puntos de cada grupo.\n",
    "    \"\"\"\n",
    "    boundary_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts_cart = polar_to_cartesian_simple(grupo['points'])\n",
    "        boundary_list.append(pts_cart[['x', 'y']].values)\n",
    "    if boundary_list:\n",
    "        return np.vstack(boundary_list)\n",
    "    return np.empty((0,2))\n",
    "\n",
    "def trace_spiral_arm(background, grupos_finales, tol_factor=1.1):\n",
    "    \"\"\"\n",
    "    Recorre los puntos del background de derecha a izquierda (por x) de forma greedy.\n",
    "    Para cada candidato, se evalúa si toca la frontera de algún grupo final usando\n",
    "    un umbral definido como tol_factor * (distancia promedio entre vecinos del background).\n",
    "    \n",
    "    Además, si se detecta conexión, se lanza una función complementaria para evaluar el extremo opuesto.\n",
    "    \n",
    "    Retorna un diccionario con:\n",
    "      - 'brazo': DataFrame con el camino (en orden) del brazo espiral.\n",
    "      - 'conexiones': información sobre la(s) conexión(es) detectada(s).\n",
    "    \"\"\"\n",
    "    bg_cart = polar_to_cartesian_simple(background)\n",
    "    pts_bg = bg_cart[['x', 'y']].values\n",
    "\n",
    "    boundary_all = get_boundary_points(grupos_finales)\n",
    "    avg_nn = compute_avg_nn_distance(pts_bg)\n",
    "    threshold = tol_factor * avg_nn\n",
    "\n",
    "    remaining_indices = list(range(len(pts_bg)))\n",
    "    start_idx = max(remaining_indices, key=lambda i: pts_bg[i, 0])\n",
    "    brazo_indices = [start_idx]\n",
    "    current_idx = start_idx\n",
    "    remaining_indices.remove(start_idx)\n",
    "\n",
    "    while remaining_indices:\n",
    "        current_pt = pts_bg[current_idx].reshape(1,2)\n",
    "        valid_candidates = [i for i in remaining_indices if pts_bg[i, 0] < pts_bg[current_idx, 0]]\n",
    "        if not valid_candidates:\n",
    "            break\n",
    "        candidates = pts_bg[valid_candidates]\n",
    "        dists = np.linalg.norm(candidates - current_pt, axis=1)\n",
    "        min_dist = np.min(dists)\n",
    "        candidate_local_idx = np.argmin(dists)\n",
    "        candidate_idx = valid_candidates[candidate_local_idx]\n",
    "        \n",
    "        if boundary_all.shape[0] > 0:\n",
    "            dist_to_boundary = np.min(np.linalg.norm(boundary_all - pts_bg[candidate_idx], axis=1))\n",
    "        else:\n",
    "            dist_to_boundary = np.inf\n",
    "        \n",
    "        # Si el candidato cumple la condición de cercanía o se toca la frontera, se anota y se evalúa el extremo opuesto.\n",
    "        if min_dist <= threshold or dist_to_boundary <= threshold:\n",
    "            brazo_indices.append(candidate_idx)\n",
    "            conexion = {\n",
    "                'punto_conexion': bg_cart.iloc[candidate_idx],\n",
    "                'distancia_a_frontera': dist_to_boundary\n",
    "            }\n",
    "            brazo_opuesto = trace_opposite_extremity(candidate_idx, pts_bg, remaining_indices, boundary_all, threshold)\n",
    "            if brazo_opuesto is not None:\n",
    "                brazo_indices.extend(brazo_opuesto['indices'])\n",
    "                conexion['extremo_opuesto'] = brazo_opuesto['punto_final']\n",
    "            else:\n",
    "                conexion['extremo_opuesto'] = None\n",
    "            break\n",
    "        else:\n",
    "            brazo_indices.append(candidate_idx)\n",
    "            current_idx = candidate_idx\n",
    "            remaining_indices.remove(candidate_idx)\n",
    "    \n",
    "    brazo = bg_cart.iloc[brazo_indices].copy()\n",
    "    conexiones = conexion if 'conexion' in locals() else None\n",
    "    return {'brazo': brazo, 'conexiones': conexiones}\n",
    "\n",
    "def trace_opposite_extremity(current_idx, pts_bg, remaining_indices, boundary_all, threshold):\n",
    "    \"\"\"\n",
    "    A partir de current_idx (punto que tocó la frontera), se busca extender el brazo en sentido opuesto\n",
    "    (por ejemplo, candidatos con x mayores) para evaluar conexión con otro grupo final.\n",
    "    Retorna un diccionario con:\n",
    "       - 'indices': la secuencia de índices extendida.\n",
    "       - 'punto_final': el último índice agregado.\n",
    "    O None si no se encuentra conexión.\n",
    "    \"\"\"\n",
    "    brazo_indices = []\n",
    "    cur_idx = current_idx\n",
    "    valid_candidates = [i for i in remaining_indices if pts_bg[i,0] > pts_bg[cur_idx,0]]\n",
    "    if not valid_candidates:\n",
    "        return None\n",
    "    candidates = pts_bg[valid_candidates]\n",
    "    dists = np.linalg.norm(candidates - pts_bg[cur_idx].reshape(1,2), axis=1)\n",
    "    min_dist = np.min(dists)\n",
    "    candidate_local_idx = np.argmin(dists)\n",
    "    candidate_idx = valid_candidates[candidate_local_idx]\n",
    "    \n",
    "    if boundary_all.shape[0] > 0:\n",
    "        dist_to_boundary = np.min(np.linalg.norm(boundary_all - pts_bg[candidate_idx], axis=1))\n",
    "    else:\n",
    "        dist_to_boundary = np.inf\n",
    "    \n",
    "    if min_dist <= threshold or dist_to_boundary <= threshold:\n",
    "        brazo_indices.append(candidate_idx)\n",
    "        return {'indices': brazo_indices, 'punto_final': candidate_idx}\n",
    "    return None\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Función de graficado final: brazo espiral con background y grupos\n",
    "# =============================================================================\n",
    "\n",
    "def plot_spiral_arm_with_background(arm_dict, background, grupos_finales, title=\"Brazo Espiral (θ, r) con Background\"):\n",
    "    \"\"\"\n",
    "    Grafica el brazo espiral obtenido (arm_dict['brazo']) en (θ, r) y superpone:\n",
    "      - Todos los puntos del background (alpha=0.1).\n",
    "      - Los puntos de cada grupo final (alpha=0.1).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,7))\n",
    "    \n",
    "    # Graficar background en gris (alpha=0.1)\n",
    "    plt.scatter(background['theta'], background['r'], c='gray', s=3, alpha=0.1, label=\"Background\")\n",
    "    \n",
    "    # Graficar cada grupo final con baja opacidad\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','lime']\n",
    "    for i, grupo in enumerate(grupos_finales):\n",
    "        pts = grupo['points']\n",
    "        c = colors[i % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, alpha=0.1, label=f\"Grupo {i+1}\")\n",
    "    \n",
    "    # Graficar el brazo espiral resaltado\n",
    "    brazo = arm_dict['brazo']\n",
    "    plt.scatter(brazo['theta'], brazo['r'], color='blue', s=20, label=\"Brazo Espiral\")\n",
    "    plt.plot(brazo['theta'], brazo['r'], '-o', color='red', label=\"Camino del Brazo\")\n",
    "    \n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: Ejecución de toda la pipeline\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paso 1: Generar los grupos finales y extraer el background\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "    # Paso 2: Trazar el brazo espiral usando el background y la información de los grupos finales\n",
    "    if not background.empty and len(grupos_finales) > 0:\n",
    "        arm_result = trace_spiral_arm(background, grupos_finales, tol_factor=1.1)\n",
    "        print(f\"Brazo espiral trazado con {len(arm_result['brazo'])} puntos.\")\n",
    "        if arm_result['conexiones']:\n",
    "            print(\"Conexión establecida con la frontera de algún grupo final:\")\n",
    "            print(arm_result['conexiones'])\n",
    "        \n",
    "        # Graficar el brazo espiral superpuesto a background y grupos finales\n",
    "        plot_spiral_arm_with_background(arm_result, background, grupos_finales)\n",
    "    else:\n",
    "        print(\"No hay datos suficientes en el background o en los grupos finales para trazar el brazo espiral.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96713076-fb55-42cd-b0b2-238d500ec8ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Código completo para:\n",
    "  1. Procesar y agrupar los datos (etapa de agrupación) a partir de los CSV.\n",
    "  2. Del conjunto background (puntos que no pertenecen a ningún grupo final) se trazan\n",
    "     varios caminos (“raíces”) de conexión, recorriendo en θ descendente. Cada camino se\n",
    "     forma comparando uno a muchos: se evalúa la cercanía entre un candidato y la unión de\n",
    "     (background + fronteras de grupos finales). Los puntos internos de un grupo ya se \n",
    "     ignoran para ahorrar cómputo.\n",
    "  3. Se grafican los caminos sobre el background y los grupos finales (con transparencia α = 0.1).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# =============================================================================\n",
    "# 0) Funciones para la etapa de agrupación (primer paso)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el pitch-angle (PA) en grados:\n",
    "        PA = arctan[(slope * 180/π) / intercept], si intercept ≠ 0.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Carga el archivo CSV, calcula 'r' y 'theta', filtra según el rango de θ y añade\n",
    "    una columna 'id' para preservar el índice original.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['id'] = df.index\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo de adyacencia (BFS) en el espacio (θ, r) usando una tolerancia.\n",
    "    \"\"\"\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas usando BFS.\n",
    "    \"\"\"\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide un clúster en subgrupos si existen \"gaps\" (saltos en θ o r).\n",
    "    \"\"\"\n",
    "    col = 'theta' if mode == 'theta' else 'r'\n",
    "    df = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "def generate_bfs_seeds(id_halo, theta_min, theta_max,\n",
    "                         quartile_threshold=0.55,\n",
    "                         theta_diff=3.0, r_diff=0.5,\n",
    "                         gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "                         file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Genera las semillas BFS a partir de los datos filtrados y retorna:\n",
    "       - final_cl: lista de clústeres resultantes.\n",
    "       - df_f: DataFrame filtrado (datos base usados para agrupación).\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "\n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    \"\"\"\n",
    "    Ajusta una línea (regresión lineal) a los puntos del clúster.\n",
    "    \"\"\"\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa = calculate_pa(s, b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.40, bounding_extrap=0.30):\n",
    "    \"\"\"\n",
    "    Fusiona los clústeres que se solapan en la caja definida.\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope', 'intercept', 'pa', 'theta_min', 'theta_max', 'r_min', 'r_max', 'points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "\n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0, t1, r0, r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max'])\n",
    "        b = exp(g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max'])\n",
    "        return not (a[1] < b[0] or b[1] < a[0]) and not (a[3] < b[2] or b[3] < a[2])\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new.append(g1)\n",
    "            i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    \"\"\"\n",
    "    Valida la dispersión de cada grupo y re-procesa aquellos que superen el umbral.\n",
    "    \"\"\"\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g)\n",
    "            continue\n",
    "        res = g['points']['r'] - (g['slope'] * g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "    \n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope', 'intercept', 'pa', 'theta_min', 'theta_max', 'r_min', 'r_max', 'points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualiza en un gráfico cartesiano los grupos finales (con ≥ 60 puntos) en (θ, r).\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            plt.plot(t, g['slope'] * t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – grupos ≥60 pts (cartesiano, PA visible)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualización en proyección polar de los grupos finales (con ≥ 60 puntos).\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c,\n",
    "                   label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            r_line = g['slope'] * t_deg + g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – proyección polar (PA visible)\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20, 1.0))\n",
    "    plt.show()\n",
    "\n",
    "def main_example_modificado():\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline completo para subhalo 17.\n",
    "    Retorna un diccionario con:\n",
    "       - 'grupos_finales': lista de grupos (con ≥ 60 puntos)\n",
    "       - 'background': DataFrame con los puntos que no están en ningún grupo final.\n",
    "    \"\"\"\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "    \n",
    "    # Almacenar grupos finales: únicamente aquellos con 60 o más puntos.\n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "    \n",
    "    # Determinar los puntos (IDs) que aparecen en los grupos finales.\n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "    \n",
    "    # Los puntos \"background\" son aquellos de df_f que no están en ningún grupo final.\n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "    \n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Funciones para el trazado de varios caminos (\"raíces\") a partir del background,\n",
    "#    recorriendo en orden descendente de θ\n",
    "# =============================================================================\n",
    "\n",
    "def trace_spiral_arms(background, grupos_finales, tol_factor=1.1):\n",
    "    \"\"\"\n",
    "    Recorre el conjunto de puntos del background (en coordenadas polares) para generar\n",
    "    *varios* caminos independientes (raíces) de conexión. Se parte de la semilla con mayor\n",
    "    valor de θ y se sigue de forma greedy (solo se consideran candidatos con θ menor que el\n",
    "    punto actual). Para cada candidato se evalúa su distancia (en coordenadas cartesianas)\n",
    "    tanto respecto al punto actual como respecto a la unión de las fronteras de los grupos\n",
    "    finales. Se selecciona el candidato si alguna de esas distancias es menor que threshold.\n",
    "    \n",
    "    Los caminos no se unirán entre sí: una vez iniciado un camino se remueven los puntos de\n",
    "    background empleados, permitiendo la creación de ramas paralelas.\n",
    "    \n",
    "    Retorna una lista de DataFrames, cada uno representando un camino (brazo espiral).\n",
    "    \"\"\"\n",
    "    # Ordenar background por θ descendente.\n",
    "    bg = background.copy().sort_values(\"theta\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Función para convertir un punto (fila) a coordenadas cartesianas.\n",
    "    def to_cartesian(row):\n",
    "        rad = np.radians(row['theta'])\n",
    "        return np.array([row['r'] * np.cos(rad), row['r'] * np.sin(rad)])\n",
    "    \n",
    "    # Obtener la unión de las fronteras de los grupos finales:\n",
    "    boundary_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts_grp = grupo['points']\n",
    "        for _, row in pts_grp.iterrows():\n",
    "            boundary_list.append(to_cartesian(row))\n",
    "    if boundary_list:\n",
    "        boundary_all = np.vstack(boundary_list)\n",
    "    else:\n",
    "        boundary_all = np.empty((0,2))\n",
    "    \n",
    "    # Para definir threshold, calcular la distancia promedio entre vecinos en el background.\n",
    "    pts_bg_cart = np.array([to_cartesian(row) for _, row in bg.iterrows()])\n",
    "    if len(pts_bg_cart) < 2:\n",
    "        avg_nn = np.inf\n",
    "    else:\n",
    "        dists = cdist(pts_bg_cart, pts_bg_cart)\n",
    "        np.fill_diagonal(dists, np.inf)\n",
    "        avg_nn = np.mean(np.min(dists, axis=1))\n",
    "    threshold = tol_factor * avg_nn\n",
    "    \n",
    "    arms = []  # Lista para almacenar los caminos resultantes.\n",
    "    \n",
    "    # Mientras hayan puntos en el background:\n",
    "    while not bg.empty:\n",
    "        # Semilla: punto con mayor θ.\n",
    "        seed_idx = bg['theta'].idxmax()\n",
    "        seed = bg.loc[seed_idx]\n",
    "        path = [seed]\n",
    "        # Remover la semilla del background.\n",
    "        bg = bg.drop(seed_idx).reset_index(drop=True)\n",
    "        \n",
    "        current = seed\n",
    "        while True:\n",
    "            # Seleccionar candidatos: aquellos con θ menor que el punto actual.\n",
    "            candidates = bg[bg['theta'] < current['theta']]\n",
    "            if candidates.empty:\n",
    "                break\n",
    "            # Calcular la distancia desde el punto actual a cada candidato en coordenadas cartesianas.\n",
    "            current_cart = to_cartesian(current)\n",
    "            candidate_coords = np.array([to_cartesian(row) for _, row in candidates.iterrows()])\n",
    "            dists_current = np.linalg.norm(candidate_coords - current_cart, axis=1)\n",
    "            # Calcular la distancia de cada candidato a la unión de fronteras.\n",
    "            if boundary_all.shape[0] > 0:\n",
    "                dists_boundary = np.array([np.min(np.linalg.norm(candidate - boundary_all, axis=1))\n",
    "                                           for candidate in candidate_coords])\n",
    "            else:\n",
    "                dists_boundary = np.full(len(candidates), np.inf)\n",
    "            # Se define la distancia efectiva como el mínimo entre la distancia al actual y a la frontera.\n",
    "            effective_dists = np.minimum(dists_current, dists_boundary)\n",
    "            min_eff = np.min(effective_dists)\n",
    "            idx_min = np.argmin(effective_dists)\n",
    "            # Verificar condición.\n",
    "            if min_eff <= threshold:\n",
    "                # Agregar el candidato a la ruta.\n",
    "                candidate = candidates.iloc[idx_min]\n",
    "                path.append(candidate)\n",
    "                current = candidate\n",
    "                # Remover candidato del background.\n",
    "                bg = bg.drop(candidate.name).reset_index(drop=True)\n",
    "            else:\n",
    "                # No hay candidato cercano, se termina el camino.\n",
    "                break\n",
    "        \n",
    "        arms.append(pd.DataFrame(path))\n",
    "    \n",
    "    return arms\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Función de graficado final: muestra los caminos (raíces) junto con background y grupos\n",
    "# =============================================================================\n",
    "\n",
    "def plot_arms_with_background(arms, background, grupos_finales, title=\"Caminos (raíces) en (θ, r) con Background\"):\n",
    "    \"\"\"\n",
    "    Grafica los caminos trazados (lista de DataFrames) sobre el background y los puntos de\n",
    "    los grupos finales. Se emplea α = 0.1 para background y grupos, para resaltarlos de forma tenue.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    # Background con baja opacidad.\n",
    "    plt.scatter(background['theta'], background['r'], c='gray', s=3, alpha=0.1, label=\"Background\")\n",
    "    \n",
    "    # Grupos finales con baja opacidad.\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','lime']\n",
    "    for i, grupo in enumerate(grupos_finales):\n",
    "        pts = grupo['points']\n",
    "        c = colors[i % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, alpha=0.1, label=f\"Grupo {i+1}\")\n",
    "    \n",
    "    # Graficar cada camino (brazo) con un color distinto.\n",
    "    for j, arm in enumerate(arms):\n",
    "        plt.plot(arm['theta'], arm['r'], '-o', label=f\"Camino {j+1}\", linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: Ejecución de toda la pipeline\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paso 1: Procesar y agrupar los datos\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "    # Paso 2: Trazar varios caminos (\"raíces\") a partir del background, recorriendo en θ descendente\n",
    "    if not background.empty and len(grupos_finales) > 0:\n",
    "        arms = trace_spiral_arms(background, grupos_finales, tol_factor=1.1)\n",
    "        print(f\"Se trazaron {len(arms)} caminos (raíces).\")\n",
    "        # Paso 3: Graficar los caminos junto con background y grupos finales\n",
    "        plot_arms_with_background(arms, background, grupos_finales)\n",
    "    else:\n",
    "        print(\"No hay datos suficientes en el background o en los grupos finales para trazar los caminos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125e361c-2507-48dc-a840-346b7c70059a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Código completo para:\n",
    "  1. Procesar y agrupar los datos (etapa de agrupación) a partir de los CSV.\n",
    "  2. Del conjunto background (puntos que no pertenecen a ningún grupo final) se trazan\n",
    "     varios caminos (\"raíces\") de conexión, recorriendo en θ descendente. Cada camino se\n",
    "     forma comparando uno a muchos: se evalúa la cercanía entre un candidato y la unión de\n",
    "     (background + fronteras de grupos finales). Se ignoran los puntos internos de un grupo\n",
    "     para ahorrar cómputo. Además, se fuerza que los caminos no estén separados de otros grupos\n",
    "     por una distancia mayor fija de 0.65 (en r y θ), salvo cuando se considere una frontera.\n",
    "  3. Se grafican los caminos sobre el background y los grupos finales (con transparencia α = 0.1).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# =============================================================================\n",
    "# 0) Funciones para la etapa de agrupación (primer paso)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el pitch-angle (PA) en grados:\n",
    "        PA = arctan[(slope * 180/π) / intercept], si intercept ≠ 0.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Carga el archivo CSV, calcula 'r' y 'theta', filtra según el rango de θ y añade\n",
    "    una columna 'id' para preservar el índice original.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['id'] = df.index\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo de adyacencia (BFS) en el espacio (θ, r) usando una tolerancia.\n",
    "    \"\"\"\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas usando BFS.\n",
    "    \"\"\"\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide un clúster en subgrupos si existen \"gaps\" (saltos en θ o r).\n",
    "    \"\"\"\n",
    "    col = 'theta' if mode == 'theta' else 'r'\n",
    "    df = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "def generate_bfs_seeds(id_halo, theta_min, theta_max,\n",
    "                         quartile_threshold=0.55,\n",
    "                         theta_diff=3.0, r_diff=0.5,\n",
    "                         gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "                         file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Genera las semillas BFS a partir de los datos filtrados y retorna:\n",
    "       - final_cl: lista de clústeres resultantes.\n",
    "       - df_f: DataFrame filtrado (datos base usados para agrupación).\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "\n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    \"\"\"\n",
    "    Ajusta una línea (regresión lineal) a los puntos del clúster.\n",
    "    \"\"\"\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa = calculate_pa(s, b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.40, bounding_extrap=0.30):\n",
    "    \"\"\"\n",
    "    Fusiona los clústeres que se solapan en la caja definida.\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope', 'intercept', 'pa', 'theta_min', 'theta_max', 'r_min', 'r_max', 'points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "\n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0, t1, r0, r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max'])\n",
    "        b = exp(g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max'])\n",
    "        return not (a[1] < b[0] or b[1] < a[0]) and not (a[3] < b[2] or b[3] < a[2])\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new.append(g1)\n",
    "            i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    \"\"\"\n",
    "    Valida la dispersión de cada grupo y re-procesa aquellos que superen el umbral.\n",
    "    \"\"\"\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g)\n",
    "            continue\n",
    "        res = g['points']['r'] - (g['slope'] * g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "    \n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope', 'intercept', 'pa', 'theta_min', 'theta_max', 'r_min', 'r_max', 'points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualiza en un gráfico cartesiano los grupos finales (con ≥ 60 puntos) en (θ, r).\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            plt.plot(t, g['slope'] * t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – grupos ≥60 pts (cartesiano, PA visible)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualización en proyección polar de los grupos finales (con ≥ 60 puntos).\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c,\n",
    "                   label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            r_line = g['slope'] * t_deg + g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – proyección polar (PA visible)\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20, 1.0))\n",
    "    plt.show()\n",
    "\n",
    "def main_example_modificado():\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline completo para subhalo 17.\n",
    "    Retorna un diccionario con:\n",
    "       - 'grupos_finales': lista de grupos (con ≥ 60 pts)\n",
    "       - 'background': DataFrame con los puntos que no están en ningún grupo final.\n",
    "    \"\"\"\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "    \n",
    "    # Almacenar grupos finales: únicamente aquellos con 60 o más puntos.\n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "    \n",
    "    # Determinar los puntos (IDs) que aparecen en los grupos finales.\n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "    \n",
    "    # Los puntos \"background\" son aquellos de df_f que no están en ningún grupo final.\n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "    \n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Funciones para el trazado de varios caminos (\"raíces\") a partir del background,\n",
    "#    recorriendo en orden descendente de θ\n",
    "# =============================================================================\n",
    "\n",
    "def trace_spiral_arms(background, grupos_finales, tol_factor=1.1):\n",
    "    \"\"\"\n",
    "    Recorre el conjunto de puntos del background (en coordenadas polares) para generar\n",
    "    *varios* caminos independientes (\"raíces\") de conexión. Se parte de la semilla con mayor\n",
    "    valor de θ y se sigue de forma greedy (solo se consideran candidatos con θ menor que el\n",
    "    punto actual). Para cada candidato se evalúa su distancia (en coordenadas cartesianas)\n",
    "    tanto respecto al punto actual como respecto a la unión de las fronteras de los grupos\n",
    "    finales. Se selecciona el candidato si alguna de esas distancias es menor que el threshold.\n",
    "    \n",
    "    Además, se fuerza que el threshold no supere un valor fijo de 0.65 (en unidades compatibles),\n",
    "    de forma que los caminos no se separen de otros grupos por más de esa distancia.\n",
    "    \n",
    "    Los caminos se generan de forma paralela (una vez empleado un punto se remueve del background)\n",
    "    para formar ramas separadas.\n",
    "    \n",
    "    Retorna una lista de DataFrames, cada uno representando un camino (brazo espiral).\n",
    "    \"\"\"\n",
    "    # Ordenar background por θ descendente.\n",
    "    bg = background.copy().sort_values(\"theta\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Función para convertir un punto (fila) a coordenadas cartesianas.\n",
    "    def to_cartesian(row):\n",
    "        rad = np.radians(row['theta'])\n",
    "        return np.array([row['r'] * np.cos(rad), row['r'] * np.sin(rad)])\n",
    "    \n",
    "    # Obtener la unión de las fronteras de los grupos finales.\n",
    "    boundary_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts_grp = grupo['points']\n",
    "        for _, row in pts_grp.iterrows():\n",
    "            boundary_list.append(to_cartesian(row))\n",
    "    if boundary_list:\n",
    "        boundary_all = np.vstack(boundary_list)\n",
    "    else:\n",
    "        boundary_all = np.empty((0,2))\n",
    "    \n",
    "    # Para definir threshold: calcular la distancia promedio entre vecinos en el background.\n",
    "    pts_bg_cart = np.array([to_cartesian(row) for _, row in bg.iterrows()])\n",
    "    if len(pts_bg_cart) < 2:\n",
    "        avg_nn = np.inf\n",
    "    else:\n",
    "        dists = cdist(pts_bg_cart, pts_bg_cart)\n",
    "        np.fill_diagonal(dists, np.inf)\n",
    "        avg_nn = np.mean(np.min(dists, axis=1))\n",
    "    # Se fuerza que el threshold no supere 0.65 (para r y θ en unidades compatibles)\n",
    "    threshold = min(tol_factor * avg_nn, 0.65)\n",
    "    \n",
    "    arms = []  # Lista para almacenar los caminos resultantes.\n",
    "    \n",
    "    # Mientras queden puntos en el background:\n",
    "    while not bg.empty:\n",
    "        # Semilla: el punto con mayor θ.\n",
    "        seed_idx = bg['theta'].idxmax()\n",
    "        seed = bg.loc[seed_idx]\n",
    "        path = [seed]\n",
    "        # Remover la semilla del background.\n",
    "        bg = bg.drop(seed_idx).reset_index(drop=True)\n",
    "        \n",
    "        current = seed\n",
    "        while True:\n",
    "            # Seleccionar candidatos con θ menor que la del punto actual.\n",
    "            candidates = bg[bg['theta'] < current['theta']]\n",
    "            if candidates.empty:\n",
    "                break\n",
    "            # Convertir candidatos a coordenadas cartesianas.\n",
    "            candidate_coords = np.array([to_cartesian(row) for _, row in candidates.iterrows()])\n",
    "            current_cart = to_cartesian(current)\n",
    "            dists_current = np.linalg.norm(candidate_coords - current_cart, axis=1)\n",
    "            # Calcular la distancia de cada candidato a la unión de fronteras.\n",
    "            if boundary_all.shape[0] > 0:\n",
    "                dists_boundary = np.array([np.min(np.linalg.norm(candidate - boundary_all, axis=1))\n",
    "                                           for candidate in candidate_coords])\n",
    "            else:\n",
    "                dists_boundary = np.full(len(candidates), np.inf)\n",
    "            # La distancia efectiva será el mínimo entre la distancia al actual y a la frontera.\n",
    "            effective_dists = np.minimum(dists_current, dists_boundary)\n",
    "            min_eff = np.min(effective_dists)\n",
    "            idx_min = np.argmin(effective_dists)\n",
    "            \n",
    "            # Si el mínimo efectivo es menor o igual al threshold, se añade el candidato.\n",
    "            if min_eff <= threshold:\n",
    "                candidate = candidates.iloc[idx_min]\n",
    "                path.append(candidate)\n",
    "                current = candidate\n",
    "                # Remover el candidato del background.\n",
    "                bg = bg.drop(candidate.name).reset_index(drop=True)\n",
    "            else:\n",
    "                # Si no se encuentra candidato cercano, se termina el camino.\n",
    "                break\n",
    "        \n",
    "        arms.append(pd.DataFrame(path))\n",
    "    \n",
    "    return arms\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Función de graficado final: muestra los caminos (\"raíces\") junto con background y grupos\n",
    "# =============================================================================\n",
    "\n",
    "def plot_arms_with_background(arms, background, grupos_finales, title=\"Caminos (raíces) en (θ, r) con Background\"):\n",
    "    \"\"\"\n",
    "    Grafica los caminos trazados (lista de DataFrames) sobre el background y los puntos de\n",
    "    los grupos finales. Se emplea α = 0.1 para background y grupos, para resaltarlos de forma tenue.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    # Background con baja opacidad.\n",
    "    plt.scatter(background['theta'], background['r'], c='gray', s=3, alpha=0.1, label=\"Background\")\n",
    "    \n",
    "    # Grupos finales con baja opacidad.\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','lime']\n",
    "    for i, grupo in enumerate(grupos_finales):\n",
    "        pts = grupo['points']\n",
    "        c = colors[i % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, alpha=0.1, label=f\"Grupo {i+1}\")\n",
    "    \n",
    "    # Graficar cada camino (brazo) con un color distinto.\n",
    "    for j, arm in enumerate(arms):\n",
    "        plt.plot(arm['theta'], arm['r'], '-o', label=f\"Camino {j+1}\", linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: Ejecución de toda la pipeline\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paso 1: Procesar y agrupar los datos\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "    # Paso 2: Trazar varios caminos (\"raíces\") a partir del background, recorriendo en θ descendente\n",
    "    if not background.empty and len(grupos_finales) > 0:\n",
    "        arms = trace_spiral_arms(background, grupos_finales, tol_factor=1.1)\n",
    "        print(f\"Se trazaron {len(arms)} caminos (raíces).\")\n",
    "        # Paso 3: Graficar los caminos junto con background y grupos finales\n",
    "        plot_arms_with_background(arms, background, grupos_finales)\n",
    "    else:\n",
    "        print(\"No hay datos suficientes en el background o en los grupos finales para trazar los caminos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b18fd06-bb59-482a-b539-b813861f52a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Código completo para:\n",
    "  1. Procesar y agrupar los datos (etapa de agrupación) a partir de los CSV.\n",
    "  2. Del conjunto background (puntos que no pertenecen a ningún grupo final) se trazan\n",
    "     varios caminos (\"raíces\") de conexión, recorriendo en orden descendente de θ.\n",
    "     Aquí se exige que el camino sea físicamente continuo, es decir, que los puntos conectados\n",
    "     tengan una distancia estricta (máx. 0.65 o tol_factor * promedio) y que la dirección de\n",
    "     cada segmento no varíe abruptamente (diferencia angular menor a 10°).\n",
    "  3. Se grafican los caminos sobre el background y los grupos finales (con α = 0.1).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# =============================================================================\n",
    "# 0) Funciones para la etapa de agrupación (primer paso)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el pitch-angle (PA) en grados:\n",
    "        PA = arctan[(slope * 180/π) / intercept], si intercept ≠ 0.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Carga el archivo CSV, calcula 'r' y 'theta', filtra según el rango de θ y añade\n",
    "    una columna 'id' para preservar el índice original.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['id'] = df.index\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo de adyacencia (BFS) en el espacio (θ, r) usando una tolerancia.\n",
    "    \"\"\"\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas usando BFS.\n",
    "    \"\"\"\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide un clúster en subgrupos si existen \"gaps\" (saltos en θ o r).\n",
    "    \"\"\"\n",
    "    col = 'theta' if mode == 'theta' else 'r'\n",
    "    df = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "def generate_bfs_seeds(id_halo, theta_min, theta_max,\n",
    "                         quartile_threshold=0.55,\n",
    "                         theta_diff=3.0, r_diff=0.5,\n",
    "                         gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "                         file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Genera las semillas BFS a partir de los datos filtrados y retorna:\n",
    "       - final_cl: lista de clústeres resultantes.\n",
    "       - df_f: DataFrame filtrado (datos base usados para agrupación).\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "\n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    \"\"\"\n",
    "    Ajusta una línea (regresión lineal) a los puntos del clúster.\n",
    "    \"\"\"\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa = calculate_pa(s, b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.40, bounding_extrap=0.30):\n",
    "    \"\"\"\n",
    "    Fusiona los clústeres que se solapan en la caja definida.\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope', 'intercept', 'pa', 'theta_min', 'theta_max', 'r_min', 'r_max', 'points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "\n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0, t1, r0, r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max'])\n",
    "        b = exp(g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max'])\n",
    "        return not (a[1] < b[0] or b[1] < a[0]) and not (a[3] < b[2] or b[3] < a[2])\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new.append(g1)\n",
    "            i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    \"\"\"\n",
    "    Valida la dispersión de cada grupo y re-procesa aquellos que superen el umbral.\n",
    "    \"\"\"\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g)\n",
    "            continue\n",
    "        res = g['points']['r'] - (g['slope'] * g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "    \n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope', 'intercept', 'pa', 'theta_min', 'theta_max', 'r_min', 'r_max', 'points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualiza en un gráfico cartesiano los grupos finales (con ≥ 60 puntos) en (θ, r).\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            plt.plot(t, g['slope'] * t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – grupos ≥60 pts (cartesiano, PA visible)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualización en proyección polar de los grupos finales (con ≥ 60 puntos).\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c,\n",
    "                   label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            r_line = g['slope'] * t_deg + g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – proyección polar (PA visible)\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20, 1.0))\n",
    "    plt.show()\n",
    "\n",
    "def main_example_modificado():\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline completo para subhalo 17.\n",
    "    Retorna un diccionario con:\n",
    "       - 'grupos_finales': lista de grupos (con ≥ 60 pts)\n",
    "       - 'background': DataFrame con los puntos que no están en ningún grupo final.\n",
    "    \"\"\"\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "    \n",
    "    # Almacenar grupos finales: únicamente aquellos con 60 o más puntos.\n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "    \n",
    "    # Determinar los puntos (IDs) que aparecen en los grupos finales.\n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "    \n",
    "    # Los puntos \"background\" son aquellos de df_f que no están en ningún grupo final.\n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "    \n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Funciones para el trazado de varios caminos (\"raíces\") a partir del background,\n",
    "#    recorriendo en orden descendente de θ, con conexión \"física\" y estricta\n",
    "# =============================================================================\n",
    "\n",
    "def trace_spiral_arms(background, grupos_finales, tol_factor=1.1, max_angle_diff=10):\n",
    "    \"\"\"\n",
    "    Genera varios caminos (\"raíces\") a partir del background (en coordenadas polares),\n",
    "    recorriendo en orden descendente de θ. Se parte del punto semilla con mayor θ y para cada\n",
    "    candidato se evalúan dos condiciones:\n",
    "      1. La distancia en coordenadas cartesianas desde el punto actual debe ser menor a un threshold,\n",
    "         definido como min(tol_factor * (promedio distancia entre vecinos), 0.65).\n",
    "      2. La diferencia angular entre el segmento actual (la dirección del camino) y la dirección\n",
    "         hacia el candidato debe ser menor a max_angle_diff (en grados).\n",
    "    Estos criterios deben cumplirse para formar un camino continuo y físico.\n",
    "    \n",
    "    Para evaluar la condición 2, si existe un segmento previo se calcula el ángulo del segmento;\n",
    "    de lo contrario se acepta el primer salto. Los puntos empleados se remueven del background\n",
    "    para que los caminos se formen de forma paralela y no se unan.\n",
    "    \n",
    "    Retorna una lista de DataFrames, cada uno representando un camino.\n",
    "    \"\"\"\n",
    "    # Ordenar background por θ descendente.\n",
    "    bg = background.copy().sort_values(\"theta\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Función para convertir un punto a coordenadas cartesianas.\n",
    "    def to_cartesian(row):\n",
    "        rad = np.radians(row['theta'])\n",
    "        return np.array([row['r'] * np.cos(rad), row['r'] * np.sin(rad)])\n",
    "    \n",
    "    # Obtener la unión de las fronteras de los grupos finales.\n",
    "    boundary_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts_grp = grupo['points']\n",
    "        for _, row in pts_grp.iterrows():\n",
    "            boundary_list.append(to_cartesian(row))\n",
    "    if boundary_list:\n",
    "        boundary_all = np.vstack(boundary_list)\n",
    "    else:\n",
    "        boundary_all = np.empty((0,2))\n",
    "    \n",
    "    # Calcular la distancia promedio entre vecinos en el background (en coordenadas cartesianas)\n",
    "    pts_bg_cart = np.array([to_cartesian(row) for _, row in bg.iterrows()])\n",
    "    if len(pts_bg_cart) < 2:\n",
    "        avg_nn = np.inf\n",
    "    else:\n",
    "        dists = cdist(pts_bg_cart, pts_bg_cart)\n",
    "        np.fill_diagonal(dists, np.inf)\n",
    "        avg_nn = np.mean(np.min(dists, axis=1))\n",
    "    threshold = min(tol_factor * avg_nn, 0.65)\n",
    "    \n",
    "    arms = []  # Lista de caminos resultantes.\n",
    "    \n",
    "    # Mientras queden puntos en el background:\n",
    "    while not bg.empty:\n",
    "        # Semilla: punto con mayor θ.\n",
    "        seed_idx = bg['theta'].idxmax()\n",
    "        seed = bg.loc[seed_idx]\n",
    "        path = [seed]\n",
    "        bg = bg.drop(seed_idx).reset_index(drop=True)\n",
    "        current = seed\n",
    "        prev_direction = None  # No hay segmento anterior para la primera conexión.\n",
    "        \n",
    "        while True:\n",
    "            # Seleccionar candidatos con θ menor que el punto actual.\n",
    "            candidates = bg[bg['theta'] < current['theta']]\n",
    "            if candidates.empty:\n",
    "                break\n",
    "            # Convertir candidatos a coordenadas cartesianas.\n",
    "            candidate_coords = np.array([to_cartesian(row) for _, row in candidates.iterrows()])\n",
    "            current_cart = to_cartesian(current)\n",
    "            dists_current = np.linalg.norm(candidate_coords - current_cart, axis=1)\n",
    "            # También, calcular distancia a la frontera de grupos finales.\n",
    "            if boundary_all.shape[0] > 0:\n",
    "                dists_boundary = np.array([np.min(np.linalg.norm(candidate - boundary_all, axis=1))\n",
    "                                           for candidate in candidate_coords])\n",
    "            else:\n",
    "                dists_boundary = np.full(len(candidates), np.inf)\n",
    "            effective_dists = np.minimum(dists_current, dists_boundary)\n",
    "            \n",
    "            # Para cada candidato, calcule su dirección desde el actual.\n",
    "            directions = []\n",
    "            for coord in candidate_coords:\n",
    "                vec = coord - current_cart\n",
    "                angle = np.degrees(np.arctan2(vec[1], vec[0])) % 360\n",
    "                directions.append(angle)\n",
    "            directions = np.array(directions)\n",
    "            \n",
    "            # Si ya existe un segmento previo, la dirección actual es prev_direction.\n",
    "            # Se exige que la diferencia angular entre prev_direction y la dirección del candidato sea < max_angle_diff.\n",
    "            if prev_direction is not None:\n",
    "                angle_diffs = np.abs((directions - prev_direction + 180) % 360 - 180)\n",
    "                valid_idx = np.where(angle_diffs < max_angle_diff)[0]\n",
    "                if len(valid_idx) == 0:\n",
    "                    break  # No hay candidato que mantenga la continuidad del camino.\n",
    "                effective_dists = effective_dists[valid_idx]\n",
    "                candidate_subset = candidates.iloc[valid_idx]\n",
    "                directions = directions[valid_idx]\n",
    "            else:\n",
    "                candidate_subset = candidates\n",
    "            \n",
    "            # Seleccionar el candidato con la mínima effective_dist.\n",
    "            if len(effective_dists) == 0:\n",
    "                break\n",
    "            min_eff = np.min(effective_dists)\n",
    "            idx_min = np.argmin(effective_dists)\n",
    "            if min_eff > threshold:\n",
    "                break  # Ningún candidato cumple la condición física.\n",
    "            \n",
    "            candidate = candidate_subset.iloc[idx_min]\n",
    "            # Actualizar el segmento: dirección actual\n",
    "            current_coord = to_cartesian(current)\n",
    "            candidate_coord = to_cartesian(candidate)\n",
    "            new_direction = np.degrees(np.arctan2(candidate_coord[1]-current_coord[1],\n",
    "                                                   candidate_coord[0]-current_coord[0])) % 360\n",
    "            \n",
    "            # Agregar el candidato al camino.\n",
    "            path.append(candidate)\n",
    "            prev_direction = new_direction\n",
    "            current = candidate\n",
    "            bg = bg.drop(candidate.name).reset_index(drop=True)\n",
    "        \n",
    "        arms.append(pd.DataFrame(path))\n",
    "    \n",
    "    return arms\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Función de graficado final: muestra los caminos (\"raíces\") junto con background y grupos\n",
    "# =============================================================================\n",
    "\n",
    "def plot_arms_with_background(arms, background, grupos_finales, title=\"Caminos (raíces) en (θ, r) con Background\"):\n",
    "    \"\"\"\n",
    "    Grafica los caminos trazados (lista de DataFrames) sobre el background y los puntos de\n",
    "    los grupos finales. Se emplea α = 0.1 para background y grupos.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    # Graficar background con baja opacidad.\n",
    "    plt.scatter(background['theta'], background['r'], c='gray', s=3, alpha=0.1, label=\"Background\")\n",
    "    \n",
    "    # Graficar cada grupo final con baja opacidad.\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','lime']\n",
    "    for i, grupo in enumerate(grupos_finales):\n",
    "        pts = grupo['points']\n",
    "        c = colors[i % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, alpha=0.1, label=f\"Grupo {i+1}\")\n",
    "    \n",
    "    # Graficar cada camino con un color diferente.\n",
    "    for j, arm in enumerate(arms):\n",
    "        plt.plot(arm['theta'], arm['r'], '-o', label=f\"Camino {j+1}\", linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: Ejecución de toda la pipeline\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paso 1: Procesar y agrupar los datos\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "    # Paso 2: Trazar varios caminos (\"raíces\") a partir del background, recorriendo en θ descendente\n",
    "    if not background.empty and len(grupos_finales) > 0:\n",
    "        arms = trace_spiral_arms(background, grupos_finales, tol_factor=1.1, max_angle_diff=10)\n",
    "        print(f\"Se trazaron {len(arms)} caminos (raíces).\")\n",
    "        # Paso 3: Graficar los caminos junto con background y grupos finales\n",
    "        plot_arms_with_background(arms, background, grupos_finales)\n",
    "    else:\n",
    "        print(\"No hay datos suficientes en el background o en los grupos finales para trazar los caminos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f64ae129-0df4-4568-a063-8c7df8b1895d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd2d863-12f8-42d7-b27b-b1abe2a3f6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Código completo para:\n",
    "  1. Procesar y agrupar los datos (etapa de agrupación) a partir de CSV.\n",
    "  2. Del conjunto background (puntos sin asignar a grupos finales) se trazan\n",
    "     varios caminos (\"raíces\") de conexión, recorriendo en orden descendente de θ.\n",
    "     Se exige que:\n",
    "       - Cada segmento tenga una distancia menor que: min(tol_factor × promedio, 0.65)\n",
    "       - La diferencia angular entre segmentos consecutivos sea menor a 10°.\n",
    "       - El candidato a conexión esté en una región de densidad local ≥ densidad media del background.\n",
    "  3. Se grafican solo los caminos con las distancias (longitudes) más largas (por encima de la mediana)\n",
    "     superpuestos al background y grupos finales (con α = 0.1).\n",
    "     \n",
    "Puedes ajustar los parámetros (tol_factor, max_angle_diff, density_radius) según tus necesidades.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# =============================================================================\n",
    "# 0) Funciones para la etapa de agrupación (primer paso)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Calcula el pitch-angle (PA) en grados:\n",
    "      PA = arctan[(slope * 180/π) / intercept], si intercept ≠ 0.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180/np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Carga el CSV, calcula 'r' y 'theta', filtra según el rango de θ y añade una\n",
    "    columna 'id' para preservar el índice original.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['id'] = df.index\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    \"\"\"\n",
    "    Construye un grafo de adyacencia (BFS) en el espacio (θ, r) usando una tolerancia.\n",
    "    \"\"\"\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    \"\"\"\n",
    "    Extrae componentes conexas usando BFS.\n",
    "    \"\"\"\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    \"\"\"\n",
    "    Subdivide un clúster en subgrupos si existen \"gaps\" (saltos en θ o r).\n",
    "    \"\"\"\n",
    "    col = 'theta' if mode=='theta' else 'r'\n",
    "    df = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts/ref, 1.0))\n",
    "\n",
    "def generate_bfs_seeds(id_halo, theta_min, theta_max,\n",
    "                         quartile_threshold=0.55,\n",
    "                         theta_diff=3.0, r_diff=0.5,\n",
    "                         gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "                         file_prefix='data_rho'):\n",
    "    \"\"\"\n",
    "    Genera las semillas BFS a partir de los datos filtrados y retorna:\n",
    "      - final_cl: lista de clústeres resultantes.\n",
    "      - df_f: DataFrame filtrado (datos base).\n",
    "    \"\"\"\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "    \n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "    \n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    \"\"\"\n",
    "    Ajusta una línea (regresión lineal) a los puntos del clúster.\n",
    "    \"\"\"\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa = calculate_pa(s, b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.40, bounding_extrap=0.30):\n",
    "    \"\"\"\n",
    "    Fusiona los clústeres que se solapan.\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "    \n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "    \n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0, t1, r0, r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max'])\n",
    "        b = exp(g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max'])\n",
    "        return not (a[1] < b[0] or b[1] < a[0]) and not (a[3] < b[2] or b[3] < a[2])\n",
    "    \n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new.append(g1)\n",
    "            i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    \"\"\"\n",
    "    Valida la dispersión de cada grupo y re-procesa aquellos que superen el umbral.\n",
    "    \"\"\"\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g)\n",
    "            continue\n",
    "        res = g['points']['r'] - (g['slope'] * g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "    \n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualiza en un gráfico cartesiano los grupos finales (con ≥ 60 puntos) en (θ, r).\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, alpha=0.3, label=f\"Grupo {i+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            plt.plot(t, g['slope']*t+g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – Grupos (≥ 60 pts) - Cartesiano\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    Visualización en proyección polar de los grupos finales (con ≥ 60 puntos).\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c, alpha=0.3,\n",
    "                   label=f\"Grupo {i+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            r_line = g['slope']*t_deg+g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – Grupos (≥ 60 pts) - Polar\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20,1.0))\n",
    "    plt.show()\n",
    "\n",
    "def main_example_modificado():\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline completo para subhalo 17.\n",
    "    Retorna un diccionario con:\n",
    "      - 'grupos_finales': lista de grupos (con ≥ 60 pts).\n",
    "      - 'background': DataFrame con los puntos que no están en ningún grupo final.\n",
    "    \"\"\"\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "    \n",
    "    # Almacenar grupos finales: aquellos con 60 o más puntos.\n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "    \n",
    "    # Puntos de grupos.\n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "    \n",
    "    # Background: puntos que no están en ningún grupo final.\n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "    \n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Funciones para trazado de caminos (\"raíces\") conectados físicamente\n",
    "# =============================================================================\n",
    "\n",
    "def local_density(point, points, radius=0.5):\n",
    "    \"\"\"\n",
    "    Calcula la cantidad de puntos en 'points' que se encuentran a menos de 'radius'\n",
    "    distancia de 'point'. Se usa como criterio local de densidad.\n",
    "    \"\"\"\n",
    "    d = np.linalg.norm(points - point, axis=1)\n",
    "    return np.sum(d < radius)\n",
    "\n",
    "def trace_spiral_arms(background, grupos_finales, tol_factor=1.1, max_angle_diff=10, density_radius=0.5):\n",
    "    \"\"\"\n",
    "    Genera varios caminos (\"raíces\") a partir del background (en coordenadas polares),\n",
    "    recorriendo en orden descendente de θ. Para cada candidato se exige:\n",
    "      1. Distancia en coordenadas cartesianas < min(tol_factor*(promedio), 0.65)\n",
    "      2. Diferencia angular entre el segmento actual y la dirección hacia el candidato < max_angle_diff\n",
    "      3. El candidato debe encontrarse en una región de densidad local (contando vecinos en un radio dado)\n",
    "         mayor o igual que la densidad media del background.\n",
    "    Los puntos usados se remueven del background para que los caminos sean independientes.\n",
    "    \n",
    "    Retorna una lista de DataFrames, cada uno representando un camino.\n",
    "    \"\"\"\n",
    "    # Ordenar background por θ descendente.\n",
    "    bg = background.copy().sort_values(\"theta\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    def to_cartesian(row):\n",
    "        rad = np.radians(row['theta'])\n",
    "        return np.array([row['r']*np.cos(rad), row['r']*np.sin(rad)])\n",
    "    \n",
    "    # Calcular coordenadas del background.\n",
    "    pts_bg_cart = np.array([to_cartesian(row) for _, row in bg.iterrows()])\n",
    "    \n",
    "    # Calcular densidad local para cada punto del background.\n",
    "    densities = np.array([local_density(pt, pts_bg_cart, radius=density_radius) for pt in pts_bg_cart])\n",
    "    global_mean_density = np.mean(densities) if len(densities) > 0 else 0\n",
    "\n",
    "    # Obtener la unión de las fronteras de los grupos finales.\n",
    "    boundary_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts_grp = grupo['points']\n",
    "        for _, row in pts_grp.iterrows():\n",
    "            boundary_list.append(to_cartesian(row))\n",
    "    if boundary_list:\n",
    "        boundary_all = np.vstack(boundary_list)\n",
    "    else:\n",
    "        boundary_all = np.empty((0,2))\n",
    "    \n",
    "    # Calcular promedio de distancia entre puntos (en coordenadas cartesianas)\n",
    "    if len(pts_bg_cart) < 2:\n",
    "        avg_nn = np.inf\n",
    "    else:\n",
    "        dists = cdist(pts_bg_cart, pts_bg_cart)\n",
    "        np.fill_diagonal(dists, np.inf)\n",
    "        avg_nn = np.mean(np.min(dists, axis=1))\n",
    "    threshold = min(tol_factor * avg_nn, 0.65)\n",
    "    \n",
    "    arms = []\n",
    "    \n",
    "    while not bg.empty:\n",
    "        # Semilla: punto con mayor θ.\n",
    "        seed_idx = bg['theta'].idxmax()\n",
    "        seed = bg.loc[seed_idx]\n",
    "        path = [seed]\n",
    "        bg = bg.drop(seed_idx).reset_index(drop=True)\n",
    "        current = seed\n",
    "        prev_direction = None\n",
    "        \n",
    "        while True:\n",
    "            candidates = bg[bg['theta'] < current['theta']]\n",
    "            if candidates.empty:\n",
    "                break\n",
    "            candidate_coords = np.array([to_cartesian(row) for _, row in candidates.iterrows()])\n",
    "            current_cart = to_cartesian(current)\n",
    "            dists_current = np.linalg.norm(candidate_coords - current_cart, axis=1)\n",
    "            if boundary_all.shape[0] > 0:\n",
    "                dists_boundary = np.array([np.min(np.linalg.norm(candidate - boundary_all, axis=1))\n",
    "                                           for candidate in candidate_coords])\n",
    "            else:\n",
    "                dists_boundary = np.full(len(candidates), np.inf)\n",
    "            effective_dists = np.minimum(dists_current, dists_boundary)\n",
    "            \n",
    "            # Calcular dirección hacia cada candidato.\n",
    "            directions = []\n",
    "            for coord in candidate_coords:\n",
    "                vec = coord - current_cart\n",
    "                angle = np.degrees(np.arctan2(vec[1], vec[0])) % 360\n",
    "                directions.append(angle)\n",
    "            directions = np.array(directions)\n",
    "            \n",
    "            # Filtrar candidatos que mantengan continuidad direccional.\n",
    "            if prev_direction is not None:\n",
    "                angle_diffs = np.abs((directions - prev_direction + 180) % 360 - 180)\n",
    "                valid_idx = np.where(angle_diffs < max_angle_diff)[0]\n",
    "                if len(valid_idx) == 0:\n",
    "                    break\n",
    "                effective_dists = effective_dists[valid_idx]\n",
    "                candidate_subset = candidates.iloc[valid_idx]\n",
    "                directions = directions[valid_idx]\n",
    "            else:\n",
    "                candidate_subset = candidates\n",
    "            \n",
    "            if len(effective_dists) == 0:\n",
    "                break\n",
    "            min_eff = np.min(effective_dists)\n",
    "            idx_min = np.argmin(effective_dists)\n",
    "            # Criterio de distancia.\n",
    "            if min_eff > threshold:\n",
    "                break\n",
    "            \n",
    "            # Criterio de densidad:\n",
    "            candidate = candidate_subset.iloc[idx_min]\n",
    "            candidate_cart = to_cartesian(candidate)\n",
    "            cand_density = local_density(candidate_cart, pts_bg_cart, radius=density_radius)\n",
    "            if cand_density < global_mean_density:\n",
    "                # Si la densidad local es menor que la media, se descarta este candidato.\n",
    "                # Se remueve el candidato y se continúa.\n",
    "                bg = bg.drop(candidate.name).reset_index(drop=True)\n",
    "                continue\n",
    "            \n",
    "            current_coord = to_cartesian(current)\n",
    "            candidate_coord = to_cartesian(candidate)\n",
    "            new_direction = np.degrees(np.arctan2(candidate_coord[1]-current_coord[1],\n",
    "                                                   candidate_coord[0]-current_coord[0])) % 360\n",
    "            \n",
    "            path.append(candidate)\n",
    "            prev_direction = new_direction\n",
    "            current = candidate\n",
    "            bg = bg.drop(candidate.name).reset_index(drop=True)\n",
    "        \n",
    "        arms.append(pd.DataFrame(path))\n",
    "    \n",
    "    # Calcular longitud de cada camino.\n",
    "    def path_length(df_path):\n",
    "        pts = np.array([to_cartesian(row) for _, row in df_path.iterrows()])\n",
    "        if len(pts) < 2:\n",
    "            return 0\n",
    "        return np.sum(np.linalg.norm(pts[1:] - pts[:-1], axis=1))\n",
    "    \n",
    "    lengths = np.array([path_length(arm) for arm in arms])\n",
    "    if len(lengths) == 0:\n",
    "        return arms\n",
    "    median_length = np.median(lengths)\n",
    "    # Se filtran y retornan únicamente los caminos cuya longitud es mayor o igual a la mediana.\n",
    "    filtered_arms = [arm for arm, L in zip(arms, lengths) if L >= median_length]\n",
    "    return filtered_arms\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Función de graficado final: mostrar caminos con background y grupos finales\n",
    "# =============================================================================\n",
    "\n",
    "def plot_arms_with_background(arms, background, grupos_finales, title=\"Caminos (raíces) con Background\"):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(background['theta'], background['r'], c='gray', s=3, alpha=0.1, label=\"Background\")\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','lime']\n",
    "    for i, grupo in enumerate(grupos_finales):\n",
    "        pts = grupo['points']\n",
    "        c = colors[i % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, alpha=0.1, label=f\"Grupo {i+1}\")\n",
    "    for j, arm in enumerate(arms):\n",
    "        plt.plot(arm['theta'], arm['r'], '-o', label=f\"Camino {j+1}\", linewidth=2)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: Ejecución de la pipeline completa\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paso 1: Procesar y agrupar los datos.\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "    # Paso 2: Trazar caminos (\"raíces\") con criterio de densidad y dirección.\n",
    "    if not background.empty and len(grupos_finales) > 0:\n",
    "        arms = trace_spiral_arms(background, grupos_finales, tol_factor=1.1, max_angle_diff=10, density_radius=0.5)\n",
    "        print(f\"Se trazaron {len(arms)} caminos (raíces) que cumplen los criterios.\")\n",
    "        # Paso 3: Graficar los caminos (solo los de mayor longitud) junto con background y grupos finales.\n",
    "        plot_arms_with_background(arms, background, grupos_finales, title=\"Caminos largos (raíces) con Background\")\n",
    "    else:\n",
    "        print(\"No hay datos suficientes en el background o en los grupos finales para trazar caminos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a8a29a-1393-4547-8140-2b8b55d06495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Ejemplo de conexión de grupos mediante una malla de densidad SPH.\n",
    "Se crea una grilla sobre el dominio, se interpola la densidad usando un kernel SPH,\n",
    "se extraen isocontornos (los pozos densos) y se “conectan” sus fronteras\n",
    "buscando en la malla siempre el vecino más cercano.\n",
    "Donde la densidad cae a cero se interpreta como un corte natural (aislamiento).\n",
    "Se grafican los caminos más largos encontrados.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# --- Definir kernel SPH (cúbico spline en 2D) ---\n",
    "def cubic_spline_kernel(r, h):\n",
    "    q = r/h\n",
    "    sigma = 10.0/(7.0*np.pi*h**2)  # normalización en 2D\n",
    "    if q >= 0 and q <= 1:\n",
    "        return sigma * (1 - 1.5*q**2 + 0.75*q**3)\n",
    "    elif q > 1 and q <= 2:\n",
    "        return sigma * (0.25 * (2 - q)**3)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# --- Interpolar la densidad en una malla ---\n",
    "def compute_density_grid(points_cart, grid_x, grid_y, h=0.5):\n",
    "    \"\"\"\n",
    "    points_cart: array (N,2) con coordenadas cartesianas de todos los puntos.\n",
    "    grid_x, grid_y: matrices resultantes de np.meshgrid.\n",
    "    h: longitud de suavizado.\n",
    "    Retorna la densidad evaluada en cada nodo de la malla.\n",
    "    \"\"\"\n",
    "    density = np.zeros(grid_x.shape)\n",
    "    for pt in points_cart:\n",
    "        # Calcular la distancia desde cada nodo al punto pt\n",
    "        d = np.sqrt((grid_x - pt[0])**2 + (grid_y - pt[1])**2)\n",
    "        # Sumar la contribución del kernel\n",
    "        density += np.vectorize(cubic_spline_kernel)(d, h)\n",
    "    return density\n",
    "\n",
    "# --- Extracción y conexión de fronteras ---\n",
    "def extract_and_connect_paths(density, grid_x, grid_y, density_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Dado un campo de densidad evaluado en la malla (grid_x, grid_y),\n",
    "    extrae los contornos donde la densidad es igual a density_threshold\n",
    "    y simula la conexión entre los contornos (usando el camino de distancia mínima).\n",
    "    \n",
    "    Esta función es ilustrativa; en una implementación real se podría usar\n",
    "    watershed o algoritmos de grafos sobre la malla.\n",
    "    \n",
    "    Retorna una lista de caminos, cada uno como array de coordenadas (x,y).\n",
    "    \"\"\"\n",
    "    # Utilizamos matplotlib para extraer contornos:\n",
    "    cs = plt.contour(grid_x, grid_y, density, levels=[density_threshold])\n",
    "    paths = []\n",
    "    # Extraer coordenadas de cada contorno\n",
    "    for collection in cs.collections:\n",
    "        for path in collection.get_paths():\n",
    "            v = path.vertices\n",
    "            paths.append(v)\n",
    "    plt.clf()  # limpiar la figura de contornos\n",
    "    # Aquí se puede aplicar un criterio para conectar contornos cercanos\n",
    "    # Por ejemplo, para cada contorno, buscar el contorno vecino cuyo punto\n",
    "    # esté a menos de una distancia crítica (p.ej. 0.65) y unirlos.\n",
    "    # En este ejemplo se devuelven directamente los contornos.\n",
    "    return paths\n",
    "\n",
    "# --- Ejemplo de uso: Crear malla, calcular densidad y extraer caminos ---\n",
    "def main_sph_path():\n",
    "    # Simulamos algunos puntos (por ejemplo, del background y fronteras de grupos)\n",
    "    # Supongamos que points_cart es un array (N,2) con coordenadas cartesianas.\n",
    "    # En un caso real usarías la información combinada de los grupos y background.\n",
    "    np.random.seed(0)\n",
    "    # Crear puntos distribuidos en dos pozos densos:\n",
    "    n1, n2 = 200, 150\n",
    "    points1 = np.random.normal(loc=[2,2], scale=0.2, size=(n1,2))\n",
    "    points2 = np.random.normal(loc=[4,4], scale=0.3, size=(n2,2))\n",
    "    points_cart = np.vstack([points1, points2])\n",
    "    \n",
    "    # Definir la malla que cubra el dominio\n",
    "    xmin, xmax = points_cart[:,0].min()-1, points_cart[:,0].max()+1\n",
    "    ymin, ymax = points_cart[:,1].min()-1, points_cart[:,1].max()+1\n",
    "    n_grid = 200\n",
    "    x = np.linspace(xmin, xmax, n_grid)\n",
    "    y = np.linspace(ymin, ymax, n_grid)\n",
    "    grid_x, grid_y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Calcular la densidad en la malla usando el kernel SPH.\n",
    "    h = 0.5\n",
    "    density = compute_density_grid(points_cart, grid_x, grid_y, h=h)\n",
    "    \n",
    "    # Opcional: Suavizar la densidad (para obtener contornos más continuos)\n",
    "    density_smooth = gaussian_filter(density, sigma=1)\n",
    "    \n",
    "    # Extraer contornos (fronteras densas) donde la densidad es mayor a un umbral.\n",
    "    density_threshold = np.percentile(density_smooth, 90)  # por ejemplo, el 90% superior\n",
    "    paths = extract_and_connect_paths(density_smooth, grid_x, grid_y, density_threshold=density_threshold)\n",
    "    \n",
    "    # Para este ejemplo, consideramos como caminos los contornos extraídos y\n",
    "    # seleccionamos los de mayor longitud.\n",
    "    path_lengths = [np.sum(np.linalg.norm(path[1:]-path[:-1], axis=1)) for path in paths]\n",
    "    if len(path_lengths)==0:\n",
    "        print(\"No se han encontrado caminos.\")\n",
    "        return\n",
    "    median_length = np.median(path_lengths)\n",
    "    selected_paths = [path for path, L in zip(paths, path_lengths) if L >= median_length]\n",
    "    \n",
    "    # Graficar: mostrar la malla de densidad y los caminos\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(density_smooth, extent=[xmin, xmax, ymin, ymax], origin='lower', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad')\n",
    "    for path in selected_paths:\n",
    "        plt.plot(path[:,0], path[:,1], '-o', linewidth=2)\n",
    "    plt.scatter(points_cart[:,0], points_cart[:,1], c='cyan', s=10, label='Puntos')\n",
    "    plt.title(\"Malla de densidad SPH y caminos densos conectados\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    paths = main_sph_path()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b4d1ab-b2e0-4037-8585-01c6e569aea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pipeline completo usando datos reales para:\n",
    "  1. Procesar y agrupar los datos (a partir de CSV, por ejemplo, data_rho_17_filtered.csv).\n",
    "  2. Del conjunto background (puntos no asignados a ningún grupo final) se crea una malla\n",
    "     y se interpola la densidad mediante un kernel SPH. Se extraen isocontornos\n",
    "     (los pozos densos) y se usan para rastrear caminos que conecten las fronteras de los grupos.\n",
    "  3. Se grafican únicamente los caminos (los de mayor longitud) superpuestos al mapa de densidad,\n",
    "     con los puntos originales y los grupos finales mostrados con baja opacidad (α = 0.1).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# =============================================================================\n",
    "# 0) Funciones del pipeline de agrupación (usa tus datos reales)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope*(180/np.pi))/intercept))\n",
    "    return np.nan\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['id'] = df.index\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i]-th)<=theta_diff) & (np.abs(rr[i]-rr)<=r_diff) & (np.arange(n)!=i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    col = 'theta' if mode=='theta' else 'r'\n",
    "    df = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif<=gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d>gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts/ref,1.0))\n",
    "\n",
    "def generate_bfs_seeds(id_halo, theta_min, theta_max,\n",
    "                         quartile_threshold=0.55,\n",
    "                         theta_diff=3.0, r_diff=0.5,\n",
    "                         gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "                         file_prefix='data_rho'):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp']>thr].copy().reset_index(drop=True)\n",
    "    \n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "    \n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster)<2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa = calculate_pa(s,b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.40, bounding_extrap=0.30):\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                                 (*fit_line_to_cluster(cl), cl))))\n",
    "    def recalc(g):\n",
    "        g['slope'],g['intercept'],g['pa'],g['theta_min'],g['theta_max'],g['r_min'],g['r_max'] = fit_line_to_cluster(g['points'])\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0,t1,r0,r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'],g1['theta_max'],g1['r_min'],g1['r_max'])\n",
    "        b = exp(g2['theta_min'],g2['theta_max'],g2['r_min'],g2['r_max'])\n",
    "        return not (a[1]<b[0] or b[1]<a[0]) and not (a[3]<b[2] or b[3]<a[2])\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb-g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb-g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new.append(g1)\n",
    "            i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g)\n",
    "            continue\n",
    "        res = g['points']['r'] - (g['slope']*g['points']['theta']+g['intercept'])\n",
    "        (todo if res.std()>dispersion_threshold else ok).append(g)\n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                                  (*fit_line_to_cluster(c), c))))\n",
    "    return ok + new\n",
    "\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, alpha=0.3, label=f\"Grupo {i+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max']-g['theta_min']\n",
    "            t = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            plt.plot(t, g['slope']*t+g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – Grupos (≥ 60 pts) - Cartesiano\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c, alpha=0.3, label=f\"Grupo {i+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max']-g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            r_line = g['slope']*t_deg+g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – Grupos (≥ 60 pts) - Polar\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20,1.0))\n",
    "    plt.show()\n",
    "\n",
    "def main_example_modificado():\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "    \n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "    \n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "    \n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "    \n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Funciones para trazado de caminos (\"raíces\") conectados basados en densidad SPH\n",
    "# =============================================================================\n",
    "\n",
    "def cubic_spline_kernel(r, h):\n",
    "    q = r/h\n",
    "    sigma = 10.0/(7.0*np.pi*h**2)\n",
    "    if 0 <= q <= 1:\n",
    "        return sigma*(1 - 1.5*q**2 + 0.75*q**3)\n",
    "    elif 1 < q <= 2:\n",
    "        return sigma*0.25*(2 - q)**3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def compute_density_grid(points_cart, grid_x, grid_y, h=0.5):\n",
    "    density = np.zeros(grid_x.shape)\n",
    "    for pt in points_cart:\n",
    "        d = np.sqrt((grid_x - pt[0])**2 + (grid_y - pt[1])**2)\n",
    "        density += np.vectorize(cubic_spline_kernel)(d, h)\n",
    "    return density\n",
    "\n",
    "def trace_spiral_arms(background, grupos_finales, tol_factor=1.1, max_angle_diff=10, h=0.5, density_radius=0.5):\n",
    "    \"\"\"\n",
    "    Genera caminos (\"raíces\") usando un criterio que combine la conexión física (distancia y ángulo)\n",
    "    y la condición de que el camino transcurra en zonas de alta densidad (según SPH).\n",
    "    \n",
    "    Se calcula la densidad en el background mediante un kernel SPH,\n",
    "    y se descartan candidatos cuya densidad sea menor que la media global.\n",
    "    Al final se filtran y solo se retornan los caminos (raíz) de longitud mayor o igual a la mediana.\n",
    "    \"\"\"\n",
    "    bg = background.copy().sort_values(\"theta\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    def to_cartesian(row):\n",
    "        rad = np.radians(row['theta'])\n",
    "        return np.array([row['r']*np.cos(rad), row['r']*np.sin(rad)])\n",
    "    \n",
    "    pts_bg_cart = np.array([to_cartesian(row) for _, row in bg.iterrows()])\n",
    "    densities = np.array([0.0]*len(pts_bg_cart))\n",
    "    for i in range(len(pts_bg_cart)):\n",
    "        d = cdist(pts_bg_cart[i].reshape(1,-1), pts_bg_cart)\n",
    "        # Se suma el kernel para cada punto; masa = 1.\n",
    "        densities[i] = np.sum(np.vectorize(cubic_spline_kernel)(d, h))\n",
    "    \n",
    "    global_mean_density = np.mean(densities)\n",
    "    bg = bg.assign(density=densities)\n",
    "    \n",
    "    if len(pts_bg_cart) < 2:\n",
    "        avg_nn = np.inf\n",
    "    else:\n",
    "        dists = cdist(pts_bg_cart, pts_bg_cart)\n",
    "        np.fill_diagonal(dists, np.inf)\n",
    "        avg_nn = np.mean(np.min(dists, axis=1))\n",
    "    connection_threshold = min(tol_factor * avg_nn, 0.65)\n",
    "    \n",
    "    # Obtener la unión de las fronteras de los grupos finales.\n",
    "    boundary_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts_grp = grupo['points']\n",
    "        for _, row in pts_grp.iterrows():\n",
    "            boundary_list.append(to_cartesian(row))\n",
    "    if boundary_list:\n",
    "        boundary_all = np.vstack(boundary_list)\n",
    "    else:\n",
    "        boundary_all = np.empty((0,2))\n",
    "    \n",
    "    arms = []\n",
    "    \n",
    "    while not bg.empty:\n",
    "        seed_idx = bg['theta'].idxmax()\n",
    "        seed = bg.loc[seed_idx]\n",
    "        # Usamos solo semillas con densidad ≥ global_mean_density.\n",
    "        if seed['density'] < global_mean_density:\n",
    "            bg = bg.drop(seed_idx).reset_index(drop=True)\n",
    "            continue\n",
    "        path = [seed]\n",
    "        bg = bg.drop(seed_idx).reset_index(drop=True)\n",
    "        current = seed\n",
    "        prev_direction = None\n",
    "        \n",
    "        while True:\n",
    "            candidates = bg[bg['theta'] < current['theta']]\n",
    "            if candidates.empty:\n",
    "                break\n",
    "            candidate_coords = np.array([to_cartesian(row) for _, row in candidates.iterrows()])\n",
    "            current_cart = to_cartesian(current)\n",
    "            dists_current = np.linalg.norm(candidate_coords - current_cart, axis=1)\n",
    "            if boundary_all.shape[0] > 0:\n",
    "                dists_boundary = np.array([np.min(np.linalg.norm(candidate - boundary_all, axis=1))\n",
    "                                           for candidate in candidate_coords])\n",
    "            else:\n",
    "                dists_boundary = np.full(len(candidates), np.inf)\n",
    "            effective_dists = np.minimum(dists_current, dists_boundary)\n",
    "            directions = []\n",
    "            for coord in candidate_coords:\n",
    "                vec = coord - current_cart\n",
    "                angle = np.degrees(np.arctan2(vec[1], vec[0])) % 360\n",
    "                directions.append(angle)\n",
    "            directions = np.array(directions)\n",
    "            if prev_direction is not None:\n",
    "                angle_diffs = np.abs((directions - prev_direction + 180) % 360 - 180)\n",
    "                valid_idx = np.where(angle_diffs < max_angle_diff)[0]\n",
    "                if len(valid_idx) == 0:\n",
    "                    break\n",
    "                effective_dists = effective_dists[valid_idx]\n",
    "                candidate_subset = candidates.iloc[valid_idx]\n",
    "                directions = directions[valid_idx]\n",
    "            else:\n",
    "                candidate_subset = candidates\n",
    "            \n",
    "            if len(effective_dists)==0:\n",
    "                break\n",
    "            min_eff = np.min(effective_dists)\n",
    "            idx_min = np.argmin(effective_dists)\n",
    "            if min_eff > connection_threshold:\n",
    "                break\n",
    "            candidate = candidate_subset.iloc[idx_min]\n",
    "            # Rechazar candidatos con densidad menor a la media.\n",
    "            if candidate['density'] < global_mean_density:\n",
    "                bg = bg.drop(candidate.name).reset_index(drop=True)\n",
    "                continue\n",
    "            current_coord = to_cartesian(current)\n",
    "            candidate_coord = to_cartesian(candidate)\n",
    "            new_direction = np.degrees(np.arctan2(candidate_coord[1]-current_coord[1],\n",
    "                                                   candidate_coord[0]-current_coord[0])) % 360\n",
    "            path.append(candidate)\n",
    "            prev_direction = new_direction\n",
    "            current = candidate\n",
    "            bg = bg.drop(candidate.name).reset_index(drop=True)\n",
    "        \n",
    "        arms.append(pd.DataFrame(path))\n",
    "    \n",
    "    # Filtrar para quedarse solo con los caminos largos.\n",
    "    def path_length(df_path):\n",
    "        pts = np.array([to_cartesian(row) for _, row in df_path.iterrows()])\n",
    "        if len(pts) < 2:\n",
    "            return 0\n",
    "        return np.sum(np.linalg.norm(pts[1:]-pts[:-1], axis=1))\n",
    "    \n",
    "    lengths = np.array([path_length(arm) for arm in arms])\n",
    "    if len(lengths)==0:\n",
    "        return arms\n",
    "    median_length = np.median(lengths)\n",
    "    filtered_arms = [arm for arm, L in zip(arms, lengths) if L >= median_length]\n",
    "    return filtered_arms\n",
    "\n",
    "# =============================================================================\n",
    "# Función de graficado final\n",
    "# =============================================================================\n",
    "\n",
    "def plot_arms_with_background(arms, background, grupos_finales, title=\"Caminos densos conectados\"):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(background['theta'], background['r'], c='gray', s=3, alpha=0.1, label=\"Background\")\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','lime']\n",
    "    for i, grupo in enumerate(grupos_finales):\n",
    "        pts = grupo['points']\n",
    "        c = colors[i % len(colors)]\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, alpha=0.1, label=f\"Grupo {i+1}\")\n",
    "    for j, arm in enumerate(arms):\n",
    "        plt.plot(arm['theta'], arm['r'], '-o', label=f\"Camino {j+1}\", linewidth=2)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: Ejecución de la pipeline completa usando datos reales.\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Primero se ejecuta el pipeline de agrupación real.\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "    if not background.empty and len(grupos_finales) > 0:\n",
    "        arms = trace_spiral_arms(background, grupos_finales, tol_factor=1.1, max_angle_diff=10, h=0.5, density_radius=0.5)\n",
    "        print(f\"Se trazaron {len(arms)} caminos densos conectados.\")\n",
    "        plot_arms_with_background(arms, background, grupos_finales, title=\"Caminos densos conectados con datos reales\")\n",
    "    else:\n",
    "        print(\"No hay datos suficientes en el background o en los grupos finales para trazar caminos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc916279-4d9f-4e35-94e4-939273f476ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Ejemplo de conexión de grupos mediante una malla de densidad SPH, usando datos reales.\n",
    "Se crea una grilla sobre el dominio de los datos (background y grupos finales), se interpola\n",
    "la densidad usando un kernel SPH, se extraen isocontornos (los pozos densos) y se muestran\n",
    "los caminos (contornos) de mayor longitud.\n",
    "Se usa como fuente de datos:\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import pandas as pd\n",
    "\n",
    "# --- Función SPH: Kernel cúbico spline en 2D ---\n",
    "def cubic_spline_kernel(r, h):\n",
    "    q = r/h\n",
    "    sigma = 10.0/(7.0*np.pi*h**2)  # normalización en 2D\n",
    "    if q >= 0 and q <= 1:\n",
    "        return sigma * (1 - 1.5*q**2 + 0.75*q**3)\n",
    "    elif q > 1 and q <= 2:\n",
    "        return sigma * (0.25 * (2 - q)**3)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# --- Función para interpolar la densidad en una malla ---\n",
    "def compute_density_grid(points_cart, grid_x, grid_y, h=0.5):\n",
    "    \"\"\"\n",
    "    points_cart: array (N,2) con coordenadas cartesianas.\n",
    "    grid_x, grid_y: matrices resultantes de np.meshgrid.\n",
    "    h: radio de suavizado.\n",
    "    Retorna la densidad evaluada en cada nodo de la malla.\n",
    "    \"\"\"\n",
    "    density = np.zeros(grid_x.shape)\n",
    "    # Vectorizamos el kernel para mayor eficiencia:\n",
    "    kernel_func = np.vectorize(cubic_spline_kernel)\n",
    "    for pt in points_cart:\n",
    "        d = np.sqrt((grid_x - pt[0])**2 + (grid_y - pt[1])**2)\n",
    "        density += kernel_func(d, h)\n",
    "    return density\n",
    "\n",
    "# --- Función para extraer isocontornos (caminos densos) ---\n",
    "def extract_and_connect_paths(density, grid_x, grid_y, density_threshold):\n",
    "    \"\"\"\n",
    "    Dado un campo de densidad sobre la malla (grid_x, grid_y), extrae los isocontornos\n",
    "    donde la densidad es igual a density_threshold.\n",
    "    \n",
    "    Retorna una lista de caminos, cada uno como un array de coordenadas (x, y).\n",
    "    \"\"\"\n",
    "    cs = plt.contour(grid_x, grid_y, density, levels=[density_threshold])\n",
    "    paths = []\n",
    "    for collection in cs.collections:\n",
    "        for path in collection.get_paths():\n",
    "            v = path.vertices\n",
    "            paths.append(v)\n",
    "    plt.clf()  # Limpiar la figura\n",
    "    return paths\n",
    "\n",
    "# --- Función principal que usa datos reales (del pipeline) ---\n",
    "def main_sph_path_reales():\n",
    "    # Cargar datos reales: se usa el pipeline anterior.\n",
    "    # Asegúrate de que la función main_example_modificado() ya se encuentre definida.\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "    # Para definir la malla, usamos la unión de background y fronteras de los grupos.\n",
    "    # Extraer puntos del background:\n",
    "    bg_points = background[['x','y']] if 'x' in background.columns and 'y' in background.columns else None\n",
    "    if bg_points is None:\n",
    "        # Si no existen \"x\" e \"y\", convertir a cartesianas a partir de \"theta\" y \"r\":\n",
    "        def pol2cart(df):\n",
    "            rad = np.radians(df['theta'])\n",
    "            df = df.copy()\n",
    "            df['x'] = df['r'] * np.cos(rad)\n",
    "            df['y'] = df['r'] * np.sin(rad)\n",
    "            return df[['x','y']]\n",
    "        bg_points = pol2cart(background)\n",
    "    \n",
    "    # Extraer puntos de los grupos finales (se supone que estos DataFrames ya tienen x,y o se convierten)\n",
    "    group_points = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts = grupo['points']\n",
    "        if not {'x','y'}.issubset(set(pts.columns)):\n",
    "            # Convertir a cartesianas\n",
    "            pts = pts.copy()\n",
    "            rad = np.radians(pts['theta'])\n",
    "            pts['x'] = pts['r']*np.cos(rad)\n",
    "            pts['y'] = pts['r']*np.sin(rad)\n",
    "        group_points.append(pts[['x','y']].values)\n",
    "    if group_points:\n",
    "        group_points = np.vstack(group_points)\n",
    "    else:\n",
    "        group_points = np.empty((0,2))\n",
    "    \n",
    "    # Combinar puntos: background y fronteras de grupos.\n",
    "    all_points = np.vstack([bg_points.values, group_points])\n",
    "    \n",
    "    # Definir la malla:\n",
    "    xmin, xmax = all_points[:,0].min()-1, all_points[:,0].max()+1\n",
    "    ymin, ymax = all_points[:,1].min()-1, all_points[:,1].max()+1\n",
    "    n_grid = 300  # resolución de la grilla\n",
    "    x = np.linspace(xmin, xmax, n_grid)\n",
    "    y = np.linspace(ymin, ymax, n_grid)\n",
    "    grid_x, grid_y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Calcular la densidad en la malla con SPH:\n",
    "    h = 0.5  # radio de suavizado\n",
    "    density = compute_density_grid(all_points, grid_x, grid_y, h=h)\n",
    "    density_smooth = gaussian_filter(density, sigma=1)\n",
    "    \n",
    "    # Elegir un umbral de densidad: por ejemplo, el percentil 90 de la densidad suavizada.\n",
    "    density_threshold = np.percentile(density_smooth, 90)\n",
    "    \n",
    "    # Extraer contornos (caminos) donde la densidad = density_threshold.\n",
    "    paths = extract_and_connect_paths(density_smooth, grid_x, grid_y, density_threshold)\n",
    "    \n",
    "    # Seleccionar los caminos más largos (por ejemplo, aquellos cuya longitud sea ≥ mediana de longitudes)\n",
    "    def path_length(path):\n",
    "        return np.sum(np.linalg.norm(path[1:]-path[:-1], axis=1))\n",
    "    lengths = np.array([path_length(p) for p in paths])\n",
    "    if len(lengths)==0:\n",
    "        print(\"No se encontraron caminos.\")\n",
    "        return\n",
    "    median_length = np.median(lengths)\n",
    "    selected_paths = [p for p, L in zip(paths, lengths) if L >= median_length]\n",
    "    \n",
    "    # Graficar la malla de densidad y los caminos\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(density_smooth, extent=[xmin, xmax, ymin, ymax], origin='lower', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad')\n",
    "    for p in selected_paths:\n",
    "        plt.plot(p[:,0], p[:,1], '-o', linewidth=2)\n",
    "    plt.scatter(all_points[:,0], all_points[:,1], c='cyan', s=0.2, label='Puntos', alpha=0.3)\n",
    "    plt.title(\"Malla de densidad SPH y caminos densos conectados (datos reales)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    paths = main_sph_path_reales()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a969a190-8461-400a-a1bd-1b9c7174fd1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e8dd0b5-ef9d-4dfb-912a-5db99aadb78f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ultimo intento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2f6906-f052-49da-a699-8be945fddf35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Código completo para:\n",
    "  1. Procesar y agrupar los datos reales (desde CSV) mediante el pipeline de agrupación.\n",
    "  2. Crear una malla en el espacio (θ, r) y calcular la densidad SPH con pesos diferenciados:\n",
    "     se asigna mayor masa a los puntos que pertenecen a los grupos finales (pozos densos) y masa 1\n",
    "     al background.\n",
    "  3. Extraer isocontornos (caminos) en la malla en función de la densidad, filtrando sólo aquellos\n",
    "     caminos de mayor longitud (por encima de la mediana).\n",
    "  4. Graficar los caminos sobre la malla en el espacio (θ, r) sin barra de color.\n",
    "  \n",
    "Asegúrate de que el archivo de datos (por ejemplo, data_rho_17_filtered.csv) y todo el entorno\n",
    "se encuentren en el mismo directorio.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Pipeline de Agrupación (ya existente)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pa(slope, intercept):\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope*(180/np.pi))/intercept))\n",
    "    return np.nan\n",
    "\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['id'] = df.index\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i]-th)<=theta_diff) & (np.abs(rr[i]-rr)<=r_diff) & (np.arange(n)!=i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    col = 'theta' if mode=='theta' else 'r'\n",
    "    df = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif<=gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d>gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts/ref,1.0))\n",
    "\n",
    "def generate_bfs_seeds(id_halo, theta_min, theta_max,\n",
    "                         quartile_threshold=0.155,\n",
    "                         theta_diff=3.0, r_diff=0.5,\n",
    "                         gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "                         file_prefix='data_rho'):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp']>thr].copy().reset_index(drop=True)\n",
    "    \n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "    \n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster)<2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa = calculate_pa(s,b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.30, bounding_extrap=0.35):\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                                 (*fit_line_to_cluster(cl), cl))))\n",
    "    def recalc(g):\n",
    "        g['slope'],g['intercept'],g['pa'],g['theta_min'],g['theta_max'],g['r_min'],g['r_max'] = fit_line_to_cluster(g['points'])\n",
    "    def boxes_overlap(g1,g2,e):\n",
    "        def exp(t0,t1,r0,r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'],g1['theta_max'],g1['r_min'],g1['r_max'])\n",
    "        b = exp(g2['theta_min'],g2['theta_max'],g2['r_min'],g2['r_max'])\n",
    "        return not (a[1]<b[0] or b[1]<a[0]) and not (a[3]<b[2] or b[3]<a[2])\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1,g2,bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new.append(g1)\n",
    "            i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points'])>=2]\n",
    "\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points'])<2:\n",
    "            ok.append(g)\n",
    "            continue\n",
    "        res = g['points']['r'] - (g['slope']*g['points']['theta']+g['intercept'])\n",
    "        (todo if res.std()>dispersion_threshold else ok).append(g)\n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                                  (*fit_line_to_cluster(c), c))))\n",
    "    return ok + new\n",
    "\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points'])>=60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i%len(colors)], g['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, alpha=0.3, label=f\"Grupo {i+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max']-g['theta_min']\n",
    "            t = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            plt.plot(t, g['slope']*t+g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – Grupos (≥ 60 pts) - Cartesiano\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points'])>=60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c, alpha=0.3, label=f\"Grupo {i+1}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max']-g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            r_line = g['slope']*t_deg+g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – Grupos (≥ 60 pts) - Polar\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20,1.0))\n",
    "    plt.show()\n",
    "\n",
    "def main_example_modificado():\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"17\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "    \n",
    "    grupos_finales = [g for g in validated if len(g['points'])>=60]\n",
    "    \n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "    \n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "    \n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Funciones SPH en el espacio (θ, r) y trazado de caminos densos\n",
    "# =============================================================================\n",
    "\n",
    "def cubic_spline_kernel(r, h):\n",
    "    q = r / h\n",
    "    sigma = 10.0/(7.0*np.pi*h**2)\n",
    "    if 0 <= q <= 1:\n",
    "        return sigma*(1 - 1.5*q**2 + 0.75*q**3)\n",
    "    elif 1 < q <= 2:\n",
    "        return sigma*0.25*(2 - q)**3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def compute_density_grid(points_pr, grid_theta, grid_r, masses, h=5.0):\n",
    "    \"\"\"\n",
    "    points_pr: array (N,2) con [θ, r] (θ en grados, r en sus unidades).\n",
    "    masses: array (N,) con la masa (peso) de cada punto.\n",
    "    grid_theta, grid_r: malla en el espacio (θ, r).\n",
    "    h: radio de suavizado.\n",
    "    Retorna la densidad evaluada en cada nodo.\n",
    "    \"\"\"\n",
    "    density = np.zeros(grid_theta.shape)\n",
    "    kernel_func = np.vectorize(cubic_spline_kernel)\n",
    "    for i, pt in enumerate(points_pr):\n",
    "        d = np.sqrt((grid_theta - pt[0])**2 + (grid_r - pt[1])**2)\n",
    "        density += masses[i] * kernel_func(d, h)\n",
    "    return density\n",
    "\n",
    "def extract_paths(density, grid_theta, grid_r, density_threshold):\n",
    "    cs = plt.contour(grid_theta, grid_r, density, levels=[density_threshold])\n",
    "    paths = []\n",
    "    for collection in cs.collections:\n",
    "        for path in collection.get_paths():\n",
    "            v = path.vertices\n",
    "            paths.append(v)\n",
    "    plt.clf()\n",
    "    return paths\n",
    "\n",
    "def main_sph_path_reales():\n",
    "    \"\"\"\n",
    "    Usa los datos reales del pipeline para construir la malla en (θ, r), calcular la densidad\n",
    "    con masa diferenciada (grupos finales tienen mayor masa) y extraer los caminos (isocontornos)\n",
    "    de mayor longitud.\n",
    "    \"\"\"\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "    # Extraer puntos en (θ, r)\n",
    "    bg_points = background[['theta','r']].values\n",
    "    bg_masses = np.ones(len(bg_points))  # masa 1 para background\n",
    "    group_points_list = []\n",
    "    group_masses_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts = grupo['points'][['theta','r']].values\n",
    "        group_points_list.append(pts)\n",
    "        # Asignar mayor masa, por ejemplo 5, a los puntos de grupos finales\n",
    "        group_masses_list.append(5 * np.ones(len(pts)))\n",
    "    if group_points_list:\n",
    "        group_points = np.vstack(group_points_list)\n",
    "        group_masses = np.hstack(group_masses_list)\n",
    "    else:\n",
    "        group_points = np.empty((0,2))\n",
    "        group_masses = np.empty((0,))\n",
    "    \n",
    "    all_points = np.vstack([bg_points, group_points])\n",
    "    all_masses = np.hstack([bg_masses, group_masses])\n",
    "    \n",
    "    # Definir la malla en (θ, r)\n",
    "    theta_min, theta_max = all_points[:,0].min()-5, all_points[:,0].max()+5\n",
    "    r_min, r_max = all_points[:,1].min()-1, all_points[:,1].max()+1\n",
    "    n_theta, n_r = 300, 300\n",
    "    theta_vals = np.linspace(theta_min, theta_max, n_theta)\n",
    "    r_vals = np.linspace(r_min, r_max, n_r)\n",
    "    grid_theta, grid_r = np.meshgrid(theta_vals, r_vals)\n",
    "    \n",
    "    # Calcular la densidad en la malla con SPH\n",
    "    h = 5.0\n",
    "    density = compute_density_grid(all_points, grid_theta, grid_r, all_masses, h=h)\n",
    "    density_smooth = gaussian_filter(density, sigma=1)\n",
    "    \n",
    "    # Elegir umbral de densidad, por ejemplo el percentil 90 de la densidad suave.\n",
    "    density_threshold = np.percentile(density_smooth, 90)\n",
    "    paths = extract_paths(density_smooth, grid_theta, grid_r, density_threshold)\n",
    "    \n",
    "    # Filtrar caminos por longitud (calcular la longitud en (θ, r))\n",
    "    def path_length(path):\n",
    "        return np.sum(np.linalg.norm(path[1:]-path[:-1], axis=1))\n",
    "    lengths = np.array([path_length(p) for p in paths])\n",
    "    if len(lengths)==0:\n",
    "        print(\"No se encontraron caminos.\")\n",
    "        return\n",
    "    median_length = np.median(lengths)\n",
    "    selected_paths = [p for p, L in zip(paths, lengths) if L >= median_length]\n",
    "    \n",
    "    # Graficar la malla de densidad y los caminos en (θ, r)\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.imshow(density_smooth, extent=[theta_min, theta_max, r_min, r_max],\n",
    "               origin='lower', cmap='inferno', aspect='auto')\n",
    "    # No mostramos colorbar para mayor claridad\n",
    "    for p in selected_paths:\n",
    "        plt.plot(p[:,0], p[:,1], '-o', linewidth=2)\n",
    "    plt.scatter(all_points[:,0], all_points[:,1], c='cyan', s=0.4, label='Puntos', alpha=0.3)\n",
    "    plt.title(\"Malla de densidad SPH y caminos densos (espacio (θ, r))\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    paths = main_sph_path_reales()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be710beb-e84a-4129-ab1f-f46e034d5af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Ejemplo de conexión de grupos mediante una malla de densidad SPH en el espacio (θ, r).\n",
    "\n",
    "Se crea una grilla en el espacio de parámetros (θ, r) de los datos reales (background y grupos finales),\n",
    "se interpola la densidad usando un kernel SPH (calculado en (θ, r)) y se extraen los isocontornos\n",
    "(de mayor longitud, filtrados por la mediana) para representar los caminos o “fronteras densas”.\n",
    "\n",
    "Se usan como datos fuente:\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "\n",
    "La visualización se realiza en un tamaño amplio y sin colorbar.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import pandas as pd\n",
    "\n",
    "# --- Función SPH: Kernel cúbico spline en 2D, aplicado en (θ, r) ---\n",
    "def cubic_spline_kernel(r, h):\n",
    "    q = r/h\n",
    "    sigma = 10.0/(7.0*np.pi*h**2)  # normalización en 2D (puedes ajustar este valor según la escala de θ y r)\n",
    "    if 0 <= q <= 1:\n",
    "        return sigma * (1 - 1.5*q**2 + 0.75*q**3)\n",
    "    elif 1 < q <= 2:\n",
    "        return sigma * (0.25 * (2 - q)**3)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# --- Función para interpolar la densidad en una malla en (θ, r) ---\n",
    "def compute_density_grid(points_pr, grid_theta, grid_r, h=5.0):\n",
    "    \"\"\"\n",
    "    points_pr: array (N,2) con columnas [θ, r] (θ en grados, r en sus unidades).\n",
    "    grid_theta, grid_r: matrices resultantes de np.meshgrid en (θ, r).\n",
    "    h: radio de suavizado (en unidades de θ y r).\n",
    "    Retorna la densidad evaluada en cada nodo de la malla.\n",
    "    \"\"\"\n",
    "    density = np.zeros(grid_theta.shape)\n",
    "    kernel_func = np.vectorize(cubic_spline_kernel)\n",
    "    for pt in points_pr:\n",
    "        # La distancia se calcula de forma Euclidiana en (θ, r):\n",
    "        d = np.sqrt((grid_theta - pt[0])**2 + (grid_r - pt[1])**2)\n",
    "        density += kernel_func(d, h)\n",
    "    return density\n",
    "\n",
    "# --- Función para extraer isocontornos (caminos densos) en (θ, r) ---\n",
    "def extract_and_connect_paths(density, grid_theta, grid_r, density_threshold):\n",
    "    \"\"\"\n",
    "    A partir del campo de densidad evaluado en la malla (grid_theta, grid_r),\n",
    "    extrae los contornos (isocontornos) donde la densidad es igual a density_threshold.\n",
    "    Retorna una lista de caminos, cada uno como un array de coordenadas [θ, r].\n",
    "    \"\"\"\n",
    "    cs = plt.contour(grid_theta, grid_r, density, levels=[density_threshold])\n",
    "    paths = []\n",
    "    for collection in cs.collections:\n",
    "        for path in collection.get_paths():\n",
    "            v = path.vertices\n",
    "            paths.append(v)\n",
    "    plt.clf()  # Limpiar la figura para no sobreponer gráficos previos\n",
    "    return paths\n",
    "\n",
    "# --- Función principal utilizando datos reales en (θ, r) ---\n",
    "def main_sph_path_reales():\n",
    "    # Se obtiene la información real del pipeline\n",
    "    datos_finales = main_example_modificado()\n",
    "    grupos_finales = datos_finales['grupos_finales']\n",
    "    background = datos_finales['background']\n",
    "    \n",
    "    # En este enfoque se trabaja directamente en (θ, r).\n",
    "    # Extraer puntos del background y de los grupos finales (usamos únicamente las columnas 'theta' y 'r')\n",
    "    bg_points = background[['theta','r']].values\n",
    "    # Para enfatizar los grupos finales (pozos densos), asignamos mayor \"peso\" a sus puntos.\n",
    "    group_points_list = []\n",
    "    for grupo in grupos_finales:\n",
    "        pts = grupo['points'][['theta','r']].values\n",
    "        group_points_list.append(pts)\n",
    "    if group_points_list:\n",
    "        group_points = np.vstack(group_points_list)\n",
    "    else:\n",
    "        group_points = np.empty((0,2))\n",
    "    \n",
    "    # Combinar puntos (trabajamos en (θ, r)):\n",
    "    all_points = np.vstack([bg_points, group_points])\n",
    "    \n",
    "    # Definir la malla en (θ, r).\n",
    "    theta_min, theta_max = all_points[:,0].min()-5, all_points[:,0].max()+5\n",
    "    r_min, r_max = all_points[:,1].min()-1, all_points[:,1].max()+1\n",
    "    n_theta, n_r = 300, 300\n",
    "    theta_vals = np.linspace(theta_min, theta_max, n_theta)\n",
    "    r_vals = np.linspace(r_min, r_max, n_r)\n",
    "    grid_theta, grid_r = np.meshgrid(theta_vals, r_vals)\n",
    "    \n",
    "    # Asignar masas: masa 1 para background y masa 5 para puntos de grupos finales.\n",
    "    bg_masses = np.ones(len(bg_points))\n",
    "    if group_points.shape[0] > 0:\n",
    "        group_masses = 5 * np.ones(len(group_points))\n",
    "        all_masses = np.hstack([bg_masses, group_masses])\n",
    "    else:\n",
    "        all_masses = bg_masses\n",
    "    \n",
    "    # Calcular la densidad en la malla usando SPH (trabajando en (θ, r)).\n",
    "    h = 5.0  # Ajusta según la escala de θ (grados) y r\n",
    "    density = compute_density_grid(all_points, grid_theta, grid_r, h=h)\n",
    "    density_smooth = gaussian_filter(density, sigma=1)\n",
    "    \n",
    "    # Definir el umbral de densidad; por ejemplo, el percentil 90\n",
    "    density_threshold = np.percentile(density_smooth, 60)\n",
    "    \n",
    "    # Extraer los caminos (isocontornos) en (θ, r) donde la densidad = density_threshold\n",
    "    paths = extract_and_connect_paths(density_smooth, grid_theta, grid_r, density_threshold)\n",
    "    \n",
    "    # Filtrar los caminos: se seleccionan aquellos que tengan una longitud mayor o igual a la mediana.\n",
    "    def path_length(path):\n",
    "        return np.sum(np.linalg.norm(path[1:] - path[:-1], axis=1))\n",
    "    lengths = np.array([path_length(p) for p in paths])\n",
    "    if len(lengths)==0:\n",
    "        print(\"No se encontraron caminos.\")\n",
    "        return\n",
    "    median_length = np.median(lengths)\n",
    "    selected_paths = [p for p, L in zip(paths, lengths) if L >= median_length]\n",
    "    \n",
    "    # Graficar la malla de densidad y los caminos en (θ, r) sin colorbar, en un tamaño amplio.\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.imshow(density_smooth, extent=[theta_min, theta_max, r_min, r_max],\n",
    "               origin='lower', cmap='inferno', aspect='auto')\n",
    "    for p in selected_paths:\n",
    "        plt.plot(p[:,0], p[:,1], '-o', linewidth=2)\n",
    "    plt.scatter(all_points[:,0], all_points[:,1], c='cyan', s=10, alpha=0.3, label='Puntos')\n",
    "    plt.title(\"Malla de densidad SPH y caminos densos (espacio (θ, r))\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    paths = main_sph_path_reales()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b39f22f7-a671-4949-8f87-83ec209a153f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b864e27-d5c1-4e4d-a263-33ac9a154b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install scikit-image --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9fd740-24a5-4163-83f9-e4175679cdce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script SPH + morfología para conectar islas de puntos en espacio (θ, r).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "from skimage import measure\n",
    "import pandas as pd\n",
    "\n",
    "# --- Función SPH: kernel cúbico spline en 2D (θ, r) ---\n",
    "def cubic_spline_kernel(r, h):\n",
    "    q = r / h\n",
    "    sigma = 10.0 / (7.0 * np.pi * h**2)\n",
    "    if 0 <= q <= 1:\n",
    "        return sigma * (1 - 1.5*q**2 + 0.75*q**3)\n",
    "    elif 1 < q <= 2:\n",
    "        return sigma * (0.25*(2 - q)**3)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# --- Interpola densidad en la malla (θ, r) ---\n",
    "def compute_density_grid(points, grid_theta, grid_r, h=5.0):\n",
    "    density = np.zeros(grid_theta.shape)\n",
    "    kernel = np.vectorize(cubic_spline_kernel)\n",
    "    for pt in points:\n",
    "        d = np.sqrt((grid_theta - pt[0])**2 + (grid_r - pt[1])**2)\n",
    "        density += kernel(d, h)\n",
    "    return density\n",
    "\n",
    "# --- Función principal ---\n",
    "def main_sph_connected():\n",
    "    # 1) Obtén tus datos reales:\n",
    "    datos = main_example_modificado()\n",
    "    grupos = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # 2) Prepara array de puntos (θ, r):\n",
    "    bg_pts = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) \\\n",
    "             if grupos else np.empty((0,2))\n",
    "    all_pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # 3) Configura malla:\n",
    "    tmin, tmax = all_pts[:,0].min()-5, all_pts[:,0].max()+5\n",
    "    rmin, rmax = all_pts[:,1].min()-1, all_pts[:,1].max()+1\n",
    "    nθ, nr = 300, 300\n",
    "    thetas = np.linspace(tmin, tmax, nθ)\n",
    "    rs    = np.linspace(rmin, rmax, nr)\n",
    "    gridθ, gridr = np.meshgrid(thetas, rs)\n",
    "\n",
    "    # 4) Calcula y suaviza densidad SPH:\n",
    "    h = 5.0\n",
    "    dens = compute_density_grid(all_pts, gridθ, gridr, h=h)\n",
    "    dens_s = gaussian_filter(dens, sigma=1)\n",
    "\n",
    "    # 5) Binariza y cierra huecos pequeños:\n",
    "    pct = 70\n",
    "    thresh = np.percentile(dens_s, pct)\n",
    "    mask = dens_s > thresh\n",
    "    closing_size = 5\n",
    "    mask_c = binary_closing(mask, structure=np.ones((closing_size, closing_size)))\n",
    "\n",
    "    # 6) Etiqueta componentes conectadas y filtra por área:\n",
    "    lbls, n_lbls = label(mask_c)\n",
    "    areas = np.bincount(lbls.ravel())[1:]  # omite etiqueta 0\n",
    "    min_area_pixels = 500  # ajusta según tu malla\n",
    "    good = np.where(areas >= min_area_pixels)[0] + 1\n",
    "\n",
    "    # 7) Extrae contornos de cada región válida:\n",
    "    final_paths = []\n",
    "    for lab in good:\n",
    "        region = (lbls == lab).astype(float)\n",
    "        contours = measure.find_contours(region, 0.5)\n",
    "        for c in contours:\n",
    "            # convierte índices de píxel a valores reales (θ, r)\n",
    "            θs = thetas[c[:,1].astype(int)]\n",
    "            rs_ = rs[c[:,0].astype(int)]\n",
    "            final_paths.append(np.vstack([θs, rs_]).T)\n",
    "\n",
    "    # 8) Grafica resultado:\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.imshow(dens_s, extent=[tmin, tmax, rmin, rmax],\n",
    "               origin='lower', cmap='inferno', aspect='auto')\n",
    "    for path in final_paths:\n",
    "        plt.plot(path[:,0], path[:,1], '-k', linewidth=2)\n",
    "    plt.scatter(all_pts[:,0], all_pts[:,1], c='cyan', s=5, alpha=0.3, label='Puntos')\n",
    "    plt.title(\"SPH + morfología: islas conectadas en (θ, r)\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return final_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    paths = main_sph_connected()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1583b46a-3d37-4cd8-a982-7c9afc4b4ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación alternativa con MeanShift en espacio (θ, r):\n",
    "encuentra islas densas sin necesidad de mallas ni grafos,\n",
    "y permite variar fácilmente el parámetro de bandwidth.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from scipy.spatial import ConvexHull\n",
    "import pandas as pd\n",
    "\n",
    "def main_meanshift_clusters(bandwidth=None, min_cluster_size=15):\n",
    "    \"\"\"\n",
    "    bandwidth: radio de ventana para MeanShift. Si es None, se estima automáticamente.\n",
    "    min_cluster_size: mínimo de puntos para conservar un cluster.\n",
    "    \"\"\"\n",
    "    # 1) Carga tus datos reales\n",
    "    datos = main_example_modificado()      # {'grupos_finales', 'background'}\n",
    "    grupos = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # 2) Prepara el array de puntos (θ, r)\n",
    "    bg_pts = background[['theta','r']].values\n",
    "    grp_pts = (np.vstack([g['points'][['theta','r']].values for g in grupos])\n",
    "               if grupos else np.empty((0,2)))\n",
    "    all_pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # 3) Estima bandwidth si no se proporcionó\n",
    "    if bandwidth is None:\n",
    "        # toma una muestra para acelerar la estimación si tienes muchos puntos\n",
    "        sample = all_pts if all_pts.shape[0] < 5000 else all_pts[np.random.choice(all_pts.shape[0], 5000, replace=False)]\n",
    "        bandwidth = estimate_bandwidth(sample, quantile=0.2, n_samples=500)\n",
    "        print(f\"Bandwidth estimado: {bandwidth:.2f}\")\n",
    "\n",
    "    # 4) Aplica MeanShift\n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    labels = ms.fit_predict(all_pts)\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # 5) Recopila clusters válidos\n",
    "    clusters = []\n",
    "    for lab in unique_labels:\n",
    "        pts = all_pts[labels == lab]\n",
    "        if pts.shape[0] < min_cluster_size:\n",
    "            continue\n",
    "        clusters.append((lab, pts))\n",
    "\n",
    "    # 6) Gráfica resultados\n",
    "    plt.figure(figsize=(16,6))\n",
    "    # fondo: todos los puntos en gris claro\n",
    "    plt.scatter(all_pts[:,0], all_pts[:,1],\n",
    "                c='lightgray', s=5, alpha=0.5, label='Background/Ruido')\n",
    "\n",
    "    for lab, pts in clusters:\n",
    "        # puntos del cluster\n",
    "        plt.scatter(pts[:,0], pts[:,1],\n",
    "                    s=10, alpha=0.7, label=f'Cluster {lab} ({pts.shape[0]} pts)')\n",
    "        # envolvente convexa para contorno\n",
    "        if pts.shape[0] >= 3:\n",
    "            hull = ConvexHull(pts)\n",
    "            hull_pts = pts[hull.vertices]\n",
    "            hull_closed = np.vstack([hull_pts, hull_pts[0]])\n",
    "            plt.plot(hull_closed[:,0], hull_closed[:,1], '-', linewidth=2)\n",
    "\n",
    "    plt.title(f\"Segmentación con MeanShift (bandwidth={bandwidth:.2f})\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2, fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 7) Devuelve solo los arrays de puntos para cada cluster\n",
    "    return [pts for _, pts in clusters]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejemplo de llamada variando parámetros:\n",
    "    #  - bandwidth=None (se estima)\n",
    "    #  - min_cluster_size=20\n",
    "    clusters = main_meanshift_clusters(bandwidth=None, min_cluster_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f3208a-8d72-4af5-bcb8-7257c1303d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera por histograma 2D + cierre morfológico + asignación de puntos.\n",
    "Evita cálculos de distancia O(N²) usando una malla de baja resolución.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import pandas as pd\n",
    "\n",
    "def main_histogram_clusters(\n",
    "    bins_theta=200,\n",
    "    bins_r=100,\n",
    "    smooth_sigma=1,\n",
    "    density_percentile=75,\n",
    "    closing_size=3,\n",
    "    min_cluster_size=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Parámetros:\n",
    "      bins_theta        → número de bins en θ\n",
    "      bins_r            → número de bins en r\n",
    "      smooth_sigma      → sigma para suavizado gaussiano del histograma\n",
    "      density_percentile→ percentil para umbralizar densidad\n",
    "      closing_size      → tamaño del structuring element para cerrar gaps\n",
    "      min_cluster_size  → mínimo de puntos para conservar un cluster\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Carga datos reales\n",
    "    datos      = main_example_modificado()       # debe devolver {'grupos_finales','background'}\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # 2) Construye array (θ, r)\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) \\\n",
    "              if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # 3) Histograma 2D\n",
    "    hist, theta_edges, r_edges = np.histogram2d(\n",
    "        pts[:,0], pts[:,1],\n",
    "        bins=[bins_theta, bins_r]\n",
    "    )\n",
    "\n",
    "    # 4) Suaviza el histograma y umbraliza\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask = hist_s > thr\n",
    "\n",
    "    # 5) Cierra pequeños huecos en la máscara\n",
    "    mask_c = binary_closing(mask, structure=np.ones((closing_size, closing_size)))\n",
    "\n",
    "    # 6) Etiqueta regiones conectadas\n",
    "    labels, n_labels = label(mask_c)\n",
    "\n",
    "    # 7) Asigna cada punto a la etiqueta de su bin\n",
    "    #    digitize da índice de bin [1..bins], restamos 1 para [0..bins-1]\n",
    "    t_idx = np.digitize(pts[:,0], theta_edges) - 1\n",
    "    r_idx = np.digitize(pts[:,1], r_edges) - 1\n",
    "    t_idx = np.clip(t_idx, 0, bins_theta-1)\n",
    "    r_idx = np.clip(r_idx, 0, bins_r-1)\n",
    "    pt_labels = labels[t_idx, r_idx]\n",
    "\n",
    "    # 8) Recolecta clusters válidos\n",
    "    clusters = []\n",
    "    for lab in range(1, n_labels+1):\n",
    "        mask_pts = (pt_labels == lab)\n",
    "        if mask_pts.sum() >= min_cluster_size:\n",
    "            clusters.append(pts[mask_pts])\n",
    "\n",
    "    # 9) Visualiza\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    # mapa de calor de densidad\n",
    "    extent = [theta_edges[0], theta_edges[-1], r_edges[0], r_edges[-1]]\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=extent, aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad (histogram2d suavizado)')\n",
    "    # superpone puntos de cada cluster\n",
    "    for i, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=10, alpha=0.7, label=f'Cluster {i} ({len(c)} pts)')\n",
    "    plt.title(\"Segmentación ligera: histograma2D + morfología\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2, fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return clusters\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters = main_histogram_clusters(\n",
    "        bins_theta=150,\n",
    "        bins_r=80,\n",
    "        smooth_sigma=1,\n",
    "        density_percentile=70,\n",
    "        closing_size=4,\n",
    "        min_cluster_size=40\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf6e4e6-4ab4-410f-9d07-763441368f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D:\n",
    "conecta islas separadas por huecos cuyo ancho real sea ≤ max_gap_dist.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=150,\n",
    "    bins_r=80,\n",
    "    smooth_sigma=1,\n",
    "    density_percentile=70,\n",
    "    closing_size=4,\n",
    "    min_cluster_size=40,\n",
    "    max_gap_dist=2.0  # distancia máxima (en unidades reales) para cruzar gaps\n",
    "):\n",
    "    # 1) Carga datos\n",
    "    datos      = main_example_modificado()\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # 2) Construye array de puntos (θ, r)\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) \\\n",
    "              if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # 3) Histograma 2D y suavizado\n",
    "    hist, theta_edges, r_edges = np.histogram2d(\n",
    "        pts[:,0], pts[:,1], bins=[bins_theta, bins_r]\n",
    "    )\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    # 4) Máscara y cierre morfológico\n",
    "    thr    = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask   = hist_s > thr\n",
    "    mask_c = binary_closing(mask, structure=np.ones((closing_size,closing_size)))\n",
    "\n",
    "    # 5) Etiqueta componentes en la máscara\n",
    "    labels_grid, n_labels = label(mask_c)\n",
    "\n",
    "    # 6) Prepara Dijkstra multinodal\n",
    "    #    Calcula los “pasos” reales en θ y r\n",
    "    dtheta = theta_edges[1] - theta_edges[0]\n",
    "    dr     = r_edges[1]     - r_edges[0]\n",
    "    #    Vecinos 8‑conectividad\n",
    "    neigh = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "\n",
    "    #    UnionFind sobre etiquetas [1..n_labels]\n",
    "    uf = UnionFind(n_labels+1)\n",
    "\n",
    "    #    Matriz de mejor distancia alcanzada (solo para background)\n",
    "    h, w = labels_grid.shape\n",
    "    best_dist = np.full((h,w), np.inf, dtype=float)\n",
    "    label_map = np.zeros((h,w), dtype=int)\n",
    "\n",
    "    #    Cola de prioridad: (dist, i, j, lab)\n",
    "    pq = []\n",
    "\n",
    "    # 7) Semillas = celdas de frontera (mask→fondo)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            lab = labels_grid[i,j]\n",
    "            if lab==0: \n",
    "                continue\n",
    "            # si algún vecino es fondo, es frontera\n",
    "            for di,dj in neigh:\n",
    "                ni,nj = i+di, j+dj\n",
    "                if 0<=ni<h and 0<=nj<w and labels_grid[ni,nj]==0:\n",
    "                    # inicializa ese fondo vecino como semilla\n",
    "                    dist0 = math.hypot(di*dtheta, dj*dr)\n",
    "                    if dist0 <= max_gap_dist:\n",
    "                        # seed position is the fondo cell\n",
    "                        if dist0 < best_dist[ni,nj]:\n",
    "                            best_dist[ni,nj], label_map[ni,nj] = dist0, lab\n",
    "                            heapq.heappush(pq, (dist0, ni, nj, lab))\n",
    "            # no buscamos en más vecinos: pasamos al siguiente\n",
    "\n",
    "    # 8) Dijkstra multinodal sobre fondo\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > best_dist[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        # mira fronteras de mask que toquen este fondo: si encuentra otra etiqueta, une\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if not (0<=ni<h and 0<=nj<w):\n",
    "                continue\n",
    "            lab2 = labels_grid[ni,nj]\n",
    "            if lab2>0 and lab2!=lab:\n",
    "                uf.union(lab, lab2)\n",
    "        # avanza en fondo\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if not (0<=ni<h and 0<=nj<w): \n",
    "                continue\n",
    "            if labels_grid[ni,nj]!=0:\n",
    "                continue  # no queremos pasar dentro de máscara\n",
    "            step = math.hypot(di*dtheta, dj*dr)\n",
    "            nd = dist + step\n",
    "            if nd <= max_gap_dist and nd < best_dist[ni,nj]:\n",
    "                best_dist[ni,nj] = nd\n",
    "                label_map[ni,nj] = lab\n",
    "                heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # 9) Reconstruye clusters finales uniendo etiquetas según UnionFind\n",
    "    merged = defaultdict(list)\n",
    "    # asigna a cada punto la etiqueta raíz de su bin\n",
    "    t_idx = np.clip(np.digitize(pts[:,0], theta_edges)-1, 0, bins_theta-1)\n",
    "    r_idx = np.clip(np.digitize(pts[:,1], r_edges)-1, 0, bins_r-1)\n",
    "    for pi, (ti, ri) in enumerate(zip(t_idx, r_idx)):\n",
    "        old_lab = labels_grid[ti, ri]\n",
    "        if old_lab>0:\n",
    "            root = uf.find(old_lab)\n",
    "            merged[root].append(pts[pi])\n",
    "\n",
    "    # filtra tamaño mínimo\n",
    "    final_clusters = [\n",
    "        np.vstack(ps) for ps in merged.values() if len(ps) >= min_cluster_size\n",
    "    ]\n",
    "\n",
    "    # 10) Visualiza\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.imshow(hist_s.T, origin='lower',\n",
    "               extent=[theta_edges[0],theta_edges[-1],\n",
    "                       r_edges[0],  r_edges[-1]],\n",
    "               aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for i, c in enumerate(final_clusters):\n",
    "        plt.scatter(c[:,0], c[:,1],\n",
    "                    s=10, alpha=0.7, label=f'Isla {i} ({len(c)} pts)')\n",
    "    plt.title(f\"Gap‑sensible BFS (max_gap={max_gap_dist})\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2, fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return final_clusters\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters = main_histogram_weighted_bfs_clusters(\n",
    "        bins_theta=100,\n",
    "        bins_r=60,\n",
    "        smooth_sigma=1,\n",
    "        density_percentile=80,\n",
    "        closing_size=2,\n",
    "        min_cluster_size=40,\n",
    "        max_gap_dist=0.5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851aead6-0138-4089-8cb6-de37b9113532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100,\n",
    "    bins_r=60,\n",
    "    smooth_sigma=1,\n",
    "    density_percentile=80,\n",
    "    closing_size=2,\n",
    "    min_cluster_size=40,\n",
    "    max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos      = main_example_modificado()\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    hist, theta_edges, r_edges = np.histogram2d(\n",
    "        pts[:,0], pts[:,1], bins=[bins_theta, bins_r]\n",
    "    )\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    thr    = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask   = hist_s > thr\n",
    "    mask_c = binary_closing(mask, structure=np.ones((closing_size,closing_size)))\n",
    "    labels_grid, n_labels = label(mask_c)\n",
    "\n",
    "    dtheta = theta_edges[1] - theta_edges[0]\n",
    "    dr     = r_edges[1]     - r_edges[0]\n",
    "    neigh = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf = UnionFind(n_labels+1)\n",
    "    h, w = labels_grid.shape\n",
    "    best_dist = np.full((h,w), np.inf, dtype=float)\n",
    "    label_map = np.zeros((h,w), dtype=int)\n",
    "    pq = []\n",
    "\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            lab = labels_grid[i,j]\n",
    "            if lab==0: continue\n",
    "            for di,dj in neigh:\n",
    "                ni,nj = i+di, j+dj\n",
    "                if 0<=ni<h and 0<=nj<w and labels_grid[ni,nj]==0:\n",
    "                    dist0 = math.hypot(di*dtheta, dj*dr)\n",
    "                    if dist0 <= max_gap_dist:\n",
    "                        if dist0 < best_dist[ni,nj]:\n",
    "                            best_dist[ni,nj], label_map[ni,nj] = dist0, lab\n",
    "                            heapq.heappush(pq, (dist0, ni, nj, lab))\n",
    "\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > best_dist[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if not (0<=ni<h and 0<=nj<w): continue\n",
    "            lab2 = labels_grid[ni,nj]\n",
    "            if lab2>0 and lab2!=lab:\n",
    "                uf.union(lab, lab2)\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if not (0<=ni<h and 0<=nj<w): continue\n",
    "            if labels_grid[ni,nj]!=0: continue\n",
    "            step = math.hypot(di*dtheta, dj*dr)\n",
    "            nd = dist + step\n",
    "            if nd <= max_gap_dist and nd < best_dist[ni,nj]:\n",
    "                best_dist[ni,nj] = nd\n",
    "                label_map[ni,nj] = lab\n",
    "                heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    merged = defaultdict(list)\n",
    "    t_idx = np.clip(np.digitize(pts[:,0], theta_edges)-1, 0, bins_theta-1)\n",
    "    r_idx = np.clip(np.digitize(pts[:,1], r_edges)-1, 0, bins_r-1)\n",
    "    for pi, (ti, ri) in enumerate(zip(t_idx, r_idx)):\n",
    "        old_lab = labels_grid[ti, ri]\n",
    "        if old_lab>0:\n",
    "            root = uf.find(old_lab)\n",
    "            merged[root].append(pts[pi])\n",
    "\n",
    "    final_clusters = [\n",
    "        np.vstack(ps) for ps in merged.values() if len(ps) >= min_cluster_size\n",
    "    ]\n",
    "\n",
    "    # CALCULAR CENTROIDES POR BLOQUE DE THETA\n",
    "    island_skeletons = []\n",
    "    for c in final_clusters:\n",
    "        theta = c[:, 0]\n",
    "        r = c[:, 1]\n",
    "        theta_bins = np.arange(np.min(theta), np.max(theta) + theta_bin_size, theta_bin_size)\n",
    "        skeleton = []\n",
    "        for i in range(len(theta_bins) - 1):\n",
    "            mask = (theta >= theta_bins[i]) & (theta < theta_bins[i+1])\n",
    "            if np.any(mask):\n",
    "                r_mean = np.mean(r[mask])\n",
    "                theta_mean = np.mean(theta[mask])\n",
    "                skeleton.append((theta_mean, r_mean))\n",
    "        island_skeletons.append(skeleton)\n",
    "\n",
    "    # VISUALIZACIÓN\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.imshow(hist_s.T, origin='lower',\n",
    "               extent=[theta_edges[0],theta_edges[-1],\n",
    "                       r_edges[0],  r_edges[-1]],\n",
    "               aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "\n",
    "    # Dibujar clusters\n",
    "    for i, c in enumerate(final_clusters):\n",
    "        plt.scatter(c[:,0], c[:,1],\n",
    "                    s=10, alpha=0.7, label=f'Isla {i} ({len(c)} pts)')\n",
    "\n",
    "    # Dibujar puntos blancos = centroides cada 10°\n",
    "    for skeleton in island_skeletons:\n",
    "        skeleton = np.array(skeleton)\n",
    "        plt.plot(skeleton[:, 0], skeleton[:, 1], 'wo', markersize=4, label='_nolegend_')\n",
    "\n",
    "    # Conexión entre centroides extremos si Δr > 0, Δθ < 0, y fondo denso\n",
    "    background_thr = np.percentile(hist_s[hist_s > 0], density_percentile)\n",
    "\n",
    "    for i, skel_i in enumerate(island_skeletons):\n",
    "        for j, skel_j in enumerate(island_skeletons):\n",
    "            if i == j or not skel_i or not skel_j:\n",
    "                continue\n",
    "            p_i = skel_i[-1]  # punto más a la derecha (θ alto)\n",
    "            p_j = skel_j[0]   # punto más a la izquierda (θ bajo)\n",
    "\n",
    "            delta_r = p_j[1] - p_i[1]\n",
    "            delta_theta = p_j[0] - p_i[0]\n",
    "            if delta_r > 0 and delta_theta < 0:\n",
    "                # Obtener índices en histograma\n",
    "                t_idx = np.clip(np.digitize([p_i[0], p_j[0]], theta_edges)-1, 0, bins_theta-1)\n",
    "                r_idx = np.clip(np.digitize([p_i[1], p_j[1]], r_edges)-1, 0, bins_r-1)\n",
    "                slice_hist = hist_s[min(t_idx):max(t_idx)+1, min(r_idx):max(r_idx)+1]\n",
    "                if np.mean(slice_hist) >= background_thr:\n",
    "                    plt.plot([p_i[0], p_j[0]], [p_i[1], p_j[1]], 'c--', lw=1.5, alpha=0.8)\n",
    "\n",
    "    plt.title(f\"Conexiones entre centroides Δr>0, Δθ<0 y fondo denso\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2, fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return final_clusters\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters = main_histogram_weighted_bfs_clusters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c684fb30-ddaa-462e-bcf6-d917ae795a29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D:\n",
    "– conecta islas separadas por huecos ≤ max_gap_dist\n",
    "– calcula centroides cada theta_bin_size grados\n",
    "– conecta cada primer y último centroide de cada isla con su vecino\n",
    "  más cercano que cumpla Δr>0 y Δθ<0 (distancia mínima)\n",
    "  para un enfoque alternativo más simple\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100,\n",
    "    bins_r=60,\n",
    "    smooth_sigma=1.0,\n",
    "    density_percentile=80,\n",
    "    closing_size=2,\n",
    "    min_cluster_size=40,\n",
    "    max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    # 1) Carga datos\n",
    "    datos      = main_example_modificado()\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # 2) Apilar puntos (θ, r)\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) \\\n",
    "              if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # 3) Histograma 2D y suavizado\n",
    "    hist, theta_edges, r_edges = np.histogram2d(\n",
    "        pts[:,0], pts[:,1], bins=[bins_theta, bins_r]\n",
    "    )\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    # 4) Máscara y cierre morfológico\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask = hist_s > thr\n",
    "    mask_c = binary_closing(mask, structure=np.ones((closing_size,closing_size)))\n",
    "    labels_grid, n_labels = label(mask_c)\n",
    "\n",
    "    # 5) BFS ponderado para unir componentes\n",
    "    dtheta = theta_edges[1] - theta_edges[0]\n",
    "    dr     = r_edges[1]     - r_edges[0]\n",
    "    neigh  = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf     = UnionFind(n_labels+1)\n",
    "    h, w   = labels_grid.shape\n",
    "    best_dist = np.full((h,w), np.inf, dtype=float)\n",
    "    pq = []\n",
    "\n",
    "    # semillas en la frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            lab = labels_grid[i,j]\n",
    "            if lab == 0: continue\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i+di, j+dj\n",
    "                if 0 <= ni < h and 0 <= nj < w and labels_grid[ni,nj] == 0:\n",
    "                    d0 = math.hypot(di*dtheta, dj*dr)\n",
    "                    if d0 <= max_gap_dist and d0 < best_dist[ni,nj]:\n",
    "                        best_dist[ni,nj] = d0\n",
    "                        heapq.heappush(pq, (d0, ni, nj, lab))\n",
    "\n",
    "    # expansión sobre el fondo\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > best_dist[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if not (0 <= ni < h and 0 <= nj < w): continue\n",
    "            lab2 = labels_grid[ni,nj]\n",
    "            if lab2 > 0 and lab2 != lab:\n",
    "                uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if not (0 <= ni < h and 0 <= nj < w): continue\n",
    "            if labels_grid[ni,nj] != 0: continue\n",
    "            step = math.hypot(di*dtheta, dj*dr)\n",
    "            nd = dist + step\n",
    "            if nd <= max_gap_dist and nd < best_dist[ni,nj]:\n",
    "                best_dist[ni,nj] = nd\n",
    "                heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # 6) Reconstruir clusters finales\n",
    "    merged = defaultdict(list)\n",
    "    t_idx = np.clip(np.digitize(pts[:,0], theta_edges)-1, 0, bins_theta-1)\n",
    "    r_idx = np.clip(np.digitize(pts[:,1], r_edges)-1,     0, bins_r-1)\n",
    "    for pi, (ti, ri) in enumerate(zip(t_idx, r_idx)):\n",
    "        lab = labels_grid[ti, ri]\n",
    "        if lab > 0:\n",
    "            root = uf.find(lab)\n",
    "            merged[root].append(pts[pi])\n",
    "    final_clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    # 7) Calcular centroides por bloque de θ\n",
    "    island_skeletons = []\n",
    "    for c in final_clusters:\n",
    "        thetas = c[:,0]; rs = c[:,1]\n",
    "        bins = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins)-1):\n",
    "            mask_block = (thetas >= bins[k]) & (thetas < bins[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        island_skeletons.append(ske)\n",
    "\n",
    "    # 8) Visualización\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower',\n",
    "               extent=[theta_edges[0],theta_edges[-1],r_edges[0],r_edges[-1]],\n",
    "               aspect='auto', cmap='inferno', zorder=0)\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "\n",
    "    # dibujar clustering\n",
    "    for i, c in enumerate(final_clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {i}', zorder=1)\n",
    "\n",
    "    # dibujar centroides\n",
    "    for ske in island_skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40,\n",
    "                        edgecolors='k', linewidths=0.5, zorder=2)\n",
    "\n",
    "    # 9) Conexiones por distancia mínima en Δr>0, Δθ<0\n",
    "    connections = []\n",
    "    for idx_i, ske_i in enumerate(island_skeletons):\n",
    "        if not ske_i:\n",
    "            continue\n",
    "        for pi in (ske_i[0], ske_i[-1]):\n",
    "            best_dist_seg = np.inf\n",
    "            best_pj = None\n",
    "            for idx_j, ske_j in enumerate(island_skeletons):\n",
    "                if idx_i == idx_j or not ske_j:\n",
    "                    continue\n",
    "                for pj in (ske_j[0], ske_j[-1]):\n",
    "                    dr_delta = pj[1] - pi[1]\n",
    "                    dtheta_delta = pj[0] - pi[0]\n",
    "                    if dr_delta > 0 and dtheta_delta < 0:\n",
    "                        dist_seg = math.hypot(dtheta_delta, dr_delta)\n",
    "                        if dist_seg < best_dist_seg:\n",
    "                            best_dist_seg = dist_seg\n",
    "                            best_pj = pj\n",
    "            if best_pj is not None:\n",
    "                connections.append((pi, best_pj))\n",
    "\n",
    "    # dibujar conexiones en blanco\n",
    "    for pi, pj in connections:\n",
    "        plt.plot([pi[0], pj[0]], [pi[1], pj[1]],\n",
    "                 linestyle='--', linewidth=2.5,\n",
    "                 color='white', alpha=0.9, zorder=3)\n",
    "\n",
    "    plt.title(\"Conexiones por distancia mínima (Δr>0, Δθ<0)\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2, fontsize='small', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return final_clusters\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters = main_histogram_weighted_bfs_clusters()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75d69cc6-0025-4a63-affb-7a1b9ee43760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488569f2-953f-448f-88ee-9fb1da4f0215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D + conexiones de centroides:\n",
    "- Conecta islas separadas por huecos ≤ max_gap_dist\n",
    "- Calcula centroides cada theta_bin_size grados\n",
    "- Conecta internamente cada isla (skeleton) entre centroides adyacentes\n",
    "- Conecta externamente solo desde los extremos de cada isla (primer y último centroid),\n",
    "  máximo dos conexiones por par de islas (Δr>0, Δθ<0)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100,\n",
    "    bins_r=60,\n",
    "    smooth_sigma=1.0,\n",
    "    density_percentile=80,\n",
    "    closing_size=2,\n",
    "    min_cluster_size=40,\n",
    "    max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos      = main_example_modificado()\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # Apilar puntos (θ, r)\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # Histograma 2D + suavizado\n",
    "    hist, theta_edges, r_edges = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    # Segmentación y etiquetado\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size,closing_size)))\n",
    "    labels_grid, n_labels = label(mask_c)\n",
    "\n",
    "    # BFS ponderado para unir componentes\n",
    "    dtheta = theta_edges[1] - theta_edges[0]\n",
    "    dr     = r_edges[1]     - r_edges[0]\n",
    "    neigh  = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf     = UnionFind(n_labels+1)\n",
    "    h, w   = labels_grid.shape\n",
    "    best_dist = np.full((h,w), np.inf)\n",
    "    pq = []\n",
    "\n",
    "    # semillas en la frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            lab = labels_grid[i,j]\n",
    "            if lab == 0: continue\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i+di, j+dj\n",
    "                if 0 <= ni < h and 0 <= nj < w and labels_grid[ni,nj] == 0:\n",
    "                    step = math.hypot(di*dtheta, dj*dr)\n",
    "                    if step <= max_gap_dist and step < best_dist[ni,nj]:\n",
    "                        best_dist[ni,nj] = step\n",
    "                        heapq.heappush(pq, (step, ni, nj, lab))\n",
    "\n",
    "    # expansión del BFS\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > best_dist[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels_grid[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if not (0 <= ni < h and 0 <= nj < w) or labels_grid[ni,nj] != 0:\n",
    "                continue\n",
    "            nd = dist + math.hypot(di*dtheta, dj*dr)\n",
    "            if nd <= max_gap_dist and nd < best_dist[ni,nj]:\n",
    "                best_dist[ni,nj] = nd\n",
    "                heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Recolectar clusters finales\n",
    "    merged = defaultdict(list)\n",
    "    t_idx = np.clip(np.digitize(pts[:,0],theta_edges)-1,0,bins_theta-1)\n",
    "    r_idx = np.clip(np.digitize(pts[:,1],r_edges)-1,0,bins_r-1)\n",
    "    for pi, (ti, ri) in enumerate(zip(t_idx, r_idx)):\n",
    "        lab = labels_grid[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[pi])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    # Calcular centroides (skeletons) y ordenar θ descendente\n",
    "    skeletons = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins)-1):\n",
    "            mask_block = (thetas >= bins[k]) & (thetas < bins[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, theta_edges, r_edges\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons):\n",
    "    # Conexiones internas: centroides adyacentes en cada isla\n",
    "    internal = []\n",
    "    for ske in skeletons:\n",
    "        for a, b in zip(ske, ske[1:]):\n",
    "            internal.append((a, b))\n",
    "\n",
    "    # Conexiones externas: solo desde extremos de cada isla\n",
    "    external = []\n",
    "    count_pair = defaultdict(int)\n",
    "    for i, ske_i in enumerate(skeletons):\n",
    "        extremes = []\n",
    "        if len(ske_i) >= 1:\n",
    "            extremes.append(ske_i[0])                   # extremo máximo θ\n",
    "        if len(ske_i) > 1:\n",
    "            extremes.append(ske_i[-1])                  # extremo mínimo θ\n",
    "        for p in extremes:\n",
    "            best = None\n",
    "            best_d = math.inf\n",
    "            best_j = None\n",
    "            for j, ske_j in enumerate(skeletons):\n",
    "                if i == j or not ske_j:\n",
    "                    continue\n",
    "                for q in ske_j:\n",
    "                    dr_delta = q[1] - p[1]\n",
    "                    dtheta_delta = q[0] - p[0]\n",
    "                    if dr_delta > 0 and dtheta_delta < 0:\n",
    "                        dist = math.hypot(dtheta_delta, dr_delta)\n",
    "                        if dist < best_d:\n",
    "                            best_d = dist\n",
    "                            best = (p, q)\n",
    "                            best_j = j\n",
    "            if best and count_pair[(i, best_j)] < 2:\n",
    "                external.append(best)\n",
    "                count_pair[(i, best_j)] += 1\n",
    "    return internal, external\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[theta_edges[0], theta_edges[-1], r_edges[0], r_edges[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k', linewidths=0.5)\n",
    "    # dibujar conexiones internas\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0], b[0]], [a[1], b[1]], '-', linewidth=2)\n",
    "    # dibujar conexiones externas\n",
    "    for p, q in external:\n",
    "        plt.plot([p[0], q[0]], [p[1], q[1]], '--', linewidth=2.5, color='white', alpha=0.9)\n",
    "    plt.title(\"Islas y conexiones (internas vs externas)\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2, fontsize='small', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters, skeletons, hist_s, theta_edges, r_edges = main_histogram_weighted_bfs_clusters()\n",
    "    internal, external = connect_skeletons(skeletons)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ea4f5b-d5ec-42fb-be4e-3ff4c01e5a4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D + conexiones de centroides:\n",
    "- Conecta islas separadas por huecos ≤ max_gap_dist\n",
    "- Calcula centroides cada theta_bin_size grados\n",
    "- Conecta internamente cada isla (skeleton) entre centroides adyacentes\n",
    "- Conecta externamente siempre el centroide de menor θ de cada isla con el centroide de mayor θ de otra isla,\n",
    "  priorizando la cercanía (distancia Euclidiana) y filtrando dr>0, dθ<0 y r>0,\n",
    "  dejando una única conexión por isla\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100,\n",
    "    bins_r=60,\n",
    "    smooth_sigma=1.0,\n",
    "    density_percentile=80,\n",
    "    closing_size=2,\n",
    "    min_cluster_size=40,\n",
    "    max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos      = main_example_modificado()\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # Apilar puntos (θ, r)\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # Histograma 2D + suavizado\n",
    "    hist, theta_edges, r_edges = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    # Segmentación y etiquetado\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size,closing_size)))\n",
    "    labels_grid, n_labels = label(mask_c)\n",
    "\n",
    "    # BFS ponderado para unir componentes\n",
    "    dtheta = theta_edges[1] - theta_edges[0]\n",
    "    dr     = r_edges[1]     - r_edges[0]\n",
    "    neigh  = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf     = UnionFind(n_labels+1)\n",
    "    h, w   = labels_grid.shape\n",
    "    best_dist_grid = np.full((h,w), np.inf)\n",
    "    pq = []\n",
    "\n",
    "    # semillas en la frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            lab = labels_grid[i,j]\n",
    "            if lab == 0: continue\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i+di, j+dj\n",
    "                if 0 <= ni < h and 0 <= nj < w and labels_grid[ni,nj] == 0:\n",
    "                    step = math.hypot(di*dtheta, dj*dr)\n",
    "                    if step <= max_gap_dist and step < best_dist_grid[ni,nj]:\n",
    "                        best_dist_grid[ni,nj] = step\n",
    "                        heapq.heappush(pq, (step, ni, nj, lab))\n",
    "\n",
    "    # expansión del BFS\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > best_dist_grid[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels_grid[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if not (0 <= ni < h and 0 <= nj < w) or labels_grid[ni,nj] != 0:\n",
    "                continue\n",
    "            nd = dist + math.hypot(di*dtheta, dj*dr)\n",
    "            if nd <= max_gap_dist and nd < best_dist_grid[ni,nj]:\n",
    "                best_dist_grid[ni,nj] = nd\n",
    "                heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Recolectar clusters finales\n",
    "    merged = defaultdict(list)\n",
    "    t_idx = np.clip(np.digitize(pts[:,0],theta_edges)-1,0,bins_theta-1)\n",
    "    r_idx = np.clip(np.digitize(pts[:,1],r_edges)-1,0,bins_r-1)\n",
    "    for pi, (ti, ri) in enumerate(zip(t_idx, r_idx)):\n",
    "        lab = labels_grid[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[pi])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    # Calcular centroides (skeletons) y ordenar θ descendente\n",
    "    skeletons = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins)-1):\n",
    "            mask_block = (thetas >= bins[k]) & (thetas < bins[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, theta_edges, r_edges\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, theta_edges, r_edges):\n",
    "    # Conexiones internas: centroides adyacentes en cada isla\n",
    "    internal = []\n",
    "    for ske in skeletons:\n",
    "        for a, b in zip(ske, ske[1:]):\n",
    "            internal.append((a, b))\n",
    "\n",
    "    # Conexiones externas: conecta el mínimo θ de cada isla con el máximo θ de la isla más cercana\n",
    "    external = []\n",
    "    for i, ske_i in enumerate(skeletons):\n",
    "        if not ske_i:\n",
    "            continue\n",
    "        p = ske_i[-1]  # centroide con menor θ en la isla i\n",
    "        best = None\n",
    "        best_dist = math.inf\n",
    "        for j, ske_j in enumerate(skeletons):\n",
    "            if i == j or not ske_j:\n",
    "                continue\n",
    "            q = ske_j[0]  # centroide con mayor θ en la isla j\n",
    "            dr_delta = q[1] - p[1]\n",
    "            dtheta_delta = q[0] - p[0]\n",
    "            if dr_delta > 0 and dtheta_delta < 0 and q[1] > 0:\n",
    "                dist = math.hypot(dtheta_delta, dr_delta)\n",
    "                if dist < best_dist:\n",
    "                    best_dist = dist\n",
    "                    best = (p, q)\n",
    "        if best:\n",
    "            external.append(best)\n",
    "    return internal, external\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[theta_edges[0], theta_edges[-1], r_edges[0], r_edges[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k', linewidths=0.5)\n",
    "    # dibujar conexiones internas\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0], b[0]], [a[1], b[1]], '-', linewidth=2)\n",
    "    # dibujar conexiones externas\n",
    "    for p, q in external:\n",
    "        plt.plot([p[0], q[0]], [p[1], q[1]], '--', linewidth=2.5, color='white', alpha=0.9)\n",
    "    plt.title(\"Islas y conexiones (internas vs externas)\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2, fontsize='small', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters, skeletons, hist_s, theta_edges, r_edges = main_histogram_weighted_bfs_clusters()\n",
    "    internal, external = connect_skeletons(skeletons, hist_s, theta_edges, r_edges)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28322ba1-90e4-4e73-9dcd-932931429e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D + conexiones de centroides:\n",
    "- Conecta islas separadas por huecos ≤ max_gap_dist\n",
    "- Calcula centroides cada theta_bin_size grados\n",
    "- Conecta internamente cada isla (skeleton) entre centroides adyacentes\n",
    "- Conecta externamente siempre el centroide de menor θ de cada isla con el centroide de mayor θ de otra isla,\n",
    "  priorizando la cercanía (distancia Euclidiana) y filtrando dr>0, dθ<0 y r>0,\n",
    "  dejando una única conexión por isla\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100,\n",
    "    bins_r=60,\n",
    "    smooth_sigma=1.0,\n",
    "    density_percentile=80,\n",
    "    closing_size=2,\n",
    "    min_cluster_size=40,\n",
    "    max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos      = main_example_modificado()\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # Apilar puntos (θ, r)\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # Histograma 2D + suavizado\n",
    "    hist, theta_edges, r_edges = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    # Segmentación y etiquetado\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size,closing_size)))\n",
    "    labels_grid, n_labels = label(mask_c)\n",
    "\n",
    "    # BFS ponderado para unir componentes\n",
    "    dtheta = theta_edges[1] - theta_edges[0]\n",
    "    dr     = r_edges[1]     - r_edges[0]\n",
    "    neigh  = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf     = UnionFind(n_labels+1)\n",
    "    h, w   = labels_grid.shape\n",
    "    best_dist_grid = np.full((h,w), np.inf)\n",
    "    pq = []\n",
    "\n",
    "    # semillas en la frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            lab = labels_grid[i,j]\n",
    "            if lab == 0: continue\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i+di, j+dj\n",
    "                if 0 <= ni < h and 0 <= nj < w and labels_grid[ni,nj] == 0:\n",
    "                    step = math.hypot(di*dtheta, dj*dr)\n",
    "                    if step <= max_gap_dist and step < best_dist_grid[ni,nj]:\n",
    "                        best_dist_grid[ni,nj] = step\n",
    "                        heapq.heappush(pq, (step, ni, nj, lab))\n",
    "\n",
    "    # expansión del BFS\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > best_dist_grid[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels_grid[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if not (0 <= ni < h and 0 <= nj < w) or labels_grid[ni,nj] != 0:\n",
    "                continue\n",
    "            nd = dist + math.hypot(di*dtheta, dj*dr)\n",
    "            if nd <= max_gap_dist and nd < best_dist_grid[ni,nj]:\n",
    "                best_dist_grid[ni,nj] = nd\n",
    "                heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Recolectar clusters finales\n",
    "    merged = defaultdict(list)\n",
    "    t_idx = np.clip(np.digitize(pts[:,0],theta_edges)-1,0,bins_theta-1)\n",
    "    r_idx = np.clip(np.digitize(pts[:,1],r_edges)-1,0,bins_r-1)\n",
    "    for pi, (ti, ri) in enumerate(zip(t_idx, r_idx)):\n",
    "        lab = labels_grid[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[pi])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    # Calcular centroides (skeletons) y ordenar θ descendente\n",
    "    skeletons = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins)-1):\n",
    "            mask_block = (thetas >= bins[k]) & (thetas < bins[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, theta_edges, r_edges\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, theta_edges, r_edges):\n",
    "    # Conexiones internas: centroides adyacentes en cada isla\n",
    "    internal = []\n",
    "    for ske in skeletons:\n",
    "        for a, b in zip(ske, ske[1:]):\n",
    "            internal.append((a, b))\n",
    "\n",
    "    # Conexiones externas: conecta el mínimo θ de cada isla con el máximo θ de la isla más cercana\n",
    "    external = []\n",
    "    for i, ske_i in enumerate(skeletons):\n",
    "        if not ske_i:\n",
    "            continue\n",
    "        p = ske_i[-1]  # centroide con menor θ en la isla i\n",
    "        best = None\n",
    "        best_dist = math.inf\n",
    "        for j, ske_j in enumerate(skeletons):\n",
    "            if i == j or not ske_j:\n",
    "                continue\n",
    "            q = ske_j[0]  # centroide con mayor θ en la isla j\n",
    "            dr_delta = q[1] - p[1]\n",
    "            dtheta_delta = q[0] - p[0]\n",
    "            if dr_delta > 0 and dtheta_delta < 0 and q[1] > 0:\n",
    "                dist = math.hypot(dtheta_delta, dr_delta)\n",
    "                if dist < best_dist:\n",
    "                    best_dist = dist\n",
    "                    best = (p, q)\n",
    "        if best:\n",
    "            external.append(best)\n",
    "    return internal, external\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[theta_edges[0], theta_edges[-1], r_edges[0], r_edges[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k', linewidths=0.5)\n",
    "    # dibujar conexiones internas\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0], b[0]], [a[1], b[1]], '-', color='white', linewidth=2)\n",
    "    # dibujar conexiones externas\n",
    "    for p, q in external:\n",
    "        plt.plot([p[0], q[0]], [p[1], q[1]], '--', linewidth=2.5, color='white', alpha=0.9)\n",
    "    plt.title(\"Islas y conexiones (internas vs externas)\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2, fontsize='small', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters, skeletons, hist_s, theta_edges, r_edges = main_histogram_weighted_bfs_clusters()\n",
    "    internal, external = connect_skeletons(skeletons, hist_s, theta_edges, r_edges)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da34d300-465b-48bf-90a5-da7498b719f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D + conexiones de centroides con centroides de densidad:\n",
    "- Conecta islas separadas por huecos ≤ max_gap_dist\n",
    "- Calcula centroides cada theta_bin_size grados\n",
    "- Conecta internamente cada isla (skeleton) entre centroides adyacentes\n",
    "- Conecta externamente siempre el centroide de menor θ de cada isla con el centroide de mayor θ de otra isla,\n",
    "  priorizando cercanía y filtrando dr>0, dθ<0 y r>0 (una conexión por isla)\n",
    "- Para cada conexión externa, rastrea densidad suavizada alrededor del segmento dentro de un radio dado\n",
    "  y construye un centroide de densidad, siguiendo los máximos en θ de la densidad suavizada\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100,\n",
    "    bins_r=60,\n",
    "    smooth_sigma=1.0,\n",
    "    density_percentile=80,\n",
    "    closing_size=2,\n",
    "    min_cluster_size=40,\n",
    "    max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos      = main_example_modificado()\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # Apilar puntos (θ, r)\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # Histograma 2D + suavizado\n",
    "    hist, theta_edges, r_edges = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    # Segmentación y etiquetado\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size,closing_size)))\n",
    "    labels_grid, n_labels = label(mask_c)\n",
    "\n",
    "    # BFS ponderado para unir componentes\n",
    "    dtheta = theta_edges[1] - theta_edges[0]\n",
    "    dr     = r_edges[1]     - r_edges[0]\n",
    "    neigh  = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf     = UnionFind(n_labels+1)\n",
    "    h, w   = labels_grid.shape\n",
    "    best_dist_grid = np.full((h,w), np.inf)\n",
    "    pq = []\n",
    "\n",
    "    # semillas en la frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            lab = labels_grid[i,j]\n",
    "            if lab == 0: continue\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i+di, j+dj\n",
    "                if 0 <= ni < h and 0 <= nj < w and labels_grid[ni,nj] == 0:\n",
    "                    step = math.hypot(di*dtheta, dj*dr)\n",
    "                    if step <= max_gap_dist and step < best_dist_grid[ni,nj]:\n",
    "                        best_dist_grid[ni,nj] = step\n",
    "                        heapq.heappush(pq, (step, ni, nj, lab))\n",
    "\n",
    "    # expansión del BFS\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > best_dist_grid[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels_grid[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if not (0 <= ni < h and 0 <= nj < w) or labels_grid[ni,nj] != 0:\n",
    "                continue\n",
    "            nd = dist + math.hypot(di*dtheta, dj*dr)\n",
    "            if nd <= max_gap_dist and nd < best_dist_grid[ni,nj]:\n",
    "                best_dist_grid[ni,nj] = nd\n",
    "                heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Recolectar clusters finales\n",
    "    merged = defaultdict(list)\n",
    "    t_idx = np.clip(np.digitize(pts[:,0],theta_edges)-1,0,bins_theta-1)\n",
    "    r_idx = np.clip(np.digitize(pts[:,1],r_edges)-1,0,bins_r-1)\n",
    "    for pi, (ti, ri) in enumerate(zip(t_idx, r_idx)):\n",
    "        lab = labels_grid[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[pi])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    # Calcular centroides (skeletons) y ordenar θ descendente\n",
    "    skeletons = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins)-1):\n",
    "            mask_block = (thetas >= bins[k]) & (thetas < bins[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, theta_edges, r_edges\n",
    "\n",
    "\n",
    "def compute_density_centroid(p, q, hist_s, theta_edges, r_edges, radius, samples=100):\n",
    "    \"\"\"\n",
    "    Para un segmento p->q, muestrea puntos a lo largo, recoge vecinos\n",
    "    dentro de \"radius\" y calcula el centroide de densidad ponderado.\n",
    "    \"\"\"\n",
    "    # parametrización del segmento\n",
    "    ts = np.linspace(p[0], q[0], samples)\n",
    "    rs = np.linspace(p[1], q[1], samples)\n",
    "    weights = []\n",
    "    points = []\n",
    "    # para cada muestra, buscar puntos en hist_s dentro del radio\n",
    "    for theta_s, r_s in zip(ts, rs):\n",
    "        # convertir a indices\n",
    "        ti = np.clip(np.digitize(theta_s, theta_edges)-1, 0, hist_s.shape[0]-1)\n",
    "        ri = np.clip(np.digitize(r_s,     r_edges)-1,     0, hist_s.shape[1]-1)\n",
    "        # rango de vecinos\n",
    "        rad_t = int(np.ceil(radius / (theta_edges[1]-theta_edges[0])))\n",
    "        rad_r = int(np.ceil(radius / (r_edges[1]-r_edges[0])))\n",
    "        for di in range(-rad_t, rad_t+1):\n",
    "            for dj in range(-rad_r, rad_r+1):\n",
    "                nti, nrj = ti+di, ri+dj\n",
    "                if 0 <= nti < hist_s.shape[0] and 0 <= nrj < hist_s.shape[1]:\n",
    "                    weights.append(hist_s[nti, nrj])\n",
    "                    # coordenadas reales\n",
    "                    theta_coord = theta_edges[nti] + (theta_edges[1]-theta_edges[0])/2\n",
    "                    r_coord     = r_edges[nrj]     + (r_edges[1]-r_edges[0])/2\n",
    "                    points.append((theta_coord, r_coord))\n",
    "    if not weights:\n",
    "        return None\n",
    "    weights = np.array(weights)\n",
    "    pts    = np.array(points)\n",
    "    # centroide ponderado\n",
    "    idx = weights > 0\n",
    "    if not np.any(idx):\n",
    "        return None\n",
    "    centroid = np.average(pts[idx], axis=0, weights=weights[idx])\n",
    "    return tuple(centroid)\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, theta_edges, r_edges,\n",
    "                      density_radius=0.2):\n",
    "    # Conexiones internas\n",
    "    internal = [(a,b) for ske in skeletons for a,b in zip(ske, ske[1:])]\n",
    "    # Conexiones externas\n",
    "    external = []\n",
    "    density_centroids = []\n",
    "    for i, ske_i in enumerate(skeletons):\n",
    "        if not ske_i:\n",
    "            continue\n",
    "        p = ske_i[-1]\n",
    "        best = None\n",
    "        best_dist = math.inf\n",
    "        for j, ske_j in enumerate(skeletons):\n",
    "            if i == j or not ske_j:\n",
    "                continue\n",
    "            q = ske_j[0]\n",
    "            dr_delta = q[1] - p[1]\n",
    "            dtheta_delta = q[0] - p[0]\n",
    "            if dr_delta>0 and dtheta_delta<0:\n",
    "                dist = math.hypot(dtheta_delta, dr_delta)\n",
    "                if dist < best_dist:\n",
    "                    best_dist = dist; best = (p,q)\n",
    "        if best:\n",
    "            external.append(best)\n",
    "            # calcular centroide de densidad alrededor del segmento\n",
    "            dcent = compute_density_centroid(best[0], best[1], hist_s,\n",
    "                                             theta_edges, r_edges,\n",
    "                                             density_radius)\n",
    "            if dcent:\n",
    "                density_centroids.append(dcent)\n",
    "    return internal, external, density_centroids\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(clusters, skeletons, hist_s,\n",
    "                                  theta_edges, r_edges,\n",
    "                                  internal, external,\n",
    "                                  density_centroids=None):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[theta_edges[0], theta_edges[-1],\n",
    "                                                 r_edges[0], r_edges[-1]],\n",
    "               aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40,\n",
    "                        edgecolors='k', linewidths=0.5)\n",
    "    # conexiones internas en blanco\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0], b[0]], [a[1], b[1]], '-', color='white', linewidth=2)\n",
    "    # conexiones externas\n",
    "    for p, q in external:\n",
    "        plt.plot([p[0], q[0]], [p[1], q[1]], '--', linewidth=2.5,\n",
    "                 color='white', alpha=0.9)\n",
    "    # centroides de densidad como puntos rojos\n",
    "    if density_centroids:\n",
    "        dc = np.array(density_centroids)\n",
    "        plt.scatter(dc[:,0], dc[:,1], marker='o', s=50,\n",
    "                    facecolors='none', edgecolors='red', linewidths=2,\n",
    "                    label='Centroides de densidad')\n",
    "    plt.title(\"Islas, conexiones y centroides de densidad\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2, fontsize='small', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters, skeletons, hist_s, theta_edges, r_edges = main_histogram_weighted_bfs_clusters()\n",
    "    internal, external, dens_centroids = connect_skeletons(skeletons,\n",
    "                                                          hist_s,\n",
    "                                                          theta_edges,\n",
    "                                                          r_edges,\n",
    "                                                          density_radius=0.2)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s,\n",
    "                                  theta_edges, r_edges,\n",
    "                                  internal, external,\n",
    "                                  dens_centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5975820-db95-4a8a-b7c6-0e311355a3d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D + conexiones de centroides con centroides de densidad:\n",
    "- Conecta islas separadas por huecos ≤ max_gap_dist\n",
    "- Calcula centroides cada theta_bin_size grados\n",
    "- Conecta internamente cada isla (skeleton) entre centroides adyacentes\n",
    "- Conecta externamente siempre el centroide de menor θ de cada isla con el centroide de mayor θ de otra isla,\n",
    "  priorizando cercanía (dist Euclidiana) y filtrando dr>0, dθ<0 y r>0 (una conexión por isla)\n",
    "- Para cada conexión externa, rastrea densidad suavizada alrededor del segmento dentro de un radio = 1/4\n",
    "  de la longitud del segmento y construye un centroide de densidad\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100,\n",
    "    bins_r=60,\n",
    "    smooth_sigma=1.0,\n",
    "    density_percentile=80,\n",
    "    closing_size=2,\n",
    "    min_cluster_size=40,\n",
    "    max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos      = main_example_modificado()\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    hist, theta_edges, r_edges = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size,closing_size)))\n",
    "    labels_grid, n_labels = label(mask_c)\n",
    "\n",
    "    dtheta = theta_edges[1] - theta_edges[0]\n",
    "    dr     = r_edges[1]     - r_edges[0]\n",
    "    neigh  = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf     = UnionFind(n_labels+1)\n",
    "    h, w   = labels_grid.shape\n",
    "    best_dist_grid = np.full((h,w), np.inf)\n",
    "    pq = []\n",
    "\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            lab = labels_grid[i,j]\n",
    "            if lab == 0: continue\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i+di, j+dj\n",
    "                if 0 <= ni < h and 0 <= nj < w and labels_grid[ni,nj] == 0:\n",
    "                    step = math.hypot(di*dtheta, dj*dr)\n",
    "                    if step <= max_gap_dist and step < best_dist_grid[ni,nj]:\n",
    "                        best_dist_grid[ni,nj] = step\n",
    "                        heapq.heappush(pq, (step, ni, nj, lab))\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > best_dist_grid[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels_grid[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if not (0 <= ni < h and 0 <= nj < w) or labels_grid[ni,nj] != 0:\n",
    "                continue\n",
    "            nd = dist + math.hypot(di*dtheta, dj*dr)\n",
    "            if nd <= max_gap_dist and nd < best_dist_grid[ni,nj]:\n",
    "                best_dist_grid[ni,nj] = nd\n",
    "                heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    merged = defaultdict(list)\n",
    "    t_idx = np.clip(np.digitize(pts[:,0],theta_edges)-1,0,bins_theta-1)\n",
    "    r_idx = np.clip(np.digitize(pts[:,1],r_edges)-1,0,bins_r-1)\n",
    "    for pi, (ti, ri) in enumerate(zip(t_idx, r_idx)):\n",
    "        lab = labels_grid[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[pi])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    skeletons = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins)-1):\n",
    "            mask_block = (thetas >= bins[k]) & (thetas < bins[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, theta_edges, r_edges\n",
    "\n",
    "\n",
    "def compute_density_centroid(p, q, hist_s, theta_edges, r_edges, radius, samples=100):\n",
    "    ts = np.linspace(p[0], q[0], samples)\n",
    "    rs = np.linspace(p[1], q[1], samples)\n",
    "    weights, points = [], []\n",
    "    for theta_s, r_s in zip(ts, rs):\n",
    "        ti = np.clip(np.digitize(theta_s, theta_edges)-1, 0, hist_s.shape[0]-1)\n",
    "        ri = np.clip(np.digitize(r_s,     r_edges)-1,     0, hist_s.shape[1]-1)\n",
    "        rad_t = int(np.ceil(radius / (theta_edges[1]-theta_edges[0])))\n",
    "        rad_r = int(np.ceil(radius / (r_edges[1]-r_edges[0])))\n",
    "        for di in range(-rad_t, rad_t+1):\n",
    "            for dj in range(-rad_r, rad_r+1):\n",
    "                nti, nrj = ti+di, ri+dj\n",
    "                if 0 <= nti < hist_s.shape[0] and 0 <= nrj < hist_s.shape[1]:\n",
    "                    weights.append(hist_s[nti, nrj])\n",
    "                    theta_coord = theta_edges[nti] + (theta_edges[1]-theta_edges[0])/2\n",
    "                    r_coord     = r_edges[nrj]     + (r_edges[1]-r_edges[0])/2\n",
    "                    points.append((theta_coord, r_coord))\n",
    "    if not weights:\n",
    "        return None\n",
    "    weights = np.array(weights)\n",
    "    pts    = np.array(points)\n",
    "    idx = weights > 0\n",
    "    if not np.any(idx):\n",
    "        return None\n",
    "    return tuple(np.average(pts[idx], axis=0, weights=weights[idx]))\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, theta_edges, r_edges, density_ratio=0.25):\n",
    "    internal = [(a,b) for ske in skeletons for a,b in zip(ske, ske[1:])]\n",
    "    external, density_centroids = [], []\n",
    "    for i, ske_i in enumerate(skeletons):\n",
    "        if not ske_i:\n",
    "            continue\n",
    "        p = ske_i[-1]\n",
    "        best, best_dist = None, math.inf\n",
    "        for j, ske_j in enumerate(skeletons):\n",
    "            if i==j or not ske_j:\n",
    "                continue\n",
    "            q = ske_j[0]\n",
    "            dr_delta = q[1]-p[1]; dtheta_delta = q[0]-p[0]\n",
    "            if dr_delta>0 and dtheta_delta<0:\n",
    "                dist = math.hypot(dtheta_delta, dr_delta)\n",
    "                if dist<best_dist:\n",
    "                    best_dist, best = dist, (p,q)\n",
    "        if best:\n",
    "            external.append(best)\n",
    "            radius = best_dist * density_ratio\n",
    "            dcent = compute_density_centroid(best[0], best[1], hist_s,\n",
    "                                             theta_edges, r_edges,\n",
    "                                             radius)\n",
    "            if dcent:\n",
    "                density_centroids.append(dcent)\n",
    "    return internal, external, density_centroids\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external, density_centroids=None):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[theta_edges[0],theta_edges[-1],r_edges[0],r_edges[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters): plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size: plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k', linewidths=0.5)\n",
    "    for a,b in internal: plt.plot([a[0],b[0]],[a[1],b[1]],'-',color='white',linewidth=2)\n",
    "    for p,q in external: plt.plot([p[0],q[0]],[p[1],q[1]],'--',linewidth=2.5,color='white',alpha=0.9)\n",
    "    if density_centroids:\n",
    "        dc = np.array(density_centroids)\n",
    "        plt.scatter(dc[:,0],dc[:,1],marker='o',s=50,facecolors='none',edgecolors='red',linewidths=2,label='Centroides de densidad')\n",
    "    plt.title(\"Islas, conexiones y centroides de densidad\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2,fontsize='small',loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters, skeletons, hist_s, theta_edges, r_edges = main_histogram_weighted_bfs_clusters()\n",
    "    internal, external, dens_centroids = connect_skeletons(skeletons, hist_s, theta_edges, r_edges)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external, dens_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e8c274-f4e2-4395-8551-25466170f405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D + conexiones de centroides con centroides de densidad:\n",
    "- Conecta islas separadas por huecos ≤ max_gap_dist\n",
    "- Calcula centroides cada theta_bin_size grados\n",
    "- Conecta internamente cada isla (skeleton) entre centroides adyacentes\n",
    "- Conecta externamente siempre el centroide de menor θ de cada isla con el centroide de mayor θ de otra isla,\n",
    "  priorizando cercanía (dist Euclidiana) y filtrando dr>0, dθ<0 y r>0 (una conexión por isla)\n",
    "- Para cada conexión externa, rastrea densidad suavizada alrededor del segmento en incrementos de 2°\n",
    "  y construye un centroide de densidad, siguiendo los máximos en θ de la densidad suavizada\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100,\n",
    "    bins_r=60,\n",
    "    smooth_sigma=1.0,\n",
    "    density_percentile=80,\n",
    "    closing_size=2,\n",
    "    min_cluster_size=40,\n",
    "    max_gap_dist=0.5,\n",
    "    theta_bin_size=2  # ahora 2° en 2°\n",
    "):\n",
    "    datos      = main_example_modificado()\n",
    "    grupos     = datos['grupos_finales']\n",
    "    background = datos['background']\n",
    "\n",
    "    # Apilar puntos (θ, r)\n",
    "    bg_pts  = background[['theta','r']].values\n",
    "    grp_pts = np.vstack([g['points'][['theta','r']].values for g in grupos]) if grupos else np.empty((0,2))\n",
    "    pts = np.vstack([bg_pts, grp_pts])\n",
    "\n",
    "    # Histograma 2D + suavizado\n",
    "    hist, theta_edges, r_edges = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    # Segmentación y etiquetado\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size,closing_size)))\n",
    "    labels_grid, n_labels = label(mask_c)\n",
    "\n",
    "    # BFS ponderado para unir componentes\n",
    "    dtheta = theta_edges[1] - theta_edges[0]\n",
    "    dr     = r_edges[1]     - r_edges[0]\n",
    "    neigh  = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf     = UnionFind(n_labels+1)\n",
    "    h, w   = labels_grid.shape\n",
    "    best_dist_grid = np.full((h,w), np.inf)\n",
    "    pq = []\n",
    "\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            lab = labels_grid[i,j]\n",
    "            if lab == 0: continue\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i+di, j+dj\n",
    "                if 0 <= ni < h and 0 <= nj < w and labels_grid[ni,nj] == 0:\n",
    "                    step = math.hypot(di*dtheta, dj*dr)\n",
    "                    if step <= max_gap_dist and step < best_dist_grid[ni,nj]:\n",
    "                        best_dist_grid[ni,nj] = step\n",
    "                        heapq.heappush(pq, (step, ni, nj, lab))\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > best_dist_grid[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels_grid[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i+di, j+dj\n",
    "            if not (0 <= ni < h and 0 <= nj < w) or labels_grid[ni,nj] != 0:\n",
    "                continue\n",
    "            nd = dist + math.hypot(di*dtheta, dj*dr)\n",
    "            if nd <= max_gap_dist and nd < best_dist_grid[ni,nj]:\n",
    "                best_dist_grid[ni,nj] = nd\n",
    "                heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Recolectar clusters finales\n",
    "    merged = defaultdict(list)\n",
    "    t_idx = np.clip(np.digitize(pts[:,0],theta_edges)-1,0,bins_theta-1)\n",
    "    r_idx = np.clip(np.digitize(pts[:,1],r_edges)-1,0,bins_r-1)\n",
    "    for pi, (ti, ri) in enumerate(zip(t_idx, r_idx)):\n",
    "        lab = labels_grid[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[pi])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    # Calcular centroides (skeletons) y ordenar θ descendente\n",
    "    skeletons = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins)-1):\n",
    "            mask_block = (thetas >= bins[k]) & (thetas < bins[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, theta_edges, r_edges\n",
    "\n",
    "\n",
    "def compute_density_centroid(p, q, hist_s, theta_edges, r_edges, radius):\n",
    "    \"\"\"\n",
    "    Para un segmento p->q, muestrea cada 2° en θ, recoge vecinos\n",
    "    dentro de \"radius\" y calcula el centroide de densidad ponderado.\n",
    "    \"\"\"\n",
    "    theta_start, theta_end = p[0], q[0]\n",
    "    step = 2 if theta_end > theta_start else -2\n",
    "    ts = np.arange(theta_start, theta_end + step, step)\n",
    "    # modelo lineal r(θ)\n",
    "    if q[0] != p[0]:\n",
    "        m = (q[1] - p[1]) / (q[0] - p[0])\n",
    "        rs = p[1] + m * (ts - p[0])\n",
    "    else:\n",
    "        rs = np.linspace(p[1], q[1], len(ts))\n",
    "    weights, points = [], []\n",
    "    for theta_s, r_s in zip(ts, rs):\n",
    "        ti = np.clip(np.digitize(theta_s, theta_edges)-1, 0, hist_s.shape[0]-1)\n",
    "        ri = np.clip(np.digitize(r_s,     r_edges)-1,     0, hist_s.shape[1]-1)\n",
    "        rad_t = int(np.ceil(radius / (theta_edges[1]-theta_edges[0])))\n",
    "        rad_r = int(np.ceil(radius / (r_edges[1]-r_edges[0])))\n",
    "        for di in range(-rad_t, rad_t+1):\n",
    "            for dj in range(-rad_r, rad_r+1):\n",
    "                nti, nrj = ti+di, ri+dj\n",
    "                if 0 <= nti < hist_s.shape[0] and 0 <= nrj < hist_s.shape[1]:\n",
    "                    w = hist_s[nti, nrj]\n",
    "                    if w > 0:\n",
    "                        weights.append(w)\n",
    "                        theta_coord = theta_edges[nti] + (theta_edges[1]-theta_edges[0])/2\n",
    "                        r_coord     = r_edges[nrj]     + (r_edges[1]-r_edges[0])/2\n",
    "                        points.append((theta_coord, r_coord))\n",
    "    if not weights:\n",
    "        return None\n",
    "    weights = np.array(weights)\n",
    "    pts    = np.array(points)\n",
    "    return tuple(np.average(pts, axis=0, weights=weights))\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, theta_edges, r_edges, density_ratio=0.25):\n",
    "    internal = [(a,b) for ske in skeletons for a,b in zip(ske, ske[1:])]\n",
    "    external, density_centroids = [], []\n",
    "    for i, ske_i in enumerate(skeletons):\n",
    "        if not ske_i:\n",
    "            continue\n",
    "        p = ske_i[-1]\n",
    "        best, best_dist = None, math.inf\n",
    "        for j, ske_j in enumerate(skeletons):\n",
    "            if i==j or not ske_j:\n",
    "                continue\n",
    "            q = ske_j[0]\n",
    "            dr_delta = q[1]-p[1]; dtheta_delta = q[0]-p[0]\n",
    "            if dr_delta>0 and dtheta_delta<0:\n",
    "                dist = math.hypot(dtheta_delta, dr_delta)\n",
    "                if dist<best_dist:\n",
    "                    best_dist, best = dist, (p,q)\n",
    "        if best:\n",
    "            external.append(best)\n",
    "            radius = best_dist * density_ratio\n",
    "            dcent = compute_density_centroid(best[0], best[1], hist_s,\n",
    "                                             theta_edges, r_edges,\n",
    "                                             radius)\n",
    "            if dcent:\n",
    "                density_centroids.append(dcent)\n",
    "    return internal, external, density_centroids\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external, density_centroids=None):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[theta_edges[0],theta_edges[-1],r_edges[0],r_edges[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters): plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size: plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k', linewidths=0.5)\n",
    "    for a,b in internal: plt.plot([a[0],b[0]],[a[1],b[1]],'-',color='white',linewidth=2)\n",
    "    for p,q in external: plt.plot([p[0],q[0]],[p[1],q[1]],'--',linewidth=2.5,color='white',alpha=0.9)\n",
    "    if density_centroids:\n",
    "        dc = np.array(density_centroids)\n",
    "        plt.scatter(dc[:,0],dc[:,1],marker='o',s=50,facecolors='none',edgecolors='red',linewidths=2,label='Centroides de densidad')\n",
    "    plt.title(\"Islas, conexiones y centroides de densidad\")\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(markerscale=2,fontsize='small',loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clusters, skeletons, hist_s, theta_edges, r_edges = main_histogram_weighted_bfs_clusters()\n",
    "    internal, external, dens_centroids = connect_skeletons(skeletons, hist_s, theta_edges, r_edges)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, theta_edges, r_edges, internal, external, dens_centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a8c4f6-c717-48e7-943f-d208f5da9663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D + conexiones dinámicas de centroides basadas en densidad:\n",
    "- Conecta islas separadas por huecos ≤ max_gap_dist\n",
    "- Calcula skeletons de centroides cada theta_bin_size grados\n",
    "- Conecta internamente centroides adyacentes en cada isla\n",
    "- Para cada isla, conecta el extremo mínimo θ con el extremo máximo θ de la isla más cercana (dr>0, dθ<0)\n",
    "- Además, genera un camino de centroides adicionales siguiendo los máximos de densidad en background\n",
    "  recorriendo el segmento a pasos de theta_step grados y buscando el punto de máxima densidad radial dentro de radius\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq, math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n): self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100, bins_r=60, smooth_sigma=1.0,\n",
    "    density_percentile=80, closing_size=2,\n",
    "    min_cluster_size=40, max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos = main_example_modificado()\n",
    "    bg = datos['background'][['theta','r']].values\n",
    "    grp = np.vstack([g['points'][['theta','r']].values for g in datos['grupos_finales']]) if datos['grupos_finales'] else np.empty((0,2))\n",
    "    pts = np.vstack([bg, grp])\n",
    "    hist, te, re = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "    thr = np.percentile(hist_s[hist_s > 0], density_percentile)\n",
    "    mask = hist_s > thr\n",
    "    mask_c = binary_closing(mask, structure=np.ones((closing_size, closing_size)))\n",
    "    labels, n_lab = label(mask_c)\n",
    "    dθ = te[1] - te[0]\n",
    "    dr = re[1] - re[0]\n",
    "    neigh = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf = UnionFind(n_lab + 1)\n",
    "    h, w = labels.shape\n",
    "    dist_grid = np.full((h, w), np.inf)\n",
    "    pq = []\n",
    "    # BFS frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if labels[i,j] > 0:\n",
    "                for di, dj in neigh:\n",
    "                    ni, nj = i + di, j + dj\n",
    "                    if 0 <= ni < h and 0 <= nj < w and labels[ni,nj] == 0:\n",
    "                        d0 = math.hypot(di * dθ, dj * dr)\n",
    "                        if d0 <= max_gap_dist and d0 < dist_grid[ni,nj]:\n",
    "                            dist_grid[ni,nj] = d0\n",
    "                            heapq.heappush(pq, (d0, ni, nj, labels[i,j]))\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > dist_grid[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < h and 0 <= nj < w and labels[ni,nj] == 0:\n",
    "                nd = dist + math.hypot(di * dθ, dj * dr)\n",
    "                if nd <= max_gap_dist and nd < dist_grid[ni,nj]:\n",
    "                    dist_grid[ni,nj] = nd\n",
    "                    heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "    # Formar clusters\n",
    "    merged = defaultdict(list)\n",
    "    tidx = np.clip(np.digitize(pts[:,0], te) - 1, 0, bins_theta - 1)\n",
    "    ridx = np.clip(np.digitize(pts[:,1], re) - 1, 0, bins_r - 1)\n",
    "    for p, (ti, ri) in enumerate(zip(tidx, ridx)):\n",
    "        lab = labels[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[p])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "    # Skeletons\n",
    "    ske_list = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins_arr = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins_arr) - 1):\n",
    "            mask_block = (thetas >= bins_arr[k]) & (thetas < bins_arr[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        ske_list.append(sorted(ske, key=lambda x: -x[0]))\n",
    "    return clusters, ske_list, hist_s, te, re\n",
    "\n",
    "\n",
    "def compute_density_path(p, q, hist_s, te, re, radius, theta_step=2):\n",
    "    \"\"\"\n",
    "    Para cada ángulo a lo largo del segmento p->q, busca en radial máximo.\n",
    "    \"\"\"\n",
    "    # Resoluciones\n",
    "    dθ = te[1] - te[0]\n",
    "    dr = re[1] - re[0]\n",
    "    # División en pasos según theta_step\n",
    "    total_dθ = q[0] - p[0]\n",
    "    steps = max(int(abs(total_dθ) / theta_step) + 1, 2)\n",
    "    thetas = np.linspace(p[0], q[0], steps)\n",
    "    rs = np.linspace(p[1], q[1], steps)\n",
    "    path = []\n",
    "    for θ_i, r_i in zip(thetas, rs):\n",
    "        ti = int(np.clip(np.digitize(θ_i, te) - 1, 0, hist_s.shape[0] - 1))\n",
    "        ri = int(np.clip(np.digitize(r_i, re) - 1, 0, hist_s.shape[1] - 1))\n",
    "        # Barrido radial para maxizar densidad\n",
    "        best_pt = None\n",
    "        best_val = -np.inf\n",
    "        max_r = int(np.ceil(radius / dr))\n",
    "        for dr_off in range(-max_r, max_r + 1):\n",
    "            rj = ri + dr_off\n",
    "            if 0 <= rj < hist_s.shape[1]:\n",
    "                val = hist_s[ti, rj]\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    # Coordenadas reales centradas\n",
    "                    θc = te[ti] + dθ / 2\n",
    "                    rc = re[rj] + dr / 2\n",
    "                    best_pt = (θc, rc)\n",
    "        if best_pt:\n",
    "            path.append(best_pt)\n",
    "    return path\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, te, re, density_ratio=0.25, theta_step=2):\n",
    "    internal = [(a, b) for ske in skeletons for a, b in zip(ske, ske[1:])]\n",
    "    external = []\n",
    "    density_paths = []\n",
    "    for i, ske in enumerate(skeletons):\n",
    "        if not ske:\n",
    "            continue\n",
    "        p = ske[-1]\n",
    "        best = None\n",
    "        bd = math.inf\n",
    "        for j, ske2 in enumerate(skeletons):\n",
    "            if i == j or not ske2:\n",
    "                continue\n",
    "            q = ske2[0]\n",
    "            drd = q[1] - p[1]\n",
    "            dtd = q[0] - p[0]\n",
    "            if drd > 0 and dtd < 0:\n",
    "                dist = math.hypot(dtd, drd)\n",
    "                if dist < bd:\n",
    "                    bd = dist\n",
    "                    best = (p, q)\n",
    "        if best:\n",
    "            external.append(best)\n",
    "            radius = bd * density_ratio\n",
    "            path = compute_density_path(best[0], best[1], hist_s, te, re, radius, theta_step)\n",
    "            density_paths.extend(path)\n",
    "    return internal, external, density_paths\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(clusters, skeletons, hist_s, te, re, internal, external, density_paths=None):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[te[0], te[-1], re[0], re[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k')\n",
    "    # Conexiones internas en blanco\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0], b[0]], [a[1], b[1]], '-', color='white', linewidth=2)\n",
    "    # Conexiones externas\n",
    "    for p, q in external:\n",
    "        plt.plot([p[0], q[0]], [p[1], q[1]], '--', color='white', linewidth=2.5)\n",
    "    # Camino de densidad en cian\n",
    "    if density_paths:\n",
    "        dp = np.array(density_paths)\n",
    "        plt.scatter(dp[:,0], dp[:,1], c='cyan', s=30, marker='x', label='Camino densidad')\n",
    "    plt.title('Islas, conexiones y camino de densidad')\n",
    "    plt.xlabel('θ (°)')\n",
    "    plt.ylabel('r')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    clusters, skeletons, hist_s, te, re = main_histogram_weighted_bfs_clusters()\n",
    "    internal, external, path = connect_skeletons(skeletons, hist_s, te, re)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, te, re, internal, external, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e1e5a2-a28f-432d-ab4f-e4807140d1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D + conexiones dinámicas de centroides basadas en densidad:\n",
    "- Conecta islas separadas por huecos ≤ max_gap_dist\n",
    "- Calcula skeletons de centroides cada theta_bin_size grados\n",
    "- Conecta internamente centroides adyacentes en cada isla\n",
    "- Para cada isla, conecta el extremo mínimo θ con el extremo máximo θ de la isla más cercana (dr>0, dθ<0)\n",
    "- Además, genera un camino de centroides adicionales siguiendo los máximos de densidad en background\n",
    "  recorriendo el segmento a pasos de theta_step grados y buscando el punto de máxima densidad radial dentro de radius\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq, math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n): self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb: self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100, bins_r=60, smooth_sigma=1.0,\n",
    "    density_percentile=80, closing_size=2,\n",
    "    min_cluster_size=40, max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos = main_example_modificado()\n",
    "    bg = datos['background'][['theta','r']].values\n",
    "    grp = np.vstack([g['points'][['theta','r']].values for g in datos['grupos_finales']]) if datos['grupos_finales'] else np.empty((0,2))\n",
    "    pts = np.vstack([bg, grp])\n",
    "    hist, te, re = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size, closing_size)))\n",
    "    labels, n_lab = label(mask_c)\n",
    "\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    neigh     = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf        = UnionFind(n_lab+1)\n",
    "    h, w      = labels.shape\n",
    "    dist_grid = np.full((h, w), np.inf)\n",
    "    pq        = []\n",
    "\n",
    "    # BFS frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if labels[i,j] > 0:\n",
    "                for di, dj in neigh:\n",
    "                    ni, nj = i+di, j+dj\n",
    "                    if 0<=ni<h and 0<=nj<w and labels[ni,nj]==0:\n",
    "                        d0 = math.hypot(di*theta_res, dj*r_res)\n",
    "                        if d0<=max_gap_dist and d0<dist_grid[ni,nj]:\n",
    "                            dist_grid[ni,nj] = d0\n",
    "                            heapq.heappush(pq, (d0, ni, nj, labels[i,j]))\n",
    "    while pq:\n",
    "        dist,i,j,lab = heapq.heappop(pq)\n",
    "        if dist>dist_grid[i,j] or dist>max_gap_dist: continue\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if 0<=ni<h and 0<=nj<w:\n",
    "                lab2 = labels[ni,nj]\n",
    "                if lab2>0 and lab2!=lab: uf.union(lab, lab2)\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if 0<=ni<h and 0<=nj<w and labels[ni,nj]==0:\n",
    "                nd = dist + math.hypot(di*theta_res, dj*r_res)\n",
    "                if nd<=max_gap_dist and nd<dist_grid[ni,nj]:\n",
    "                    dist_grid[ni,nj] = nd\n",
    "                    heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Formar clusters\n",
    "    merged = defaultdict(list)\n",
    "    tidx = np.clip(np.digitize(pts[:,0], te)-1, 0, bins_theta-1)\n",
    "    ridx = np.clip(np.digitize(pts[:,1], re)-1, 0, bins_r-1)\n",
    "    for p, (ti, ri) in enumerate(zip(tidx, ridx)):\n",
    "        lab = labels[ti, ri]\n",
    "        if lab>0: merged[uf.find(lab)].append(pts[p])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v)>=min_cluster_size]\n",
    "\n",
    "    # Skeletons\n",
    "    ske_list = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins_arr = np.arange(thetas.min(), thetas.max()+theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins_arr)-1):\n",
    "            mask_block = (thetas>=bins_arr[k]) & (thetas<bins_arr[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        ske_list.append(sorted(ske, key=lambda x:-x[0]))\n",
    "    return clusters, ske_list, hist_s, te, re\n",
    "\n",
    "\n",
    "def compute_density_path(p, q, hist_s, te, re, radius, theta_step=2):\n",
    "    \"\"\"\n",
    "    Para cada paso en θ a lo largo de p->q, busca radialmente dentro de radius\n",
    "    el punto de máxima densidad en hist_s.\n",
    "    \"\"\"\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    total_dθ  = q[0] - p[0]\n",
    "    steps = max(int(abs(total_dθ)/theta_step)+1, 2)\n",
    "    thetas = np.linspace(p[0], q[0], steps)\n",
    "    rs_vals = np.linspace(p[1], q[1], steps)\n",
    "    path = []\n",
    "    for θ_i, r_i in zip(thetas, rs_vals):\n",
    "        ti = int(np.clip(np.digitize(θ_i, te)-1, 0, hist_s.shape[0]-1))\n",
    "        ri = int(np.clip(np.digitize(r_i, re)-1, 0, hist_s.shape[1]-1))\n",
    "        best_pt = None\n",
    "        best_val = -np.inf\n",
    "        max_off = int(np.ceil(radius / r_res))\n",
    "        for off in range(-max_off, max_off+1):\n",
    "            idx_r = ri + off\n",
    "            if 0<=idx_r<hist_s.shape[1]:\n",
    "                val = hist_s[ti, idx_r]\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    θc = te[ti] + theta_res/2\n",
    "                    rc = re[idx_r] + r_res/2\n",
    "                    best_pt = (θc, rc)\n",
    "        if best_pt:\n",
    "            path.append(best_pt)\n",
    "    return path\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, te, re, density_ratio=0.25, theta_step=2):\n",
    "    internal = [(a, b) for ske in skeletons for a, b in zip(ske, ske[1:])]\n",
    "    external = []\n",
    "    density_paths = []\n",
    "    for i, ske in enumerate(skeletons):\n",
    "        if not ske: continue\n",
    "        p = ske[-1]\n",
    "        best, bd = None, math.inf\n",
    "        for j, ske2 in enumerate(skeletons):\n",
    "            if i==j or not ske2: continue\n",
    "            q = ske2[0]\n",
    "            drd = q[1] - p[1]\n",
    "            dtd = q[0] - p[0]\n",
    "            if drd>0 and dtd<0:\n",
    "                dist = math.hypot(dtd, drd)\n",
    "                if dist<bd: bd, best = dist, (p,q)\n",
    "        if best:\n",
    "            external.append(best)\n",
    "            radius = bd * density_ratio\n",
    "            density_paths.extend(compute_density_path(best[0], best[1], hist_s, te, re, radius, theta_step))\n",
    "    return internal, external, density_paths\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(clusters, skeletons, hist_s, te, re, internal, external, density_paths=None):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[te[0], te[-1], re[0], re[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k')\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0], b[0]], [a[1], b[1]], '-', color='white', linewidth=2)\n",
    "    for p, q in external:\n",
    "        plt.plot([p[0], q[0]], [p[1], q[1]], '--', color='white', linewidth=2.5)\n",
    "    if density_paths:\n",
    "        dp = np.array(density_paths)\n",
    "        plt.scatter(dp[:,0], dp[:,1], c='cyan', s=30, marker='x', label='Camino densidad')\n",
    "    plt.title('Islas, conexiones y camino de densidad')\n",
    "    plt.xlabel('θ (°)'); plt.ylabel('r (units)')\n",
    "    plt.legend(loc='upper right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    clusters, skeletons, hist_s, te, re = main_histogram_weighted_bfs_clusters()\n",
    "    internal, external, path = connect_skeletons(skeletons, hist_s, te, re)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, te, re, internal, external, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd3bf4d-9168-4e55-8aa7-f374916f29d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación ligera + BFS ponderado sobre histograma 2D + conexiones dinámicas de centroides basadas en densidad:\n",
    "- Conecta islas separadas por huecos ≤ max_gap_dist\n",
    "- Calcula skeletons de centroides cada theta_bin_size grados\n",
    "- Conecta internamente centroides adyacentes en cada isla\n",
    "- Para cada isla, conecta el extremo mínimo θ con el extremo máximo θ de la isla más cercana (dr>0, dθ<0)\n",
    "- Además, genera un camino de centroides adicionales siguiendo los máximos de densidad en background\n",
    "  recorriendo el segmento a pasos de theta_step grados y buscando el punto de máxima densidad radial dentro de radius\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq, math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n): self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb: self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100, bins_r=60, smooth_sigma=1.0,\n",
    "    density_percentile=80, closing_size=2,\n",
    "    min_cluster_size=40, max_gap_dist=0.5,\n",
    "    theta_bin_size=8\n",
    "):\n",
    "    datos = main_example_modificado()\n",
    "    bg = datos['background'][['theta','r']].values\n",
    "    grp = np.vstack([g['points'][['theta','r']].values for g in datos['grupos_finales']]) if datos['grupos_finales'] else np.empty((0,2))\n",
    "    pts = np.vstack([bg, grp])\n",
    "    hist, te, re = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "    thr = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size, closing_size)))\n",
    "    labels, n_lab = label(mask_c)\n",
    "\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    neigh     = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf        = UnionFind(n_lab+1)\n",
    "    h, w      = labels.shape\n",
    "    dist_grid = np.full((h, w), np.inf)\n",
    "    pq        = []\n",
    "\n",
    "    # BFS frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if labels[i,j] > 0:\n",
    "                for di, dj in neigh:\n",
    "                    ni, nj = i+di, j+dj\n",
    "                    if 0<=ni<h and 0<=nj<w and labels[ni,nj]==0:\n",
    "                        d0 = math.hypot(di*theta_res, dj*r_res)\n",
    "                        if d0<=max_gap_dist and d0<dist_grid[ni,nj]:\n",
    "                            dist_grid[ni,nj] = d0\n",
    "                            heapq.heappush(pq, (d0, ni, nj, labels[i,j]))\n",
    "    while pq:\n",
    "        dist,i,j,lab = heapq.heappop(pq)\n",
    "        if dist>dist_grid[i,j] or dist>max_gap_dist: continue\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if 0<=ni<h and 0<=nj<w:\n",
    "                lab2 = labels[ni,nj]\n",
    "                if lab2>0 and lab2!=lab: uf.union(lab, lab2)\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if 0<=ni<h and 0<=nj<w and labels[ni,nj]==0:\n",
    "                nd = dist + math.hypot(di*theta_res, dj*r_res)\n",
    "                if nd<=max_gap_dist and nd<dist_grid[ni,nj]:\n",
    "                    dist_grid[ni,nj] = nd\n",
    "                    heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Formar clusters\n",
    "    merged = defaultdict(list)\n",
    "    tidx = np.clip(np.digitize(pts[:,0], te)-1, 0, bins_theta-1)\n",
    "    ridx = np.clip(np.digitize(pts[:,1], re)-1, 0, bins_r-1)\n",
    "    for p, (ti, ri) in enumerate(zip(tidx, ridx)):\n",
    "        lab = labels[ti, ri]\n",
    "        if lab>0: merged[uf.find(lab)].append(pts[p])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v)>=min_cluster_size]\n",
    "\n",
    "    # Skeletons\n",
    "    ske_list = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins_arr = np.arange(thetas.min(), thetas.max()+theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins_arr)-1):\n",
    "            mask_block = (thetas>=bins_arr[k]) & (thetas<bins_arr[k+1])\n",
    "            if mask_block.any():\n",
    "                ske.append((thetas[mask_block].mean(), rs[mask_block].mean()))\n",
    "        ske_list.append(sorted(ske, key=lambda x:-x[0]))\n",
    "    return clusters, ske_list, hist_s, te, re\n",
    "\n",
    "\n",
    "def compute_density_path(p, q, hist_s, te, re, radius, theta_step=2):\n",
    "    \"\"\"\n",
    "    Para cada paso en θ a lo largo de p->q:\n",
    "    - samplea radios alrededor del punto en [−radius,+radius]\n",
    "    - identifica todos los máximos locales en el perfil radial (val > vecinos inmediatos)\n",
    "      con densidad > 0 como centroides débiles\n",
    "    - construye coordenadas reales de cada máximo detectado\n",
    "    \"\"\"\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    total_dθ  = q[0] - p[0]\n",
    "    steps     = max(int(abs(total_dθ)/theta_step) + 1, 2)\n",
    "    thetas    = np.linspace(p[0], q[0], steps)\n",
    "    rs_vals   = np.linspace(p[1], q[1], steps)\n",
    "    path = []\n",
    "    for θ_i, r_i in zip(thetas, rs_vals):\n",
    "        # índice central\n",
    "        ti = int(np.clip(np.digitize(θ_i, te) - 1, 0, hist_s.shape[0] - 1))\n",
    "        ri0 = int(np.clip(np.digitize(r_i, re) - 1, 0, hist_s.shape[1] - 1))\n",
    "        max_off = int(np.ceil(radius / r_res))\n",
    "        # perfil radial\n",
    "        vals = []\n",
    "        offs = []\n",
    "        for off in range(-max_off, max_off + 1):\n",
    "            idx_r = ri0 + off\n",
    "            if 0 <= idx_r < hist_s.shape[1]:\n",
    "                vals.append(hist_s[ti, idx_r])\n",
    "                offs.append(off)\n",
    "        # detectar máximos locales sobre perfil vals\n",
    "        for k in range(1, len(vals)-1):\n",
    "            if vals[k] > vals[k-1] and vals[k] > vals[k+1] and vals[k] > 0:\n",
    "                off = offs[k]\n",
    "                θc = te[ti] + theta_res / 2\n",
    "                rc = (re[ri0 + off] + r_res/2) if 0 <= ri0+off < len(re) else r_i\n",
    "                path.append((θc, rc))\n",
    "    return path\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, te, re, density_ratio=0.25, theta_step=5):\n",
    "    internal = [(a, b) for ske in skeletons for a, b in zip(ske, ske[1:])]\n",
    "    external = []\n",
    "    density_paths = []\n",
    "    for i, ske in enumerate(skeletons):\n",
    "        if not ske: continue\n",
    "        p = ske[-1]\n",
    "        best, bd = None, math.inf\n",
    "        for j, ske2 in enumerate(skeletons):\n",
    "            if i==j or not ske2: continue\n",
    "            q = ske2[0]\n",
    "            drd = q[1] - p[1]\n",
    "            dtd = q[0] - p[0]\n",
    "            if drd>0 and dtd<0:\n",
    "                dist = math.hypot(dtd, drd)\n",
    "                if dist<bd: bd, best = dist, (p,q)\n",
    "        if best:\n",
    "            external.append(best)\n",
    "            radius = bd * density_ratio\n",
    "            density_paths.extend(compute_density_path(best[0], best[1], hist_s, te, re, radius, theta_step))\n",
    "    return internal, external, density_paths\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(clusters, skeletons, hist_s, te, re, internal, external, density_paths=None):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[te[0], te[-1], re[0], re[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=20, edgecolors='k')\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0], b[0]], [a[1], b[1]], '-', color='white', linewidth=2)\n",
    "    for p, q in external:\n",
    "        plt.plot([p[0], q[0]], [p[1], q[1]], '--', color='white', linewidth=2.5)\n",
    "    if density_paths:\n",
    "        dp = np.array(density_paths)\n",
    "        plt.scatter(dp[:,0], dp[:,1], c='red', s=10, marker='o', edgecolors='k' ,label='Camino densidad')\n",
    "    plt.title('Islas, conexiones y camino de densidad')\n",
    "    plt.xlabel('θ (°)'); plt.ylabel('r (units)')\n",
    "    plt.legend(loc='upper right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    clusters, skeletons, hist_s, te, re = main_histogram_weighted_bfs_clusters()\n",
    "    internal, external, path = connect_skeletons(skeletons, hist_s, te, re)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, te, re, internal, external, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e7255aa-2e2e-4db0-9da4-773c05e726e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7424a3cd-a3e2-487b-9a9f-7e1f7d98997a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación + BFS + conexiones dinámicas + contornos filtrados:\n",
    "- Detecta islas en histograma 2D suavizado\n",
    "- Calcula skeletons de centroides cada theta_bin_size grados\n",
    "- Conecta internamente centroides adyacentes\n",
    "- Para cada isla, conecta el extremo mínimo θ con el extremo máximo θ de la isla más cercana (dr>0, dθ<0)\n",
    "- Genera puntos adicionales por máximos locales de densidad en background a lo largo del segmento\n",
    "- Solo retiene esos puntos si |r_c - r_segmento| ≤ r_threshold (0.9)\n",
    "- Construye contornos cerrados con esos centroides y los dibuja en un segundo plot\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq, math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self,n): self.parent=list(range(n))\n",
    "    def find(self,x):\n",
    "        while self.parent[x]!=x:\n",
    "            self.parent[x]=self.parent[self.parent[x]]\n",
    "            x=self.parent[x]\n",
    "        return x\n",
    "    def union(self,a,b):\n",
    "        ra,rb=self.find(a),self.find(b)\n",
    "        if ra!=rb: self.parent[rb]=ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100, bins_r=60, smooth_sigma=1.0,\n",
    "    density_percentile=80, closing_size=2,\n",
    "    min_cluster_size=40, max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos = main_example_modificado()\n",
    "    bg    = datos['background'][['theta','r']].values\n",
    "    grp   = np.vstack([g['points'][['theta','r']].values for g in datos['grupos_finales']])\n",
    "    pts   = np.vstack([bg, grp])\n",
    "\n",
    "    # Histograma 2D + suavizado\n",
    "    hist, te, re = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s       = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    # Segmentación y etiquetado\n",
    "    thr    = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size,closing_size)))\n",
    "    labels, n_lab = label(mask_c)\n",
    "\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    neigh     = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf        = UnionFind(n_lab+1)\n",
    "    h, w      = labels.shape\n",
    "    dist_grid = np.full((h,w), np.inf)\n",
    "    pq        = []\n",
    "\n",
    "    # BFS ponderado en frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if labels[i,j] > 0:\n",
    "                for di,dj in neigh:\n",
    "                    ni,nj = i+di, j+dj\n",
    "                    if 0 <= ni < h and 0 <= nj < w and labels[ni,nj] == 0:\n",
    "                        d0 = math.hypot(di*theta_res, dj*r_res)\n",
    "                        if d0 <= max_gap_dist and d0 < dist_grid[ni,nj]:\n",
    "                            dist_grid[ni,nj] = d0\n",
    "                            heapq.heappush(pq, (d0, ni, nj, labels[i,j]))\n",
    "    while pq:\n",
    "        dist,i,j,lab = heapq.heappop(pq)\n",
    "        if dist > dist_grid[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di,dj in neigh:\n",
    "            ni,nj = i+di, j+dj\n",
    "            if 0 <= ni < h and 0 <= nj < w and labels[ni,nj] == 0:\n",
    "                nd = dist + math.hypot(di*theta_res, dj*r_res)\n",
    "                if nd <= max_gap_dist and nd < dist_grid[ni,nj]:\n",
    "                    dist_grid[ni,nj] = nd\n",
    "                    heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Recolectar clusters\n",
    "    merged = defaultdict(list)\n",
    "    tidx   = np.clip(np.digitize(pts[:,0], te) - 1, 0, bins_theta-1)\n",
    "    ridx   = np.clip(np.digitize(pts[:,1], re) - 1, 0, bins_r-1)\n",
    "    for p, (ti, ri) in enumerate(zip(tidx, ridx)):\n",
    "        lab = labels[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[p])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    # Skeletons\n",
    "    skeletons = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins_arr    = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins_arr)-1):\n",
    "            m = (thetas >= bins_arr[k]) & (thetas < bins_arr[k+1])\n",
    "            if m.any():\n",
    "                ske.append((thetas[m].mean(), rs[m].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, te, re\n",
    "\n",
    "\n",
    "def compute_density_path(p, q, hist_s, te, re, radius,\n",
    "                         theta_step=2, r_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Muestrea el segmento p->q en θ cada theta_step,\n",
    "    para cada muestra explora radialmente dentro de ±radius,\n",
    "    detecta máximos locales y sólo retiene aquellos con\n",
    "    |r_c - r_segmento| ≤ r_threshold.\n",
    "    \"\"\"\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    total_dθ  = q[0] - p[0]\n",
    "    steps     = max(int(abs(total_dθ)/theta_step) + 1, 2)\n",
    "    thetas    = np.linspace(p[0], q[0], steps)\n",
    "    rs_vals   = np.linspace(p[1], q[1], steps)\n",
    "    path = []\n",
    "    for θ_i, r_i in zip(thetas, rs_vals):\n",
    "        ti  = int(np.clip(np.digitize(θ_i, te) - 1, 0, hist_s.shape[0] - 1))\n",
    "        ri0 = int(np.clip(np.digitize(r_i, re) - 1, 0, hist_s.shape[1] - 1))\n",
    "        max_off = int(np.ceil(radius / r_res))\n",
    "        vals, offs = [], []\n",
    "        for off in range(-max_off, max_off + 1):\n",
    "            idx_r = ri0 + off\n",
    "            if 0 <= idx_r < hist_s.shape[1]:\n",
    "                vals.append(hist_s[ti, idx_r])\n",
    "                offs.append(off)\n",
    "        for k in range(1, len(vals)-1):\n",
    "            if vals[k] > vals[k-1] and vals[k] > vals[k+1] and vals[k] > 0:\n",
    "                off = offs[k]\n",
    "                θc  = te[ti] + theta_res/2\n",
    "                rc  = (re[ri0+off] + r_res/2) if 0 <= ri0+off < len(re) else r_i\n",
    "                if abs(rc - r_i) <= r_threshold:\n",
    "                    path.append((θc, rc))\n",
    "    return path\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, te, re,\n",
    "                      density_ratio=0.25, theta_step=2, r_threshold=0.9):\n",
    "    internal = []\n",
    "    contours = []\n",
    "    # internas\n",
    "    for ske in skeletons:\n",
    "        internal += list(zip(ske, ske[1:]))\n",
    "    # contornos cerrados\n",
    "    for i, ske in enumerate(skeletons):\n",
    "        if not ske:\n",
    "            continue\n",
    "        p = ske[-1]\n",
    "        best, bd = None, math.inf\n",
    "        for j, ske2 in enumerate(skeletons):\n",
    "            if i == j or not ske2:\n",
    "                continue\n",
    "            q = ske2[0]\n",
    "            drd, dtd = q[1] - p[1], q[0] - p[0]\n",
    "            if drd > 0 and dtd < 0:\n",
    "                d = math.hypot(dtd, drd)\n",
    "                if d < bd:\n",
    "                    bd, best = d, (p, q)\n",
    "        if best:\n",
    "            radius = bd * density_ratio\n",
    "            path   = compute_density_path(best[0], best[1], hist_s, te, re,\n",
    "                                           radius, theta_step, r_threshold)\n",
    "            contour = [best[0]] + path + [best[1]]\n",
    "            contours.append(contour)\n",
    "    return internal, contours\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(\n",
    "    clusters, skeletons, hist_s, te, re,\n",
    "    internal, contours\n",
    "):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[te[0],te[-1],re[0],re[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k')\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0], b[0]], [a[1], b[1]], '-', color='white', linewidth=2)\n",
    "    for cnt in contours:\n",
    "        xs, ys = zip(*cnt)\n",
    "        plt.plot(xs, ys, '-o', color='red', markersize=4, linewidth=2, alpha=0.8)\n",
    "    plt.title('Islas, conexiones y contornos filtrados')\n",
    "    plt.xlabel('θ (°)'); plt.ylabel('r (units)')\n",
    "    plt.legend(loc='upper right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    clusters, skeletons, hist_s, te, re = main_histogram_weighted_bfs_clusters()\n",
    "    internal, contours = connect_skeletons(skeletons, hist_s, te, re)\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, te, re, internal, contours)\n",
    "    bg    = datos['background'][['theta','r']].values\n",
    "    grp   = np.vstack([g['points'][['theta','r']].values for g in datos['grupos_finales']])\n",
    "    pts   = np.vstack([bg, grp])\n",
    "\n",
    "    hist, te, re = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s       = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    thr    = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size,closing_size)))\n",
    "    labels, n_lab = label(mask_c)\n",
    "\n",
    "    theta_res = te[1]-te[0]\n",
    "    r_res     = re[1]-re[0]\n",
    "    neigh     = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf        = UnionFind(n_lab+1)\n",
    "    h, w      = labels.shape\n",
    "    dist_grid = np.full((h,w), np.inf)\n",
    "    pq        = []\n",
    "\n",
    "    # BFS ponderado en frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if labels[i,j]>0:\n",
    "                for di,dj in neigh:\n",
    "                    ni,nj = i+di, j+dj\n",
    "                    if 0<=ni<h and 0<=nj<w and labels[ni,nj]==0:\n",
    "                        d0 = math.hypot(di*theta_res, dj*r_res)\n",
    "                        if d0<=max_gap_dist and d0<dist_grid[ni,nj]:\n",
    "                            dist_grid[ni,nj]=d0\n",
    "                            heapq.heappush(pq,(d0,ni,nj,labels[i,j]))\n",
    "    while pq:\n",
    "        dist,i,j,lab=heapq.heappop(pq)\n",
    "        if dist>dist_grid[i,j] or dist>max_gap_dist: continue\n",
    "        for di,dj in neigh:\n",
    "            ni,nj=i+di, j+dj\n",
    "            if 0<=ni<h and 0<=nj<w:\n",
    "                lab2 = labels[ni,nj]\n",
    "                if lab2>0 and lab2!=lab: uf.union(lab, lab2)\n",
    "        for di,dj in neigh:\n",
    "            ni,nj=i+di, j+dj\n",
    "            if 0<=ni<h and 0<=nj<w and labels[ni,nj]==0:\n",
    "                nd = dist + math.hypot(di*theta_res, dj*r_res)\n",
    "                if nd<=max_gap_dist and nd<dist_grid[ni,nj]:\n",
    "                    dist_grid[ni,nj]=nd\n",
    "                    heapq.heappush(pq,(nd,ni,nj,lab))\n",
    "\n",
    "    # Recolectar clusters\n",
    "    merged=defaultdict(list)\n",
    "    tidx=np.clip(np.digitize(pts[:,0],te)-1,0,bins_theta-1)\n",
    "    ridx=np.clip(np.digitize(pts[:,1],re)-1,0,bins_r-1)\n",
    "    for p,(ti,ri) in enumerate(zip(tidx,ridx)):\n",
    "        lab=labels[ti,ri]\n",
    "        if lab>0: merged[uf.find(lab)].append(pts[p])\n",
    "    clusters=[np.vstack(v) for v in merged.values() if len(v)>=min_cluster_size]\n",
    "\n",
    "    # Skeletons\n",
    "    skeletons=[]\n",
    "    for c in clusters:\n",
    "        thetas,rs=c[:,0],c[:,1]\n",
    "        bins_arr=np.arange(thetas.min(), thetas.max()+theta_bin_size, theta_bin_size)\n",
    "        ske=[]\n",
    "        for k in range(len(bins_arr)-1):\n",
    "            m=(thetas>=bins_arr[k])&(thetas<bins_arr[k+1])\n",
    "            if m.any(): ske.append((thetas[m].mean(), rs[m].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x:-x[0]))\n",
    "\n",
    "        return clusters, skeletons, hist_s, te, re\n",
    "\n",
    "\n",
    "def compute_density_path(p, q, hist_s, te, re, radius,\n",
    "                         theta_step=2, r_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Muestrea el segmento p->q en θ cada theta_step,\n",
    "    para cada muestra explora radialmente dentro de ±radius,\n",
    "    detecta máximos locales y sólo retiene aquellos con\n",
    "    |r_c - r_segmento| ≤ r_threshold.\n",
    "    \"\"\"\n",
    "    theta_res=te[1]-te[0]\n",
    "    r_res    =re[1]-re[0]\n",
    "    total_dθ =q[0]-p[0]\n",
    "    steps    =max(int(abs(total_dθ)/theta_step)+1,2)\n",
    "    thetas   =np.linspace(p[0], q[0], steps)\n",
    "    rs_vals  =np.linspace(p[1], q[1], steps)\n",
    "    path=[]\n",
    "    for θ_i, r_i in zip(thetas, rs_vals):\n",
    "        ti  =int(np.clip(np.digitize(θ_i,te)-1,0,hist_s.shape[0]-1))\n",
    "        ri0 =int(np.clip(np.digitize(r_i,re)-1,0,hist_s.shape[1]-1))\n",
    "        max_off=int(np.ceil(radius/r_res))\n",
    "        vals, offs = [], []\n",
    "        for off in range(-max_off, max_off+1):\n",
    "            idx_r=ri0+off\n",
    "            if 0<=idx_r<hist_s.shape[1]:\n",
    "                vals.append(hist_s[ti, idx_r]); offs.append(off)\n",
    "        for k in range(1,len(vals)-1):\n",
    "            if vals[k]>vals[k-1] and vals[k]>vals[k+1] and vals[k]>0:\n",
    "                off=offs[k]\n",
    "                θc = te[ti] + theta_res/2\n",
    "                rc = (re[ri0+off] + r_res/2) if 0<=ri0+off<len(re) else r_i\n",
    "                if abs(rc - r_i) <= r_threshold:\n",
    "                    path.append((θc, rc))\n",
    "    return path\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, te, re,\n",
    "                      density_ratio=0.25, theta_step=2, r_threshold=0.9):\n",
    "    internal=[]; contours=[]\n",
    "    # internas\n",
    "    for ske in skeletons:\n",
    "        internal += list(zip(ske, ske[1:]))\n",
    "    # contornos cerrados\n",
    "    for i, ske in enumerate(skeletons):\n",
    "        if not ske: continue\n",
    "        p=ske[-1]; best, bd=None, math.inf\n",
    "        for j, ske2 in enumerate(skeletons):\n",
    "            if i==j or not ske2: continue\n",
    "            q=ske2[0]\n",
    "            drd, dtd = q[1]-p[1], q[0]-p[0]\n",
    "            if drd>0 and dtd<0:\n",
    "                d=math.hypot(dtd,drd)\n",
    "                if d<bd: bd, best=d, (p,q)\n",
    "        if best:\n",
    "            radius=bd*density_ratio\n",
    "            path=compute_density_path(best[0],best[1],hist_s,te,re,\n",
    "                                      radius, theta_step, r_threshold)\n",
    "            contour=[best[0]] + path + [best[1]]\n",
    "            contours.append(contour)\n",
    "    return internal, contours\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(\n",
    "    clusters, skeletons, hist_s, te, re,\n",
    "    internal, contours\n",
    "):\n",
    "    # gráfico principal\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower',\n",
    "               extent=[te[0],te[-1],re[0],re[-1]],\n",
    "               aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6,\n",
    "                    label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40,\n",
    "                        edgecolors='k')\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0],b[0]],[a[1],b[1]], '-',\n",
    "                 color='white', linewidth=2)\n",
    "    for cnt in contours:\n",
    "        xs, ys = zip(*cnt)\n",
    "        plt.plot(xs, ys, '-o', color='red',\n",
    "                 markersize=4, linewidth=2, alpha=0.8)\n",
    "    plt.title('Islas, conexiones y contornos filtrados')\n",
    "    plt.xlabel('θ (°)'); plt.ylabel('r');\n",
    "    plt.legend(loc='upper right'); plt.tight_layout();\n",
    "    plt.show()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    clusters, skeletons, hist_s, te, re = \\\n",
    "        main_histogram_weighted_bfs_clusters()\n",
    "    internal, contours = \\\n",
    "        connect_skeletons(skeletons, hist_s, te, re)\n",
    "    plot_clusters_and_connections(\n",
    "        clusters, skeletons,\n",
    "        hist_s, te, re,\n",
    "        internal, contours\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1bc2b8-4b66-4d8e-a4ef-e41c3aca930f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### version caminos ok "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2c9abf-bfc5-427e-98be-e7a3d92639d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación + BFS + conexiones dinámicas + contornos filtrados:\n",
    "- Detecta islas en histograma 2D suavizado\n",
    "- Calcula skeletons de centroides cada theta_bin_size grados\n",
    "- Conecta internamente centroides adyacentes\n",
    "- Para cada isla, conecta el extremo mínimo θ con el extremo máximo θ de la isla más cercana (dr>0, dθ<0)\n",
    "- Genera puntos adicionales por máximos locales de densidad en background a lo largo del segmento\n",
    "- Solo retiene esos puntos si |r_c - r_segmento| ≤ r_threshold (0.9)\n",
    "- Construye contornos cerrados con esos centroides y los dibuja en un segundo plot\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq, math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self,n): self.parent=list(range(n))\n",
    "    def find(self,x):\n",
    "        while self.parent[x]!=x:\n",
    "            self.parent[x]=self.parent[self.parent[x]]\n",
    "            x=self.parent[x]\n",
    "        return x\n",
    "    def union(self,a,b):\n",
    "        ra,rb=self.find(a),self.find(b)\n",
    "        if ra!=rb: self.parent[rb]=ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100, bins_r=60, smooth_sigma=1.0,\n",
    "    density_percentile=80, closing_size=2,\n",
    "    min_cluster_size=40, max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos = main_example_modificado()\n",
    "    bg    = datos['background'][['theta','r']].values\n",
    "    grp   = np.vstack([g['points'][['theta','r']].values for g in datos['grupos_finales']])\n",
    "    pts   = np.vstack([bg, grp])\n",
    "\n",
    "    hist, te, re = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s       = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    thr    = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size,closing_size)))\n",
    "    labels, n_lab = label(mask_c)\n",
    "\n",
    "    theta_res = te[1]-te[0]\n",
    "    r_res     = re[1]-re[0]\n",
    "    neigh     = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf        = UnionFind(n_lab+1)\n",
    "    h, w      = labels.shape\n",
    "    dist_grid = np.full((h,w), np.inf)\n",
    "    pq        = []\n",
    "\n",
    "    # BFS ponderado en frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if labels[i,j]>0:\n",
    "                for di,dj in neigh:\n",
    "                    ni,nj = i+di, j+dj\n",
    "                    if 0<=ni<h and 0<=nj<w and labels[ni,nj]==0:\n",
    "                        d0 = math.hypot(di*theta_res, dj*r_res)\n",
    "                        if d0<=max_gap_dist and d0<dist_grid[ni,nj]:\n",
    "                            dist_grid[ni,nj]=d0\n",
    "                            heapq.heappush(pq,(d0,ni,nj,labels[i,j]))\n",
    "    while pq:\n",
    "        dist,i,j,lab=heapq.heappop(pq)\n",
    "        if dist>dist_grid[i,j] or dist>max_gap_dist: continue\n",
    "        for di,dj in neigh:\n",
    "            ni,nj=i+di, j+dj\n",
    "            if 0<=ni<h and 0<=nj<w:\n",
    "                lab2 = labels[ni,nj]\n",
    "                if lab2>0 and lab2!=lab: uf.union(lab, lab2)\n",
    "        for di,dj in neigh:\n",
    "            ni,nj=i+di, j+dj\n",
    "            if 0<=ni<h and 0<=nj<w and labels[ni,nj]==0:\n",
    "                nd = dist + math.hypot(di*theta_res, dj*r_res)\n",
    "                if nd<=max_gap_dist and nd<dist_grid[ni,nj]:\n",
    "                    dist_grid[ni,nj]=nd\n",
    "                    heapq.heappush(pq,(nd,ni,nj,lab))\n",
    "\n",
    "    # Recolectar clusters\n",
    "    merged=defaultdict(list)\n",
    "    tidx=np.clip(np.digitize(pts[:,0],te)-1,0,bins_theta-1)\n",
    "    ridx=np.clip(np.digitize(pts[:,1],re)-1,0,bins_r-1)\n",
    "    for p,(ti,ri) in enumerate(zip(tidx,ridx)):\n",
    "        lab=labels[ti,ri]\n",
    "        if lab>0: merged[uf.find(lab)].append(pts[p])\n",
    "    clusters=[np.vstack(v) for v in merged.values() if len(v)>=min_cluster_size]\n",
    "\n",
    "    # Skeletons\n",
    "    skeletons=[]\n",
    "    for c in clusters:\n",
    "        thetas,rs=c[:,0],c[:,1]\n",
    "        bins_arr=np.arange(thetas.min(), thetas.max()+theta_bin_size, theta_bin_size)\n",
    "        ske=[]\n",
    "        for k in range(len(bins_arr)-1):\n",
    "            m=(thetas>=bins_arr[k])&(thetas<bins_arr[k+1])\n",
    "            if m.any(): ske.append((thetas[m].mean(), rs[m].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x:-x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, te, re\n",
    "\n",
    "\n",
    "def compute_density_path(p, q, hist_s, te, re, radius,\n",
    "                         theta_step=2, r_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Muestrea el segmento p->q en θ cada theta_step,\n",
    "    para cada muestra explora radialmente dentro de ±radius,\n",
    "    detecta máximos locales y sólo retiene aquellos con\n",
    "    |r_c - r_segmento| ≤ r_threshold.\n",
    "    \"\"\"\n",
    "    theta_res=te[1]-te[0]\n",
    "    r_res    =re[1]-re[0]\n",
    "    total_dθ =q[0]-p[0]\n",
    "    steps    =max(int(abs(total_dθ)/theta_step)+1,2)\n",
    "    thetas   =np.linspace(p[0], q[0], steps)\n",
    "    rs_vals  =np.linspace(p[1], q[1], steps)\n",
    "    path=[]\n",
    "    for θ_i, r_i in zip(thetas, rs_vals):\n",
    "        ti  =int(np.clip(np.digitize(θ_i,te)-1,0,hist_s.shape[0]-1))\n",
    "        ri0 =int(np.clip(np.digitize(r_i,re)-1,0,hist_s.shape[1]-1))\n",
    "        max_off=int(np.ceil(radius/r_res))\n",
    "        vals, offs = [], []\n",
    "        for off in range(-max_off, max_off+1):\n",
    "            idx_r=ri0+off\n",
    "            if 0<=idx_r<hist_s.shape[1]:\n",
    "                vals.append(hist_s[ti, idx_r]); offs.append(off)\n",
    "        for k in range(1,len(vals)-1):\n",
    "            if vals[k]>vals[k-1] and vals[k]>vals[k+1] and vals[k]>0:\n",
    "                off=offs[k]\n",
    "                θc = te[ti] + theta_res/2\n",
    "                rc = (re[ri0+off] + r_res/2) if 0<=ri0+off<len(re) else r_i\n",
    "                if abs(rc - r_i) <= r_threshold:\n",
    "                    path.append((θc, rc))\n",
    "    return path\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, te, re,\n",
    "                      density_ratio=0.25, theta_step=2, r_threshold=0.9):\n",
    "    internal=[]; contours=[]\n",
    "    # internas\n",
    "    for ske in skeletons:\n",
    "        internal += list(zip(ske, ske[1:]))\n",
    "    # contornos cerrados\n",
    "    for i, ske in enumerate(skeletons):\n",
    "        if not ske: continue\n",
    "        p=ske[-1]; best, bd=None, math.inf\n",
    "        for j, ske2 in enumerate(skeletons):\n",
    "            if i==j or not ske2: continue\n",
    "            q=ske2[0]\n",
    "            drd, dtd = q[1]-p[1], q[0]-p[0]\n",
    "            if drd>0 and dtd<0:\n",
    "                d=math.hypot(dtd,drd)\n",
    "                if d<bd: bd, best=d, (p,q)\n",
    "        if best:\n",
    "            radius=bd*density_ratio\n",
    "            path=compute_density_path(best[0],best[1],hist_s,te,re,\n",
    "                                      radius, theta_step, r_threshold)\n",
    "            contour=[best[0]] + path + [best[1]]\n",
    "            contours.append(contour)\n",
    "    return internal, contours\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(\n",
    "    clusters, skeletons, hist_s, te, re,\n",
    "    internal, contours\n",
    "):\n",
    "    # gráfico principal\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower',\n",
    "               extent=[te[0],te[-1],re[0],re[-1]],\n",
    "               aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters):\n",
    "        plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6,\n",
    "                    label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size:\n",
    "            plt.scatter(arr[:,0], arr[:,1], c='w', s=40,\n",
    "                        edgecolors='k')\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0],b[0]],[a[1],b[1]], '-',\n",
    "                 color='white', linewidth=2)\n",
    "    for cnt in contours:\n",
    "        xs, ys = zip(*cnt)\n",
    "        plt.plot(xs, ys, '-o', color='red',\n",
    "                 markersize=4, linewidth=2, alpha=0.8)\n",
    "    plt.title('Islas, conexiones y contornos filtrados')\n",
    "    plt.xlabel('θ (°)'); plt.ylabel('r');\n",
    "    plt.legend(loc='upper right'); plt.tight_layout();\n",
    "    plt.show()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    clusters, skeletons, hist_s, te, re = \\\n",
    "        main_histogram_weighted_bfs_clusters()\n",
    "    internal, contours = \\\n",
    "        connect_skeletons(skeletons, hist_s, te, re)\n",
    "    plot_clusters_and_connections(\n",
    "        clusters, skeletons,\n",
    "        hist_s, te, re,\n",
    "        internal, contours\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f94e5bc-adb0-411c-ad90-6c73aebd82e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## version cuasi_definitiva:\n",
    "\n",
    "    def __init__(self, bins_theta=100, bins_r=80, smooth_sigma=1.0,\n",
    "                 density_percentile=80, closing_size=2, min_cluster_size=40,\n",
    "                 max_gap_dist=0.45, theta_bin_size=8, density_ratio=0.25,\n",
    "                 theta_step=3, r_threshold=0.9, dr_multiplier=3.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9922d70-bb69-4c3a-8248-766f8848543b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Histogram Segmenter\n",
    "===================\n",
    "Módulo autocontenido y reutilizable para:\n",
    "1. Detección de \"islas\" en un histograma 2D (θ, r).\n",
    "2. Cálculo de esqueletos por bins angulares (θ).\n",
    "3. Generación **siempre** de centroides adicionales sobre *todas* las rectas\n",
    "   candidatas (pares de esqueletos contiguos e inter‑isla), muestreando la\n",
    "   densidad de fondo.\n",
    "4. Segmentación de contornos: las conexiones se parten si Δr > *dr_multiplier*·μ,\n",
    "   donde μ es el promedio de Δr de los segmentos generados.\n",
    "5. Clasificación de objetos resultantes:\n",
    "   • ``isla‑conexion‑isla``  (contornos válidos)\n",
    "   • ``isla‑conexiones‑adicionales``  (segmentos descartados > umbral)\n",
    "6. Salida estructurada + visualización opcional.\n",
    "\n",
    "El diseño busca **estabilidad** ante pequeños cambios en parámetros para poder\n",
    "aplicarlo a múltiples datasets sin retocar el código.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import heapq\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dataclasses de apoyo\n",
    "# ---------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class Connection:\n",
    "    a: Tuple[float, float]\n",
    "    b: Tuple[float, float]\n",
    "    delta_r: float\n",
    "    euclidean: float\n",
    "\n",
    "@dataclass\n",
    "class IslandObject:\n",
    "    type: str  # \"isla-conexion-isla\" | \"isla-conexiones-adicionales\"\n",
    "    boundary: List[Tuple[float, float]]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Algoritmo principal\n",
    "# ---------------------------------------------------------------------------\n",
    "class HistogramSegmenter:\n",
    "    \"\"\"Segmentador configurable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *bins_theta*, *bins_r* : resolución del histograma (θ, r).\n",
    "    *smooth_sigma*        : σ del filtro gaussiano.\n",
    "    *density_percentile*  : umbral de densidad para binarización.\n",
    "    *closing_size*        : tamaño del cierre morfológico.\n",
    "    *min_cluster_size*    : puntos mínimos por isla.\n",
    "    *max_gap_dist*        : distancia máx. para unir islas en BFS.\n",
    "    *theta_bin_size*      : paso angular para esqueletos.\n",
    "    *density_ratio*       : radio *relative* para muestreo de densidad.\n",
    "    *theta_step*          : Δθ entre muestras sobre la recta.\n",
    "    *r_threshold*         : tolerancia radial al validar centroides.\n",
    "    *dr_multiplier*       : factor de corte (p.e. 1.3 o 3.1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 bins_theta: int = 100,\n",
    "                 bins_r: int = 80,\n",
    "                 smooth_sigma: float = 1.0,\n",
    "                 density_percentile: float = 80,\n",
    "                 closing_size: int = 2,\n",
    "                 min_cluster_size: int = 40,\n",
    "                 max_gap_dist: float = 0.45,\n",
    "                 theta_bin_size: float = 8,\n",
    "                 density_ratio: float = 0.25,\n",
    "                 theta_step: float = 3,\n",
    "                 r_threshold: float = 0.9,\n",
    "                 dr_multiplier: float = 3.0):\n",
    "        self.bins_theta = bins_theta\n",
    "        self.bins_r = bins_r\n",
    "        self.smooth_sigma = smooth_sigma\n",
    "        self.density_percentile = density_percentile\n",
    "        self.closing_size = closing_size\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.max_gap_dist = max_gap_dist\n",
    "        self.theta_bin_size = theta_bin_size\n",
    "        self.density_ratio = density_ratio\n",
    "        self.theta_step = theta_step\n",
    "        self.r_threshold = r_threshold\n",
    "        self.dr_multiplier = dr_multiplier\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 1 – Histograma + suavizado\n",
    "    # ------------------------------------------------------------------\n",
    "    def _histogram(self, pts: np.ndarray):\n",
    "        hist, te, re = np.histogram2d(\n",
    "            pts[:, 0], pts[:, 1],\n",
    "            bins=[self.bins_theta, self.bins_r]\n",
    "        )\n",
    "        hist_s = gaussian_filter(hist, sigma=self.smooth_sigma)\n",
    "        return hist_s, te, re\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 2 – BFS ponderado para unir islas próximas\n",
    "    # ------------------------------------------------------------------\n",
    "    def _weighted_bfs(self, hist_s, te, re):\n",
    "        thr = np.percentile(hist_s[hist_s > 0], self.density_percentile)\n",
    "        mask = binary_closing(hist_s > thr,\n",
    "                              structure=np.ones((self.closing_size,\n",
    "                                                 self.closing_size)))\n",
    "        labels, n_lab = label(mask)\n",
    "\n",
    "        theta_res, r_res = te[1] - te[0], re[1] - re[0]\n",
    "        neigh = [(-1, 0), (1, 0), (0, -1), (0, 1),\n",
    "                 (-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
    "\n",
    "        parent = list(range(n_lab + 1))\n",
    "        def find(x):\n",
    "            while parent[x] != x:\n",
    "                parent[x] = parent[parent[x]]\n",
    "                x = parent[x]\n",
    "            return x\n",
    "        def union(a, b):\n",
    "            ra, rb = find(a), find(b)\n",
    "            if ra != rb:\n",
    "                parent[rb] = ra\n",
    "\n",
    "        h, w = labels.shape\n",
    "        dist_grid = np.full((h, w), np.inf)\n",
    "        pq: List[Tuple[float, int, int, int]] = []\n",
    "\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                lab = labels[i, j]\n",
    "                if lab:\n",
    "                    for di, dj in neigh:\n",
    "                        ni, nj = i + di, j + dj\n",
    "                        if 0 <= ni < h and 0 <= nj < w and labels[ni, nj] == 0:\n",
    "                            d0 = math.hypot(di * theta_res, dj * r_res)\n",
    "                            if d0 <= self.max_gap_dist and d0 < dist_grid[ni, nj]:\n",
    "                                dist_grid[ni, nj] = d0\n",
    "                                heapq.heappush(pq, (d0, ni, nj, lab))\n",
    "\n",
    "        while pq:\n",
    "            dist, i, j, lab = heapq.heappop(pq)\n",
    "            if dist > dist_grid[i, j] or dist > self.max_gap_dist:\n",
    "                continue\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i + di, j + dj\n",
    "                if 0 <= ni < h and 0 <= nj < w:\n",
    "                    lab2 = labels[ni, nj]\n",
    "                    if lab2 and lab2 != lab:\n",
    "                        union(lab, lab2)\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i + di, j + dj\n",
    "                if 0 <= ni < h and 0 <= nj < w and labels[ni, nj] == 0:\n",
    "                    nd = dist + math.hypot(di * theta_res, dj * r_res)\n",
    "                    if nd <= self.max_gap_dist and nd < dist_grid[ni, nj]:\n",
    "                        dist_grid[ni, nj] = nd\n",
    "                        heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "        return find, labels\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 3 – Clusters y esqueletos\n",
    "    # ------------------------------------------------------------------\n",
    "    def _clusters_and_skeletons(self, pts, find, labels, te, re):\n",
    "        tidx = np.clip(np.digitize(pts[:, 0], te) - 1, 0, self.bins_theta - 1)\n",
    "        ridx = np.clip(np.digitize(pts[:, 1], re) - 1, 0, self.bins_r - 1)\n",
    "        merged: Dict[int, List[np.ndarray]] = {}\n",
    "        for p, (ti, ri) in enumerate(zip(tidx, ridx)):\n",
    "            lab = labels[ti, ri]\n",
    "            if lab:\n",
    "                merged.setdefault(find(lab), []).append(pts[p])\n",
    "        clusters = [np.vstack(v) for v in merged.values()\n",
    "                    if len(v) >= self.min_cluster_size]\n",
    "\n",
    "        skeletons: List[List[Tuple[float, float]]] = []\n",
    "        for cl in clusters:\n",
    "            thetas, rs = cl[:, 0], cl[:, 1]\n",
    "            bins_arr = np.arange(thetas.min(),\n",
    "                                 thetas.max() + self.theta_bin_size,\n",
    "                                 self.theta_bin_size)\n",
    "            ske = []\n",
    "            for k in range(len(bins_arr) - 1):\n",
    "                m = (thetas >= bins_arr[k]) & (thetas < bins_arr[k + 1])\n",
    "                if m.any():\n",
    "                    ske.append((thetas[m].mean(), rs[m].mean()))\n",
    "            skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "        return clusters, skeletons\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 4 – Centroides adicionales sobre *todas* las rectas candidatas\n",
    "    # ------------------------------------------------------------------\n",
    "    def _density_path(self, a, b, hist_s, te, re):\n",
    "        \"\"\"Genera centroides extra a lo largo del segmento a‑b.\"\"\"\n",
    "        theta_res, r_res = te[1] - te[0], re[1] - re[0]\n",
    "        steps = max(int(abs(b[0] - a[0]) / self.theta_step) + 1, 2)\n",
    "        thetas = np.linspace(a[0], b[0], steps)\n",
    "        rs_seg = np.linspace(a[1], b[1], steps)\n",
    "        radius = math.hypot(b[0] - a[0], b[1] - a[1]) * self.density_ratio\n",
    "\n",
    "        out: List[Tuple[float, float]] = []\n",
    "        for θ_i, r_i in zip(thetas, rs_seg):\n",
    "            ti = int(np.clip(np.digitize(θ_i, te) - 1, 0, hist_s.shape[0] - 1))\n",
    "            ri0 = int(np.clip(np.digitize(r_i, re) - 1, 0, hist_s.shape[1] - 1))\n",
    "            max_off = int(math.ceil(radius / r_res))\n",
    "            vals, offs = [], []\n",
    "            for off in range(-max_off, max_off + 1):\n",
    "                idx = ri0 + off\n",
    "                if 0 <= idx < hist_s.shape[1]:\n",
    "                    vals.append(hist_s[ti, idx]); offs.append(off)\n",
    "            for k in range(1, len(vals) - 1):\n",
    "                if vals[k] > vals[k - 1] and vals[k] > vals[k + 1] and vals[k] > 0:\n",
    "                    rc = re[ri0 + offs[k]] + r_res / 2.0\n",
    "                    if abs(rc - r_i) <= self.r_threshold:\n",
    "                        out.append((θ_i, rc))\n",
    "        return out\n",
    "\n",
    "    def _candidate_segments(self, skeletons):\n",
    "        segs = []\n",
    "        # 4a) internos (pares contiguos)\n",
    "        for ske in skeletons:\n",
    "            segs.extend(list(zip(ske, ske[1:])))\n",
    "        # 4b) mejores conexiones inter‑isla (criterio dr>0, dθ<0, distancia mínima)\n",
    "        for i, ske in enumerate(skeletons):\n",
    "            if not ske:\n",
    "                continue\n",
    "            a = ske[-1]\n",
    "            best, bd = None, math.inf\n",
    "            for j, ske2 in enumerate(skeletons):\n",
    "                if i == j or not ske2:\n",
    "                    continue\n",
    "                b = ske2[0]\n",
    "                dr, dtheta = b[1] - a[1], b[0] - a[0]\n",
    "                if dr > 0 and dtheta < 0:\n",
    "                    d = math.hypot(dr, dtheta)\n",
    "                    if d < bd:\n",
    "                        bd, best = d, (a, b)\n",
    "            if best:\n",
    "                segs.append(best)\n",
    "        return segs\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 5 – Construir rutas (contours) y filtrar por Δr\n",
    "    # ------------------------------------------------------------------\n",
    "    def _build_contours(self, segs, hist_s, te, re):\n",
    "        contours: List[List[Tuple[float, float]]] = []\n",
    "        connections: List[Connection] = []\n",
    "        for a, b in segs:\n",
    "            path = self._density_path(a, b, hist_s, te, re)\n",
    "            route = [a] + path + [b]\n",
    "            contours.append(route)\n",
    "            # registrar Δr entre a y b (para estadística global)\n",
    "            connections.append(Connection(a, b, abs(b[1] - a[1]),\n",
    "                                           math.hypot(b[0] - a[0], b[1] - a[1])))\n",
    "        return contours, connections\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_by_threshold(contours, thr):\n",
    "        \"\"\"Parte las rutas cuando Δr > thr.\"\"\"\n",
    "        out = []\n",
    "        for cnt in contours:\n",
    "            tmp = [cnt[0]]\n",
    "            for a, b in zip(cnt, cnt[1:]):\n",
    "                if abs(b[1] - a[1]) <= thr:\n",
    "                    tmp.append(b)\n",
    "                else:\n",
    "                    if len(tmp) > 1:\n",
    "                        out.append(tmp)\n",
    "                    tmp = [b]\n",
    "            if len(tmp) > 1:\n",
    "                out.append(tmp)\n",
    "        return out\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Método público -----------------------------------------------------------------\n",
    "    # ------------------------------------------------------------------\n",
    "    def run(self, datos: Dict[str, Any]):\n",
    "        # Entrada esperada\n",
    "        bg = datos['background'][['theta', 'r']].values\n",
    "        grp = np.vstack([g['points'][['theta', 'r']].values\n",
    "                          for g in datos['grupos_finales']])\n",
    "        pts = np.vstack([bg, grp])\n",
    "\n",
    "        # 1) Histograma\n",
    "        hist_s, te, re = self._histogram(pts)\n",
    "\n",
    "        # 2) BFS + 3) clusters + esqueletos\n",
    "        find, labels = self._weighted_bfs(hist_s, te, re)\n",
    "        clusters, skeletons = self._clusters_and_skeletons(pts, find, labels, te, re)\n",
    "\n",
    "        # 4) segmentos candidatos + rutas con centroides extra\n",
    "        segs = self._candidate_segments(skeletons)\n",
    "        raw_contours, conns = self._build_contours(segs, hist_s, te, re)\n",
    "\n",
    "        # 5) filtrado por Δr\n",
    "        mean_dr = float(np.mean([c.delta_r for c in conns])) if conns else 0.0\n",
    "        thr = mean_dr * self.dr_multiplier\n",
    "        final_contours = self._split_by_threshold(raw_contours, thr)\n",
    "\n",
    "        # Objetos\n",
    "        objects = [IslandObject('isla-conexion-isla', cnt) for cnt in final_contours]\n",
    "        for c in conns:\n",
    "            if c.delta_r > thr:\n",
    "                objects.append(IslandObject('isla-conexiones-adicionales', [c.a, c.b]))\n",
    "\n",
    "        return {\n",
    "            'hist_s': hist_s, 'te': te, 're': re,\n",
    "            'clusters': clusters, 'skeletons': skeletons,\n",
    "            'contours': final_contours, 'connections': conns,\n",
    "            'mean_dr': mean_dr, 'threshold': thr, 'objects': objects\n",
    "        }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Utilidad rápida para visualizar\n",
    "    # ------------------------------------------------------------------\n",
    "    def quick_plot(self, result, figsize=(14, 6)):\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(result['hist_s'].T, origin='lower',\n",
    "                   extent=[result['te'][0], result['te'][-1],\n",
    "                           result['re'][0], result['re'][-1]],\n",
    "                   aspect='auto', cmap='inferno')\n",
    "        plt.colorbar(label='Densidad suavizada')\n",
    "        for idx, cl in enumerate(result['clusters']):\n",
    "            plt.scatter(cl[:, 0], cl[:, 1], s=8, alpha=0.6,\n",
    "                        label=f'Isla {idx}')\n",
    "        for ske in result['skeletons']:\n",
    "            arr = np.array(ske)\n",
    "            if arr.size:\n",
    "                plt.scatter(arr[:, 0], arr[:, 1], c='w', s=40,\n",
    "                            edgecolors='k')\n",
    "        for cnt in result['contours']:\n",
    "            xs, ys = zip(*cnt)\n",
    "            plt.plot(xs, ys, '-o', color='cyan', markersize=5)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  Importa el módulo y los datos de ejemplo\n",
    "# ------------------------------------------------------------\n",
    "#from histogram_segmenter import HistogramSegmenter   #  <-- el archivo del canvas\n",
    "#from your_data_module import main_example_modificado #  <-- tu función de carga\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Carga datos y crea la instancia con los parámetros deseados\n",
    "# ------------------------------------------------------------\n",
    "datos = main_example_modificado()          # debe devolver un dict con background y grupos_finales\n",
    "segmenter = HistogramSegmenter(            # puedes ajustar cualquier parámetro aquí\n",
    "    bins_theta=120,\n",
    "    bins_r=80,\n",
    "    smooth_sigma=1.0,\n",
    "    theta_bin_size=8,\n",
    "    dr_multiplier=3.0                      # 3.0 × μ para partir conexiones\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Ejecuta el algoritmo\n",
    "# ------------------------------------------------------------\n",
    "result = segmenter.run(datos)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  Consulta la salida estructurada\n",
    "# ------------------------------------------------------------\n",
    "print(\"Islas encontradas:\", len(result['clusters']))\n",
    "print(\"Promedio Δr:\", result['mean_dr'])\n",
    "print(\"Umbral usado:\", result['threshold'])\n",
    "print(\"Objetos clasificados:\", len(result['objects']))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5.  Visualiza (opcional)\n",
    "# ------------------------------------------------------------\n",
    "segmenter.quick_plot(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32b1bbc-413b-495e-a6c5-01a14db913e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as PathEffects\n",
    "def plot_islands_and_paths_polar(result, datos, id_halo=\"11\", line_extrap=0.5):\n",
    "    \"\"\"\n",
    "    Función para crear un gráfico polar destacando las islas encontradas, los caminos de los centroides,\n",
    "    los contornos y añadiendo los puntos de fondo con un color gris muy tenue.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtrar islas grandes\n",
    "    big_islands = [cl for cl in result['clusters'] if len(cl) >= 60]\n",
    "    print(f\"Islas ≥60 puntos (polar): {len(big_islands)}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 8))\n",
    "    ax = fig.add_subplot(111, projection='polar')\n",
    "\n",
    "    # Añadir puntos de fondo con gris muy tenue\n",
    "    bg = datos['background'][['theta', 'r']].values\n",
    "    ax.scatter(np.radians(bg[:, 0]), bg[:, 1], s=3, color='gray', edgecolor='black' ,alpha=0.21, label='Puntos de fondo')\n",
    "\n",
    "    # Colores para las islas\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'gray', 'lime']\n",
    "\n",
    "    # Resaltar islas y caminos\n",
    "    for idx, cl in enumerate(big_islands):\n",
    "        c = colors[idx % len(colors)]\n",
    "        theta = np.radians(cl[:, 0])  # Convertir theta de grados a radianes\n",
    "        r = cl[:, 1]\n",
    "        ax.scatter(theta, r, s=10, color=c, alpha=0.34, label=f'Isla {idx+1} ({len(cl)})')\n",
    "\n",
    "        # Resaltar el camino de los centroides (esqueletos) con borde\n",
    "        for ske in result['skeletons']:\n",
    "            arr = np.array(ske)\n",
    "            if arr.size:\n",
    "                theta_ske = np.radians(arr[:, 0])  # Convertir theta a radianes\n",
    "                r_ske = arr[:, 1]\n",
    "                line = ax.plot(theta_ske, r_ske, color='yellow', alpha=0.95, linewidth=2, label='Camino centroides')\n",
    "\n",
    "                # Añadir borde a la línea\n",
    "                for l in line:\n",
    "                    l.set_path_effects([PathEffects.withStroke(linewidth=3, alpha=0.51,foreground='black')])\n",
    "\n",
    "        #Resaltar las conexiones entre los centroides\n",
    "        for cnt in result['contours']:\n",
    "            theta_cnt, r_cnt = zip(*cnt)\n",
    "            ax.plot(np.radians(theta_cnt), r_cnt, 'o', color='cyan',markersize=2.5, alpha=0.585, label='Contorno')\n",
    "            ax.plot(np.radians(theta_cnt), r_cnt, '-o', color='cyan',markersize=1.0, alpha=0.465, label='Contorno')\n",
    "\n",
    "        # Extrapolación de la línea (si es necesario)\n",
    "        if 'slope' in cl and cl['slope'] is not None:  # Usamos 'cl' aquí en lugar de 'g'\n",
    "            dt = cl['theta_max'] - cl['theta_min']\n",
    "            t_deg = np.linspace(cl['theta_min'] - line_extrap * dt, cl['theta_max'] + line_extrap * dt, 200)\n",
    "            r_line = cl['slope'] * t_deg + cl['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "\n",
    "    # Configuración de los ejes polares\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(f\"Halo{id_halo} - Caminos de Centroides\", y=1.05)\n",
    "    ax.grid(True, alpha=0.1, color='gray')\n",
    "    #ax.legend(loc='upper right', bbox_to_anchor=(1.20, 1.0))\n",
    "\n",
    "    # Mostrar gráfico\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Llamada a la nueva celda después de ejecutar el algoritmo\n",
    "# ------------------------------------------------------------\n",
    "plot_islands_and_paths_polar(result, datos, id_halo=\"17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b914fb7f-cef8-45d6-9630-c56f5793fd4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### termina verison definitiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31d74fc6-2198-4e2d-a526-6f4bdebf2e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d27af8c3-e448-4072-aedf-d325cf0ed3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## intento de mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ed363d-4e39-41e4-888d-d105b564a29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación + BFS + conexiones dinámicas + contornos filtrados,\n",
    "almacenamiento de centroides adicionales y conexiones con distancias Euclidiana y radial,\n",
    "resaltando segmentos con Δr > 1 en el gráfico.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq, math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100, bins_r=60, smooth_sigma=1.0,\n",
    "    density_percentile=80, closing_size=2,\n",
    "    min_cluster_size=40, max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos = main_example_modificado()\n",
    "    bg    = datos['background'][['theta','r']].values\n",
    "    grp   = np.vstack([g['points'][['theta','r']].values for g in datos['grupos_finales']])\n",
    "    pts   = np.vstack([bg, grp])\n",
    "\n",
    "    hist, te, re = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s       = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    thr    = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size, closing_size)))\n",
    "    labels, n_lab = label(mask_c)\n",
    "\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    neigh     = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf        = UnionFind(n_lab + 1)\n",
    "    h, w      = labels.shape\n",
    "    dist_grid = np.full((h, w), np.inf)\n",
    "    pq        = []\n",
    "\n",
    "    # BFS ponderado en frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if labels[i,j] > 0:\n",
    "                for di, dj in neigh:\n",
    "                    ni, nj = i + di, j + dj\n",
    "                    if 0 <= ni < h and 0 <= nj < w and labels[ni,nj] == 0:\n",
    "                        d0 = math.hypot(di*theta_res, dj*r_res)\n",
    "                        if d0 <= max_gap_dist and d0 < dist_grid[ni,nj]:\n",
    "                            dist_grid[ni,nj] = d0\n",
    "                            heapq.heappush(pq, (d0, ni, nj, labels[i,j]))\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > dist_grid[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < h and 0 <= nj < w and labels[ni,nj] == 0:\n",
    "                nd = dist + math.hypot(di*theta_res, dj*r_res)\n",
    "                if nd <= max_gap_dist and nd < dist_grid[ni,nj]:\n",
    "                    dist_grid[ni,nj] = nd\n",
    "                    heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Recolectar clusters\n",
    "    merged = defaultdict(list)\n",
    "    tidx = np.clip(np.digitize(pts[:,0], te) - 1, 0, bins_theta - 1)\n",
    "    ridx = np.clip(np.digitize(pts[:,1], re) - 1, 0, bins_r - 1)\n",
    "    for p, (ti, ri) in enumerate(zip(tidx, ridx)):\n",
    "        lab = labels[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[p])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    # Skeletons\n",
    "    skeletons = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins_arr = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins_arr) - 1):\n",
    "            mask = (thetas >= bins_arr[k]) & (thetas < bins_arr[k+1])\n",
    "            if mask.any():\n",
    "                ske.append((thetas[mask].mean(), rs[mask].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, te, re\n",
    "\n",
    "\n",
    "def compute_density_path(p, q, hist_s, te, re, radius,\n",
    "                         theta_step=2, r_threshold=0.9):\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    total_dθ  = q[0] - p[0]\n",
    "    steps     = max(int(abs(total_dθ) / theta_step) + 1, 2)\n",
    "    thetas    = np.linspace(p[0], q[0], steps)\n",
    "    rs_vals   = np.linspace(p[1], q[1], steps)\n",
    "    path = []\n",
    "    for θ_i, r_i in zip(thetas, rs_vals):\n",
    "        ti  = int(np.clip(np.digitize(θ_i, te) - 1, 0, hist_s.shape[0] - 1))\n",
    "        ri0 = int(np.clip(np.digitize(r_i, re) - 1, 0, hist_s.shape[1] - 1))\n",
    "        max_off = int(np.ceil(radius / r_res))\n",
    "        vals, offs = [], []\n",
    "        for off in range(-max_off, max_off + 1):\n",
    "            idx_r = ri0 + off\n",
    "            if 0 <= idx_r < hist_s.shape[1]:\n",
    "                vals.append(hist_s[ti, idx_r])\n",
    "                offs.append(off)\n",
    "        for k in range(1, len(vals)-1):\n",
    "            if vals[k] > vals[k-1] and vals[k] > vals[k+1] and vals[k] > 0:\n",
    "                off = offs[k]\n",
    "                θc = te[ti] + theta_res/2\n",
    "                rc = (re[ri0+off] + r_res/2) if 0 <= ri0+off < len(re) else r_i\n",
    "                if abs(rc - r_i) <= r_threshold:\n",
    "                    path.append((θc, rc))\n",
    "    return path\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, te, re,\n",
    "                      density_ratio=0.25, theta_step=2, r_threshold=0.9):\n",
    "    internal = []\n",
    "    contours = []\n",
    "    merged_conns = []\n",
    "\n",
    "    for ske in skeletons:\n",
    "        internal += list(zip(ske, ske[1:]))\n",
    "\n",
    "    for i, ske in enumerate(skeletons):\n",
    "        if not ske:\n",
    "            continue\n",
    "        p = ske[-1]\n",
    "        best_pair = None\n",
    "        best_dist = math.inf\n",
    "        for j, ske2 in enumerate(skeletons):\n",
    "            if i == j or not ske2:\n",
    "                continue\n",
    "            q = ske2[0]\n",
    "            drd = q[1] - p[1]\n",
    "            dtd = q[0] - p[0]\n",
    "            if drd > 0 and dtd < 0:\n",
    "                d = math.hypot(dtd, drd)\n",
    "                if d < best_dist:\n",
    "                    best_dist = d\n",
    "                    best_pair = (p, q)\n",
    "        if best_pair:\n",
    "            p0, q0 = best_pair\n",
    "            radius = best_dist * density_ratio\n",
    "            path = compute_density_path(p0, q0, hist_s, te, re, radius,\n",
    "                                        theta_step, r_threshold)\n",
    "            contour = [p0] + path + [q0]\n",
    "            contours.append(contour)\n",
    "\n",
    "    additional_centroids = []\n",
    "    connections = []\n",
    "\n",
    "    for contour in contours:\n",
    "        additional_centroids.extend(contour[1:-1])\n",
    "        for a, b in zip(contour, contour[1:]):\n",
    "            euclidean = math.hypot(b[0]-a[0], b[1]-a[1])\n",
    "            delta_r = abs(b[1]-a[1])\n",
    "            connections.append({'points':(a,b), 'delta_r':delta_r, 'euclidean':euclidean})\n",
    "    for a, b in internal:\n",
    "        euclidean = math.hypot(b[0]-a[0], b[1]-a[1])\n",
    "        delta_r = abs(b[1]-a[1])\n",
    "        connections.append({'points':(a,b), 'delta_r':delta_r, 'euclidean':euclidean})\n",
    "\n",
    "    additional_centroids = list(dict.fromkeys(additional_centroids))\n",
    "    return internal, contours, additional_centroids, connections\n",
    "\n",
    "\n",
    "def plot_clusters_and_connections(\n",
    "    clusters, skeletons, hist_s, te, re,\n",
    "    internal, contours, connections\n",
    "):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[te[0],te[-1],re[0],re[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters): plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size: plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k')\n",
    "    # conexiones normales\n",
    "    for a, b in internal:\n",
    "        plt.plot([a[0],b[0]],[a[1],b[1]], '-', linewidth=2)\n",
    "    for cnt in contours:\n",
    "        xs, ys = zip(*cnt)\n",
    "        plt.plot(xs, ys, '-o', markersize=4, linewidth=2, alpha=0.8)\n",
    "    # resaltar segmentos con delta_r > 1\n",
    "    for conn in connections:\n",
    "        a, b = conn['points']\n",
    "        if conn['delta_r'] > 1:\n",
    "            # línea blanca de fondo gruesa\n",
    "            plt.plot([a[0],b[0]],[a[1],b[1]], '-', color='white', linewidth=6, zorder=5)\n",
    "            # línea roja encima\n",
    "            plt.plot([a[0],b[0]],[a[1],b[1]], '-', color='red', linewidth=4, zorder=6)\n",
    "    plt.title('Islas, conexiones y contornos filtrados')\n",
    "    plt.xlabel('θ (°)'); plt.ylabel('r'); plt.legend(loc='upper right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    clusters, skeletons, hist_s, te, re = main_histogram_weighted_bfs_clusters()\n",
    "    internal, contours, additional_pts, connections = connect_skeletons(skeletons, hist_s, te, re)\n",
    "    info = {'additional_centroids': additional_pts, 'connections': connections}\n",
    "    print(f\"Centroides adicionales: {len(additional_pts)}\")\n",
    "    print(f\"Conexiones totales: {len(connections)}\")\n",
    "    plot_clusters_and_connections(clusters, skeletons, hist_s, te, re, internal, contours, connections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a62d77-22ee-4b2c-a138-8b7bcb2a5f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Segmentación + BFS + conexiones dinámicas + contornos filtrados,\n",
    "almacenamiento de centroides adicionales y conexiones con distancias Euclidiana y radial,\n",
    "resaltando segmentos con Δr > 1 y considerando aislados los centroides adicionales\n",
    "con Δr > 1.3 * promedio.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "import heapq, math\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra != rb:\n",
    "            self.parent[rb] = ra\n",
    "\n",
    "\n",
    "def main_histogram_weighted_bfs_clusters(\n",
    "    bins_theta=100, bins_r=60, smooth_sigma=1.0,\n",
    "    density_percentile=80, closing_size=2,\n",
    "    min_cluster_size=40, max_gap_dist=0.5,\n",
    "    theta_bin_size=10\n",
    "):\n",
    "    datos = main_example_modificado()\n",
    "    bg    = datos['background'][['theta','r']].values\n",
    "    grp   = np.vstack([g['points'][['theta','r']].values for g in datos['grupos_finales']])\n",
    "    pts   = np.vstack([bg, grp])\n",
    "\n",
    "    hist, te, re = np.histogram2d(pts[:,0], pts[:,1], bins=[bins_theta, bins_r])\n",
    "    hist_s       = gaussian_filter(hist, sigma=smooth_sigma)\n",
    "\n",
    "    thr    = np.percentile(hist_s[hist_s>0], density_percentile)\n",
    "    mask_c = binary_closing(hist_s>thr, structure=np.ones((closing_size, closing_size)))\n",
    "    labels, n_lab = label(mask_c)\n",
    "\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    neigh     = [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    uf        = UnionFind(n_lab + 1)\n",
    "    h, w      = labels.shape\n",
    "    dist_grid = np.full((h, w), np.inf)\n",
    "    pq        = []\n",
    "\n",
    "    # BFS ponderado en frontera\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if labels[i,j] > 0:\n",
    "                for di, dj in neigh:\n",
    "                    ni, nj = i + di, j + dj\n",
    "                    if 0 <= ni < h and 0 <= nj < w and labels[ni,nj] == 0:\n",
    "                        d0 = math.hypot(di*theta_res, dj*r_res)\n",
    "                        if d0 <= max_gap_dist and d0 < dist_grid[ni,nj]:\n",
    "                            dist_grid[ni,nj] = d0\n",
    "                            heapq.heappush(pq, (d0, ni, nj, labels[i,j]))\n",
    "    while pq:\n",
    "        dist, i, j, lab = heapq.heappop(pq)\n",
    "        if dist > dist_grid[i,j] or dist > max_gap_dist:\n",
    "            continue\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < h and 0 <= nj < w:\n",
    "                lab2 = labels[ni,nj]\n",
    "                if lab2 > 0 and lab2 != lab:\n",
    "                    uf.union(lab, lab2)\n",
    "        for di, dj in neigh:\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < h and 0 <= nj < w and labels[ni,nj] == 0:\n",
    "                nd = dist + math.hypot(di*theta_res, dj*r_res)\n",
    "                if nd <= max_gap_dist and nd < dist_grid[ni,nj]:\n",
    "                    dist_grid[ni,nj] = nd\n",
    "                    heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "\n",
    "    # Recolectar clusters\n",
    "    merged = defaultdict(list)\n",
    "    tidx = np.clip(np.digitize(pts[:,0], te) - 1, 0, bins_theta - 1)\n",
    "    ridx = np.clip(np.digitize(pts[:,1], re) - 1, 0, bins_r - 1)\n",
    "    for p, (ti, ri) in enumerate(zip(tidx, ridx)):\n",
    "        lab = labels[ti, ri]\n",
    "        if lab > 0:\n",
    "            merged[uf.find(lab)].append(pts[p])\n",
    "    clusters = [np.vstack(v) for v in merged.values() if len(v) >= min_cluster_size]\n",
    "\n",
    "    # Skeletons\n",
    "    skeletons = []\n",
    "    for c in clusters:\n",
    "        thetas, rs = c[:,0], c[:,1]\n",
    "        bins_arr = np.arange(thetas.min(), thetas.max() + theta_bin_size, theta_bin_size)\n",
    "        ske = []\n",
    "        for k in range(len(bins_arr) - 1):\n",
    "            mask = (thetas >= bins_arr[k]) & (thetas < bins_arr[k+1])\n",
    "            if mask.any():\n",
    "                ske.append((thetas[mask].mean(), rs[mask].mean()))\n",
    "        skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "\n",
    "    return clusters, skeletons, hist_s, te, re\n",
    "\n",
    "\n",
    "def compute_density_path(p, q, hist_s, te, re, radius,\n",
    "                         theta_step=2, r_threshold=0.9):\n",
    "    theta_res = te[1] - te[0]\n",
    "    r_res     = re[1] - re[0]\n",
    "    total_dθ  = q[0] - p[0]\n",
    "    steps     = max(int(abs(total_dθ) / theta_step) + 1, 2)\n",
    "    thetas    = np.linspace(p[0], q[0], steps)\n",
    "    rs_vals   = np.linspace(p[1], q[1], steps)\n",
    "    path = []\n",
    "    for θ_i, r_i in zip(thetas, rs_vals):\n",
    "        ti  = int(np.clip(np.digitize(θ_i, te) - 1, 0, hist_s.shape[0] - 1))\n",
    "        ri0 = int(np.clip(np.digitize(r_i, re) - 1, 0, hist_s.shape[1] - 1))\n",
    "        max_off = int(np.ceil(radius / r_res))\n",
    "        vals, offs = [], []\n",
    "        for off in range(-max_off, max_off + 1):\n",
    "            idx_r = ri0 + off\n",
    "            if 0 <= idx_r < hist_s.shape[1]:\n",
    "                vals.append(hist_s[ti, idx_r])\n",
    "                offs.append(off)\n",
    "        for k in range(1, len(vals)-1):\n",
    "            if vals[k] > vals[k-1] and vals[k] > vals[k+1] and vals[k] > 0:\n",
    "                off = offs[k]\n",
    "                θc = te[ti] + theta_res/2\n",
    "                rc = (re[ri0+off] + r_res/2) if 0 <= ri0+off < len(re) else r_i\n",
    "                if abs(rc - r_i) <= r_threshold:\n",
    "                    path.append((θc, rc))\n",
    "    return path\n",
    "\n",
    "\n",
    "def connect_skeletons(skeletons, hist_s, te, re,\n",
    "                      density_ratio=0.25, theta_step=2, r_threshold=0.9):\n",
    "    internal = []\n",
    "    contours = []\n",
    "    connections = []\n",
    "\n",
    "    # conexiones internas\n",
    "    for ske in skeletons:\n",
    "        internal += list(zip(ske, ske[1:]))\n",
    "    # contornos\n",
    "    for i, ske in enumerate(skeletons):\n",
    "        if not ske: continue\n",
    "        p = ske[-1]; best_pair, best_dist = None, math.inf\n",
    "        for j, ske2 in enumerate(skeletons):\n",
    "            if i==j or not ske2: continue\n",
    "            q = ske2[0]\n",
    "            drd, dtd = q[1]-p[1], q[0]-p[0]\n",
    "            if drd>0 and dtd<0:\n",
    "                d = math.hypot(dtd, drd)\n",
    "                if d<best_dist: best_dist, best_pair = d, (p,q)\n",
    "        if best_pair:\n",
    "            p0,q0 = best_pair\n",
    "            radius = best_dist * density_ratio\n",
    "            path = compute_density_path(p0,q0,hist_s,te,re,radius,theta_step,r_threshold)\n",
    "            contour = [p0] + path + [q0]\n",
    "            contours.append(contour)\n",
    "    # extraer conexiones con distancias\n",
    "    for a,b in internal + [pt for contour in contours for pt in zip(contour, contour[1:])]:\n",
    "        euclid = math.hypot(b[0]-a[0], b[1]-a[1])\n",
    "        delta_r = abs(b[1]-a[1])\n",
    "        connections.append({'points':(a,b), 'delta_r':delta_r, 'euclidean':euclid})\n",
    "\n",
    "    return internal, contours, connections\n",
    "\n",
    "\n",
    "def plot_and_analyze(\n",
    "    clusters, skeletons, hist_s, te, re,\n",
    "    internal, contours, connections\n",
    "):\n",
    "    # Estadísticos para conexiones de centroides adicionales\n",
    "    # Filtrar solo conexiones con centroides generados (contours)\n",
    "    ad_conns = [c for c in connections if c['points'][0] in {pt for cnt in contours for pt in cnt[1:-1]} or\n",
    "                                           c['points'][1] in {pt for cnt in contours for pt in cnt[1:-1]}]\n",
    "    dr_vals = [c['delta_r'] for c in ad_conns]\n",
    "    mean_dr = np.mean(dr_vals)\n",
    "    median_dr = np.median(dr_vals)\n",
    "    threshold = 3.1 * mean_dr\n",
    "    isolated = {pt for c in ad_conns if c['delta_r']>threshold for pt in c['points']}\n",
    "\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.imshow(hist_s.T, origin='lower', extent=[te[0],te[-1],re[0],re[-1]], aspect='auto', cmap='inferno')\n",
    "    plt.colorbar(label='Densidad suavizada')\n",
    "    for idx, c in enumerate(clusters): plt.scatter(c[:,0], c[:,1], s=8, alpha=0.6, label=f'Isla {idx}')\n",
    "    for ske in skeletons:\n",
    "        arr = np.array(ske)\n",
    "        if arr.size: plt.scatter(arr[:,0], arr[:,1], c='w', s=40, edgecolors='k')\n",
    "    # conexiones normales y contornos\n",
    "    for a,b in internal:\n",
    "        plt.plot([a[0],b[0]],[a[1],b[1]], '-', linewidth=2)\n",
    "    for cnt in contours:\n",
    "        xs, ys = zip(*cnt)\n",
    "        plt.plot(xs, ys, '-o', markersize=4, linewidth=2, alpha=0.8)\n",
    "    # resaltar Δr >1 y aislados\n",
    "    for c in connections:\n",
    "        a,b = c['points']\n",
    "        if c['delta_r']>1:\n",
    "            plt.plot([a[0],b[0]],[a[1],b[1]], '-', color='white', linewidth=6, zorder=5)\n",
    "            plt.plot([a[0],b[0]],[a[1],b[1]], '-', color='red', linewidth=4, zorder=6)\n",
    "    # marcar aislados\n",
    "    for pt in isolated:\n",
    "        plt.scatter(pt[0], pt[1], marker='X', s=100, edgecolors='yellow', linewidth=2, facecolors='none', zorder=7)\n",
    "    plt.title(f'Umbral Δr>1.3∗μ={threshold:.2f}; mediana={median_dr:.2f}')\n",
    "    plt.xlabel('θ (°)'); plt.ylabel('r'); plt.legend(loc='upper right'); plt.tight_layout(); plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    clusters, skeletons, hist_s, te, re = main_histogram_weighted_bfs_clusters()\n",
    "    internal, contours, connections = connect_skeletons(skeletons, hist_s, te, re)\n",
    "    print(f'Promedio Δr adicionales: {np.mean([c[\"delta_r\"] for c in connections]):.3f}')\n",
    "    print(f'Mediana Δr adicionales: {np.median([c[\"delta_r\"] for c in connections]):.3f}')\n",
    "    plot_and_analyze(clusters, skeletons, hist_s, te, re, internal, contours, connections)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e969af7-1111-46d2-bfe6-667fa2d6be1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b30c0ea3-7ec3-4ff4-9143-749d0356f90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## halo 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df856a9f-ef5f-4dfe-ac8a-d591c0fdf0da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ====================================\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ====================================\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "# ====================================\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ====================================\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors = np.where(mask_neighbors)[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "# ====================================\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ====================================\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "# ====================================\n",
    "# 4) Generar Semillas BFS\n",
    "# ====================================\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "# ====================================\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ====================================\n",
    "def calculate_pa(slope):\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return None, None, None, None, None, None, None\n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    slope_ = model.coef_[0]\n",
    "    intercept_ = model.intercept_\n",
    "    pa_ = calculate_pa(slope_)\n",
    "    t_min = df_cluster['theta'].min()\n",
    "    t_max = df_cluster['theta'].max()\n",
    "    r_min = df_cluster['r'].min()\n",
    "    r_max = df_cluster['r'].max()\n",
    "    return slope_, intercept_, pa_, t_min, t_max, r_min, r_max\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.40,\n",
    "    bounding_extrap=0.30\n",
    "):\n",
    "    groups = []\n",
    "    for clust_df in bfs_clusters:\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(clust_df)\n",
    "        grp = {\n",
    "            'points': clust_df.copy(),\n",
    "            'slope': slope_,\n",
    "            'intercept': intercept_,\n",
    "            'pa': pa_,\n",
    "            'theta_min': tmin,\n",
    "            'theta_max': tmax,\n",
    "            'r_min': rmin,\n",
    "            'r_max': rmax\n",
    "        }\n",
    "        groups.append(grp)\n",
    "    \n",
    "    def recalc_properties(group):\n",
    "        dfp = group['points']\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(dfp)\n",
    "        group['slope'] = slope_\n",
    "        group['intercept'] = intercept_\n",
    "        group['pa'] = pa_\n",
    "        group['theta_min'] = tmin\n",
    "        group['theta_max'] = tmax\n",
    "        group['r_min'] = rmin\n",
    "        group['r_max'] = rmax\n",
    "\n",
    "    def boxes_overlap(g1, g2, extrap=0.30):\n",
    "        tmin1, tmax1 = g1['theta_min'], g1['theta_max']\n",
    "        rmin1, rmax1 = g1['r_min'], g1['r_max']\n",
    "        tmin2, tmax2 = g2['theta_min'], g2['theta_max']\n",
    "        rmin2, rmax2 = g2['r_min'], g2['r_max']\n",
    "        if tmin1 is None or tmin2 is None:\n",
    "            return False\n",
    "        dt1 = (tmax1 - tmin1)\n",
    "        dr1 = (rmax1 - rmin1)\n",
    "        dt2 = (tmax2 - tmin2)\n",
    "        dr2 = (rmax2 - rmin2)\n",
    "        extr_dt1 = dt1*extrap\n",
    "        extr_dr1 = dr1*extrap\n",
    "        extr_dt2 = dt2*extrap\n",
    "        extr_dr2 = dr2*extrap\n",
    "        \n",
    "        tmin1e = tmin1 - extr_dt1\n",
    "        tmax1e = tmax1 + extr_dt1\n",
    "        rmin1e = rmin1 - extr_dr1\n",
    "        rmax1e = rmax1 + extr_dr1\n",
    "        \n",
    "        tmin2e = tmin2 - extr_dt2\n",
    "        tmax2e = tmax2 + extr_dt2\n",
    "        rmin2e = rmin2 - extr_dr2\n",
    "        rmax2e = rmax2 + extr_dr2\n",
    "        \n",
    "        overlap_theta = not (tmax1e < tmin2e or tmax2e < tmin1e)\n",
    "        overlap_r = not (rmax1e < rmin2e or rmax2e < rmin1e)\n",
    "        return overlap_theta and overlap_r\n",
    "    \n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        new_groups = []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1 = groups[i]\n",
    "            j = i+1\n",
    "            fused_any = False\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    combined = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    slope_, intercept_, _, _, _, _, _ = fit_line_to_cluster(combined)\n",
    "                    if slope_ is not None:\n",
    "                        var1 = abs(slope_ - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        var2 = abs(slope_ - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if var1 < slope_variation_threshold and var2 < slope_variation_threshold:\n",
    "                            g1['points'] = combined\n",
    "                            recalc_properties(g1)\n",
    "                            groups.pop(j)\n",
    "                            fused_any = True\n",
    "                            merged_flag = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new_groups.append(g1)\n",
    "            i += 1\n",
    "        groups = new_groups\n",
    "    \n",
    "    final_groups = []\n",
    "    for gg in groups:\n",
    "        if len(gg['points']) >= 2:\n",
    "            final_groups.append(gg)\n",
    "    return final_groups\n",
    "\n",
    "# ====================================\n",
    "# 6) Validar dispersión y re-procesar (opcional)\n",
    "# ====================================\n",
    "def validate_dispersion_and_reprocess(\n",
    "    groups,\n",
    "    dispersion_threshold=2.0,   # umbral de desviación std (ejemplo)\n",
    "    reproc_theta_diff=2.0,      # BFS para reprocesar\n",
    "    reproc_r_diff=0.5\n",
    "):\n",
    "    groups_ok = []\n",
    "    groups_toreprocess = []\n",
    "    \n",
    "    for grp in groups:\n",
    "        slope = grp['slope']\n",
    "        intercept = grp['intercept']\n",
    "        pts = grp['points']\n",
    "        if slope is None or len(pts) < 2:\n",
    "            groups_ok.append(grp)\n",
    "            continue\n",
    "        \n",
    "        # Calcular std de residuo\n",
    "        predicted_r = slope*pts['theta'] + intercept\n",
    "        residuals = pts['r'] - predicted_r\n",
    "        std_resid = residuals.std()\n",
    "        if std_resid > dispersion_threshold:\n",
    "            groups_toreprocess.append(grp)\n",
    "        else:\n",
    "            groups_ok.append(grp)\n",
    "    \n",
    "    # Reprocesar los grupos con alta dispersión\n",
    "    new_groups = []\n",
    "    for grp in groups_toreprocess:\n",
    "        subdf = grp['points'].copy()\n",
    "        subdf.reset_index(drop=True, inplace=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        clusters_sub = bfs_components(graph, subdf)\n",
    "        \n",
    "        for csub in clusters_sub:\n",
    "            slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(csub)\n",
    "            new_groups.append({\n",
    "                'points': csub,\n",
    "                'slope': slope_,\n",
    "                'intercept': intercept_,\n",
    "                'pa': pa_,\n",
    "                'theta_min': tmin,\n",
    "                'theta_max': tmax,\n",
    "                'r_min': rmin,\n",
    "                'r_max': rmax\n",
    "            })\n",
    "    \n",
    "    final_res = groups_ok + new_groups\n",
    "    return final_res\n",
    "\n",
    "# =========================================\n",
    "# 7) Nueva fase: Elegir grupos con >= 60 puntos y graficar con bounding box extrapolado\n",
    "# =========================================\n",
    "def plot_final_segments_over_60(groups, df_all, bounding_extrap=0.30):\n",
    "    \"\"\"\n",
    "    - Toma los grupos finales\n",
    "    - Filtra los que tienen >= 60 puntos\n",
    "    - Grafica en un nuevo plot:\n",
    "        * Los puntos de cada grupo\n",
    "        * La recta\n",
    "        * Un rectángulo que muestre el bounding box extrapolado un 30%\n",
    "    \"\"\"\n",
    "    # 1. Filtrar\n",
    "    large_groups = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos con >= 60 puntos: {len(large_groups)}\")\n",
    "    \n",
    "    # 2. Crear figura\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Fondo con df_all en gris\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "    \n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    \n",
    "    for i, grp in enumerate(large_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = grp['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"Grupo_{i+1} ({len(pts)} pts)\")\n",
    "        \n",
    "        slope_ = grp['slope']\n",
    "        intercept_ = grp['intercept']\n",
    "        if slope_ is not None:\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 100)\n",
    "            r_lin = slope_*t_lin + intercept_\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "        \n",
    "        # 3. Dibujar bounding box extrapolado 30%\n",
    "        # bounding box actual\n",
    "        tmin_bb = grp['theta_min']\n",
    "        tmax_bb = grp['theta_max']\n",
    "        rmin_bb = grp['r_min']\n",
    "        rmax_bb = grp['r_max']\n",
    "        \n",
    "        # Calcular ancho/alto\n",
    "        dt = (tmax_bb - tmin_bb)\n",
    "        dr = (rmax_bb - rmin_bb)\n",
    "        dt_ext = dt * bounding_extrap\n",
    "        dr_ext = dr * bounding_extrap\n",
    "        \n",
    "        # bounding box expandido\n",
    "        tmin_e = tmin_bb - dt_ext\n",
    "        tmax_e = tmax_bb + dt_ext\n",
    "        rmin_e = rmin_bb - dr_ext\n",
    "        rmax_e = rmax_bb + dr_ext\n",
    "        \n",
    "        # Dibujar rectángulo con plt.plot (4 lados)\n",
    "        # forma => [(x1, y1), (x1, y2), (x2, y2), (x2, y1), (x1, y1)]\n",
    "        # donde x=theta, y=r\n",
    "        box_x = [tmin_e, tmin_e, tmax_e, tmax_e, tmin_e]\n",
    "        box_y = [rmin_e, rmax_e, rmax_e, rmin_e, rmin_e]\n",
    "        plt.plot(box_x, box_y, color=c, linewidth=1.2)\n",
    "    \n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.title(\"Grupos con >= 60 puntos y su bounding box extrapolado\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =========================================\n",
    "# 8) Función principal: ejemplo\n",
    "# =========================================\n",
    "def main_example():\n",
    "    # Paso 1: generar BFS\n",
    "    bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "        id_halo=\"11\",\n",
    "        theta_min=20,\n",
    "        theta_max=350,\n",
    "        quartile_threshold=0.155,\n",
    "        theta_diff=3.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=1.80,\n",
    "        gap_threshold_r=1.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs_clusters)} grupos\")\n",
    "    \n",
    "    # Paso 2: ajustar/fusionar\n",
    "    merged_groups = adjust_and_merge_seeds(\n",
    "        bfs_clusters,\n",
    "        slope_variation_threshold=0.30,\n",
    "        bounding_extrap=0.35\n",
    "    )\n",
    "    print(f\"Grupos tras fusión inicial: {len(merged_groups)}\")\n",
    "    \n",
    "    # Paso 3: validación de dispersión (opcional)\n",
    "    validated_groups = validate_dispersion_and_reprocess(\n",
    "        merged_groups,\n",
    "        dispersion_threshold=0.80,\n",
    "        reproc_theta_diff=1.025,\n",
    "        reproc_r_diff=0.34\n",
    "    )\n",
    "    print(f\"Grupos tras validación dispersión: {len(validated_groups)}\")\n",
    "    \n",
    "    # Plot de esos grupos en general\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(df_filtrado['theta'], df_filtrado['r'], s=3, alpha=0.3, label='Datos BFS')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, group in enumerate(validated_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = group['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)}pts)\")\n",
    "        if group['slope'] is not None:\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 100)\n",
    "            r_lin = group['slope']*t_lin + group['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "    plt.xlabel(\"θ (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    #plt.legend(loc='best')\n",
    "    plt.title(\"Grupos finales (validados)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Paso 4: ahora filtrar los grupos con >=60 puntos y graficarlos aparte\n",
    "    plot_final_segments_over_60(validated_groups, df_filtrado, bounding_extrap=0.10)\n",
    "\n",
    "\n",
    "main_example()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73e71be-d78d-481e-bc95-21e630cd2f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ====================================\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ====================================\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df['theta'] = np.where(df['theta'] < 0, df['theta'] + 360, df['theta'])\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r'] = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "# ====================================\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ====================================\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    theta_arr = df_points['theta'].values\n",
    "    r_arr = df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dtheta = np.abs(theta_arr[i] - theta_arr)\n",
    "        dr = np.abs(r_arr[i] - r_arr)\n",
    "        mask_neighbors = (dtheta <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        neighbors = np.where(mask_neighbors)[0]\n",
    "        graph[i] = list(neighbors)\n",
    "    return graph, n\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n = len(graph)\n",
    "    visited = [False]*n\n",
    "    clusters = []\n",
    "    from collections import deque\n",
    "    for start in range(n):\n",
    "        if not visited[start]:\n",
    "            queue = deque([start])\n",
    "            visited[start] = True\n",
    "            comp_indices = [start]\n",
    "            while queue:\n",
    "                u = queue.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        queue.append(v)\n",
    "                        comp_indices.append(v)\n",
    "            cluster_df = df_points.iloc[comp_indices].copy()\n",
    "            clusters.append(cluster_df)\n",
    "    return clusters\n",
    "\n",
    "# ====================================\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ====================================\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    order_col = 'theta' if mode == 'theta' else 'r'\n",
    "    df_cluster = df_cluster.sort_values(by=order_col)\n",
    "    arr = df_cluster[order_col].values\n",
    "    diffs = np.diff(arr)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df_cluster]\n",
    "    subclusters = []\n",
    "    start = 0\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if diff > gap_threshold:\n",
    "            subclusters.append(df_cluster.iloc[start:i+1].copy())\n",
    "            start = i+1\n",
    "    subclusters.append(df_cluster.iloc[start:].copy())\n",
    "    return subclusters\n",
    "\n",
    "# ====================================\n",
    "# 4) Generar Semillas BFS\n",
    "# ====================================\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.155,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    threshold = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_filtered = df[df['rho_resta_final_exp'] > threshold].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    graph, _ = build_graph_rectangular(df_filtered, theta_diff, r_diff)\n",
    "    clusters = bfs_components(graph, df_filtered)\n",
    "    \n",
    "    final_clusters = []\n",
    "    for clust in clusters:\n",
    "        sub_theta = subdivide_by_gap(clust, gap_threshold=gap_threshold_theta, mode='theta')\n",
    "        for st in sub_theta:\n",
    "            sub_r = subdivide_by_gap(st, gap_threshold=gap_threshold_r, mode='r')\n",
    "            final_clusters.extend(sub_r)\n",
    "    \n",
    "    return final_clusters, df_filtered\n",
    "\n",
    "# ====================================\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ====================================\n",
    "def calculate_pa(slope):\n",
    "    if slope is None:\n",
    "        return None\n",
    "    return np.degrees(np.arctan(slope))\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return None, None, None, None, None, None, None\n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    slope_ = model.coef_[0]\n",
    "    intercept_ = model.intercept_\n",
    "    pa_ = calculate_pa(slope_)\n",
    "    t_min = df_cluster['theta'].min()\n",
    "    t_max = df_cluster['theta'].max()\n",
    "    r_min = df_cluster['r'].min()\n",
    "    r_max = df_cluster['r'].max()\n",
    "    return slope_, intercept_, pa_, t_min, t_max, r_min, r_max\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.20,\n",
    "    bounding_extrap=0.20\n",
    "):\n",
    "    groups = []\n",
    "    for clust_df in bfs_clusters:\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(clust_df)\n",
    "        grp = {\n",
    "            'points': clust_df.copy(),\n",
    "            'slope': slope_,\n",
    "            'intercept': intercept_,\n",
    "            'pa': pa_,\n",
    "            'theta_min': tmin,\n",
    "            'theta_max': tmax,\n",
    "            'r_min': rmin,\n",
    "            'r_max': rmax\n",
    "        }\n",
    "        groups.append(grp)\n",
    "    \n",
    "    def recalc_properties(group):\n",
    "        dfp = group['points']\n",
    "        slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(dfp)\n",
    "        group['slope'] = slope_\n",
    "        group['intercept'] = intercept_\n",
    "        group['pa'] = pa_\n",
    "        group['theta_min'] = tmin\n",
    "        group['theta_max'] = tmax\n",
    "        group['r_min'] = rmin\n",
    "        group['r_max'] = rmax\n",
    "\n",
    "    def boxes_overlap(g1, g2, extrap=0.30):\n",
    "        tmin1, tmax1 = g1['theta_min'], g1['theta_max']\n",
    "        rmin1, rmax1 = g1['r_min'], g1['r_max']\n",
    "        tmin2, tmax2 = g2['theta_min'], g2['theta_max']\n",
    "        rmin2, rmax2 = g2['r_min'], g2['r_max']\n",
    "        if tmin1 is None or tmin2 is None:\n",
    "            return False\n",
    "        dt1 = (tmax1 - tmin1)\n",
    "        dr1 = (rmax1 - rmin1)\n",
    "        dt2 = (tmax2 - tmin2)\n",
    "        dr2 = (rmax2 - rmin2)\n",
    "        extr_dt1 = dt1*extrap\n",
    "        extr_dr1 = dr1*extrap\n",
    "        extr_dt2 = dt2*extrap\n",
    "        extr_dr2 = dr2*extrap\n",
    "        \n",
    "        tmin1e = tmin1 - extr_dt1\n",
    "        tmax1e = tmax1 + extr_dt1\n",
    "        rmin1e = rmin1 - extr_dr1\n",
    "        rmax1e = rmax1 + extr_dr1\n",
    "        \n",
    "        tmin2e = tmin2 - extr_dt2\n",
    "        tmax2e = tmax2 + extr_dt2\n",
    "        rmin2e = rmin2 - extr_dr2\n",
    "        rmax2e = rmax2 + extr_dr2\n",
    "        \n",
    "        overlap_theta = not (tmax1e < tmin2e or tmax2e < tmin1e)\n",
    "        overlap_r = not (rmax1e < rmin2e or rmax2e < rmin1e)\n",
    "        return overlap_theta and overlap_r\n",
    "    \n",
    "    merged_flag = True\n",
    "    while merged_flag:\n",
    "        merged_flag = False\n",
    "        new_groups = []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1 = groups[i]\n",
    "            j = i+1\n",
    "            fused_any = False\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    combined = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    slope_, intercept_, _, _, _, _, _ = fit_line_to_cluster(combined)\n",
    "                    if slope_ is not None:\n",
    "                        var1 = abs(slope_ - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        var2 = abs(slope_ - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if var1 < slope_variation_threshold and var2 < slope_variation_threshold:\n",
    "                            g1['points'] = combined\n",
    "                            recalc_properties(g1)\n",
    "                            groups.pop(j)\n",
    "                            fused_any = True\n",
    "                            merged_flag = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new_groups.append(g1)\n",
    "            i += 1\n",
    "        groups = new_groups\n",
    "    \n",
    "    final_groups = []\n",
    "    for gg in groups:\n",
    "        if len(gg['points']) >= 2:\n",
    "            final_groups.append(gg)\n",
    "    return final_groups\n",
    "\n",
    "# ====================================\n",
    "# 6) [Nuevo] Validar dispersión y re-procesar\n",
    "# ====================================\n",
    "def validate_dispersion_and_reprocess(\n",
    "    groups,\n",
    "    dispersion_threshold=2.0,   # umbral de desviación std (ejemplo)\n",
    "    reproc_theta_diff=2.0,      # BFS para reprocesar\n",
    "    reproc_r_diff=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    - Verifica la 'dispersión' (std de los residuos de cada grupo).\n",
    "    - Si un grupo excede 'dispersion_threshold', se 'rompe':\n",
    "       * Tomamos sus puntos\n",
    "       * Reaplicamos BFS (o un mini-proceso) a ese subconjunto\n",
    "       * Ajustamos rectas otra vez\n",
    "    - Devuelve la lista final de grupos (los que no se rompieron + \n",
    "      los subgrupos reagrupados).\n",
    "    \"\"\"\n",
    "    # 1) Separar grupos \"buenos\" de grupos \"a re-procesar\"\n",
    "    groups_ok = []\n",
    "    groups_toreprocess = []\n",
    "    \n",
    "    for grp in groups:\n",
    "        slope = grp['slope']\n",
    "        intercept = grp['intercept']\n",
    "        pts = grp['points']\n",
    "        if slope is None or len(pts) < 2:\n",
    "            # Grupo muy pequeño -> considerarlo \"ok\" sin dispersión\n",
    "            groups_ok.append(grp)\n",
    "            continue\n",
    "        \n",
    "        # Calcular residuos\n",
    "        predicted_r = slope*pts['theta'] + intercept\n",
    "        residuals = pts['r'] - predicted_r\n",
    "        std_resid = residuals.std()  # desviación estándar\n",
    "        # Checar si std_resid > threshold\n",
    "        if std_resid > dispersion_threshold:\n",
    "            groups_toreprocess.append(grp)\n",
    "        else:\n",
    "            groups_ok.append(grp)\n",
    "    \n",
    "    # 2) Re-procesar los que superaron la dispersión\n",
    "    #    Por ejemplo, usando BFS de nuevo\n",
    "    new_groups = []\n",
    "    for grp in groups_toreprocess:\n",
    "        # Tomar sus puntos:\n",
    "        subdf = grp['points'].copy()\n",
    "        subdf.reset_index(drop=True, inplace=True)\n",
    "        # BFS con reproc_theta_diff, reproc_r_diff\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        clusters_sub = bfs_components(graph, subdf)\n",
    "        \n",
    "        # A) Ajustamos recta a cada mini-cluster (sin fusión en este ejemplo)\n",
    "        #    O podrías re-llamar a adjust_and_merge_seeds con slope_variation_threshold,\n",
    "        #    si quieres un pipeline más largo.\n",
    "        \n",
    "        for csub in clusters_sub:\n",
    "            slope_, intercept_, pa_, tmin, tmax, rmin, rmax = fit_line_to_cluster(csub)\n",
    "            new_groups.append({\n",
    "                'points': csub,\n",
    "                'slope': slope_,\n",
    "                'intercept': intercept_,\n",
    "                'pa': pa_,\n",
    "                'theta_min': tmin,\n",
    "                'theta_max': tmax,\n",
    "                'r_min': rmin,\n",
    "                'r_max': rmax\n",
    "            })\n",
    "    \n",
    "    # 3) Unir \"groups_ok\" + \"new_groups\" en la lista final\n",
    "    final_res = groups_ok + new_groups\n",
    "    \n",
    "    return final_res\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# 7) Función principal que muestra TODO\n",
    "# ====================================\n",
    "def main_example():\n",
    "    # 1) Generar BFS\n",
    "    bfs_clusters, df_filtrado = generate_bfs_seeds(\n",
    "        id_halo=\"11\",\n",
    "        theta_min=20,\n",
    "        theta_max=350,\n",
    "        quartile_threshold=0.55,\n",
    "        theta_diff=3.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=1.80,\n",
    "        gap_threshold_r=1.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs_clusters)} grupos\")\n",
    "    \n",
    "    # 2) Ajustar y fusionar\n",
    "    merged_groups = adjust_and_merge_seeds(\n",
    "        bfs_clusters,\n",
    "        slope_variation_threshold=0.40,\n",
    "        bounding_extrap=0.30\n",
    "    )\n",
    "    print(f\"Grupos tras fusión inicial: {len(merged_groups)}\")\n",
    "    \n",
    "    # 3) Validar dispersión y re-procesar grupos con alta dispersión\n",
    "    #    Por ejemplo, dispersion_threshold=2.0\n",
    "    validated_groups = validate_dispersion_and_reprocess(\n",
    "        merged_groups,\n",
    "        dispersion_threshold=0.6920, # si la std del residuo >2 -> re-proceso BFS\n",
    "        reproc_theta_diff=1.05,\n",
    "        reproc_r_diff=0.4\n",
    "    )\n",
    "    print(f\"Grupos tras validación dispersión: {len(validated_groups)}\")\n",
    "    \n",
    "    # 4) Graficar\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_filtrado['theta'], df_filtrado['r'], s=3, alpha=0.3, label='Datos BFS')\n",
    "    \n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, group in enumerate(validated_groups):\n",
    "        c = colors[i % len(colors)]\n",
    "        pts = group['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1}_{len(pts)}\")\n",
    "        if group['slope'] is not None:\n",
    "            tmin = pts['theta'].min()\n",
    "            tmax = pts['theta'].max()\n",
    "            t_lin = np.linspace(tmin, tmax, 50)\n",
    "            r_lin = group['slope']*t_lin + group['intercept']\n",
    "            plt.plot(t_lin, r_lin, '--', color=c)\n",
    "    plt.xlabel(\"theta (grados)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"BFS + Fusión + Validación de Dispersión (reprocesado)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Ejecución:\n",
    "# main_example()\n",
    "# ====================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf1e152-8046-4b0b-b897-1adc45b2cc10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af8afa08-9074-4302-a583-75f3d9e09496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Algoritmo mejorado (versión subhalo 11) con validación adaptativa\n",
    "para datasets > 2000 puntos y visualizaciones completas.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 1) Cargar y filtrar datos\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r'] = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r']     = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 2) Grafo rectangular (BFS)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    th = df_points['theta'].values\n",
    "    rr = df_points['r'].values\n",
    "    n  = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        dth = np.abs(th[i] - th)\n",
    "        dr  = np.abs(rr[i] - rr)\n",
    "        mask = (dth <= theta_diff) & (dr <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    n, visited, clusters = len(graph), [False]*len(graph), []\n",
    "    for s in range(n):\n",
    "        if not visited[s]:\n",
    "            q = deque([s]); visited[s] = True; comp = [s]\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 3) Subdividir por “gaps”\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    col   = 'theta' if mode == 'theta' else 'r'\n",
    "    df    = df_cluster.sort_values(col)\n",
    "    diffs = np.diff(df[col].values)\n",
    "    if np.all(diffs <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(diffs):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 4) Generar semillas BFS  (validación adaptativa)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50,\n",
    "    theta_max=250,\n",
    "    quartile_threshold=0.155,\n",
    "    theta_diff=3.0,\n",
    "    r_diff=0.5,\n",
    "    gap_threshold_theta=2.0,\n",
    "    gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    # ─── ajuste adaptativo ──────────────────────────────────────────────\n",
    "    fct                 = _adaptive_factor(len(df_f))\n",
    "    theta_diff_eff      = theta_diff / fct\n",
    "    r_diff_eff          = r_diff     / fct\n",
    "    gap_theta_eff       = gap_threshold_theta / fct\n",
    "    gap_r_eff           = gap_threshold_r     / fct\n",
    "\n",
    "    graph, _  = build_graph_rectangular(df_f, theta_diff_eff, r_diff_eff)\n",
    "    clusters  = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl  = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_theta_eff, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r_eff, 'r'))\n",
    "\n",
    "    return final_cl, df_f\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 5) Ajustar rectas y fusionar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def calculate_pa(slope):\n",
    "    return None if slope is None else np.degrees(np.arctan(slope))\n",
    "\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    X = df_cluster[['theta']].values\n",
    "    y = df_cluster['r'].values\n",
    "    mdl = LinearRegression().fit(X, y)\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    tmin, tmax = df_cluster['theta'].min(), df_cluster['theta'].max()\n",
    "    rmin, rmax = df_cluster['r'].min(),    df_cluster['r'].max()\n",
    "    return s, b, calculate_pa(s), tmin, tmax, rmin, rmax\n",
    "\n",
    "\n",
    "def adjust_and_merge_seeds(\n",
    "    bfs_clusters,\n",
    "    slope_variation_threshold=0.20,\n",
    "    bounding_extrap=0.20\n",
    "):\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        s, b, pa, tmin, tmax, rmin, rmax = fit_line_to_cluster(cl)\n",
    "        groups.append({'points': cl, 'slope': s, 'intercept': b, 'pa': pa,\n",
    "                       'theta_min': tmin, 'theta_max': tmax, 'r_min': rmin, 'r_max': rmax})\n",
    "\n",
    "    def recalc(g):\n",
    "        s, b, pa, tmin, tmax, rmin, rmax = fit_line_to_cluster(g['points'])\n",
    "        g.update({'slope': s, 'intercept': b, 'pa': pa,\n",
    "                  'theta_min': tmin, 'theta_max': tmax,\n",
    "                  'r_min': rmin, 'r_max': rmax})\n",
    "\n",
    "    def boxes_overlap(g1, g2, extr):\n",
    "        def exp(tmin, tmax, rmin, rmax):\n",
    "            dt, dr = (tmax-tmin)*extr, (rmax-rmin)*extr\n",
    "            return tmin-dt, tmax+dt, rmin-dr, rmax+dr\n",
    "        t1l, t1u, r1l, r1u = exp(g1['theta_min'], g1['theta_max'], g1['r_min'], g1['r_max'])\n",
    "        t2l, t2u, r2l, r2u = exp(g2['theta_min'], g2['theta_max'], g2['r_min'], g2['r_max'])\n",
    "        return not (t1u < t2l or t2u < t1l) and not (r1u < r2l or r2u < r1l)\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1; continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb; recalc(g1)\n",
    "                            groups.pop(j); merged = True; continue\n",
    "                j += 1\n",
    "            new.append(g1); i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 6) Validar dispersión y re‑procesar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def validate_dispersion_and_reprocess(\n",
    "    groups,\n",
    "    dispersion_threshold=0.6920,\n",
    "    reproc_theta_diff=1.05,\n",
    "    reproc_r_diff=0.4\n",
    "):\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g); continue\n",
    "        res = g['points']['r'] - (g['slope']*g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "\n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            s, b, pa, tmin, tmax, rmin, rmax = fit_line_to_cluster(c)\n",
    "            new.append({'points': c, 'slope': s, 'intercept': b, 'pa': pa,\n",
    "                        'theta_min': tmin, 'theta_max': tmax, 'r_min': rmin, 'r_max': rmax})\n",
    "    return ok + new\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7) Plot de grupos grandes con recta extrapolada\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_final_segments_over_60_with_line_extrapolation(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i%len(colors)], g['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)})\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t1 = g['theta_min'] - line_extrap*dt\n",
    "            t2 = g['theta_max'] + line_extrap*dt\n",
    "            t = np.linspace(t1, t2, 200)\n",
    "            plt.plot(t, g['slope']*t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\"); plt.ylabel(\"r\")\n",
    "    plt.title(\"Grupos ≥60 puntos – extrapolación 15 %\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 8) Ejecución de ejemplo completo (subhalo 11)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def main_example():\n",
    "    # A) BFS + subdivisión adaptativa\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"11\",\n",
    "        theta_min=20,\n",
    "        theta_max=350,\n",
    "        quartile_threshold=0.55,\n",
    "        theta_diff=3.0,\n",
    "        r_diff=0.5,\n",
    "        gap_threshold_theta=1.80,\n",
    "        gap_threshold_r=1.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "\n",
    "    # B) Fusionar\n",
    "    merged = adjust_and_merge_seeds(\n",
    "        bfs,\n",
    "        slope_variation_threshold=0.40,   # llamada con 0.40\n",
    "        bounding_extrap=0.30\n",
    "    )\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "\n",
    "    # C) Validar dispersión\n",
    "    validated = validate_dispersion_and_reprocess(\n",
    "        merged,\n",
    "        dispersion_threshold=0.6920,\n",
    "        reproc_theta_diff=1.05,\n",
    "        reproc_r_diff=0.4\n",
    "    )\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "\n",
    "    # D) Gráfico global\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_f['theta'], df_f['r'], s=3, alpha=0.3, label='Datos filtrados')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray']\n",
    "    for i, g in enumerate(validated):\n",
    "        c, pts = colors[i%len(colors)], g['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)})\")\n",
    "        if g['slope'] is not None:\n",
    "            t = np.linspace(pts['theta'].min(), pts['theta'].max(), 80)\n",
    "            plt.plot(t, g['slope']*t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\"); plt.ylabel(\"r\")\n",
    "    plt.title(\"Subhalo 11 – grupos finales\"); plt.legend(loc='upper right')\n",
    "    plt.grid(True); plt.show()\n",
    "\n",
    "    # E) Grupos grandes con extrapolación\n",
    "    plot_final_segments_over_60_with_line_extrapolation(validated, df_f, line_extrap=0.15)\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 9) Llamada a main\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "if __name__ == \"__main__\":\n",
    "    main_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88cba3bb-df19-4b4b-81e6-74741466d497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575f92dd-772c-4d41-97f8-16e77c213231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "###############################################################################\n",
    "#                           🌌  SUBHALO 11 – VERSIÓN FINAL  🌌\n",
    "#  ░█▀▀░█░█░█▀▀░█▀▀░█░█░█▀█░█░█   INCLUYE:\n",
    "#  ░█░░░░█░░█▀▀░█░░░█▀▄░█▀█░█▀█   1) VALIDACIÓN ADAPTATIVA > 2000 PUNTOS\n",
    "#  ░▀▀▀░░▀░░▀▀▀░▀▀▀░▀░▀░▀░▀░▀░▀   2) PLOTS CARTESIANO Y **¡¡NUEVO!!** POLAR\n",
    "###############################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 1) Cargar y filtrar datos\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r']     = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r']     = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 2) Grafo rectangular (BFS)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 3) Subdividir por “gaps”\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    col = 'theta' if mode == 'theta' else 'r'\n",
    "    df  = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 4) Generar semillas BFS  (💥 VALIDACIÓN ADAPTATIVA 💥)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50, theta_max=250,\n",
    "    quartile_threshold=0.155,\n",
    "    theta_diff=3.0, r_diff=0.5,\n",
    "    gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    f = _adaptive_factor(len(df_f))                    # ←—— FACTOR ADAPTATIVO\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "\n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 5) Ajustar rectas y fusionar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def calculate_pa(slope):\n",
    "    return None if slope is None else np.degrees(np.arctan(slope))\n",
    "\n",
    "\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    return s, b, calculate_pa(s), df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.20, bounding_extrap=0.20):\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "\n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0,t1,r0,r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'],g1['theta_max'],g1['r_min'],g1['r_max'])\n",
    "        b = exp(g2['theta_min'],g2['theta_max'],g2['r_min'],g2['r_max'])\n",
    "        return not (a[1]<b[0] or b[1]<a[0]) and not (a[3]<b[2] or b[3]<a[2])\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1; continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb-g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb-g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb; recalc(g1)\n",
    "                            groups.pop(j); merged = True; continue\n",
    "                j += 1\n",
    "            new.append(g1); i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 6) Validar dispersión y re‑procesar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=0.6920, reproc_theta_diff=1.05, reproc_r_diff=0.4):\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g); continue\n",
    "        res = g['points']['r'] - (g['slope']*g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "\n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph,_ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑A) Plot cartesiano grupos grandes\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_final_segments_over_60(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i%len(colors)], g['points']\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)})\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            plt.plot(t, g['slope']*t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\"); plt.ylabel(\"r\"); plt.grid(True)\n",
    "    plt.title(\"Grupos ≥60 puntos – extrapolación 15 %\")\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑B) ⭐ NUEVO ⭐  PLOT POLAR grupos grandes\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_final_segments_over_60_polar(groups, df_all, line_extrap=0.15):\n",
    "    \"\"\"\n",
    "    🔭 Representa los grupos (≥60 pts) en coordenadas polares.\n",
    "    \"\"\"\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax  = plt.subplot(111, projection='polar')\n",
    "\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i%len(colors)], g['points']\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c, label=f\"G{i+1} ({len(pts)})\")\n",
    "\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            r_line = g['slope']*t_deg + g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Grupos ≥60 puntos – proyección polar (15 % extrap.)\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.15,1.0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 8) MAIN – pipeline completo subhalo 11\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def main_example():\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"11\",\n",
    "        theta_min=20, theta_max=350,\n",
    "        quartile_threshold=0.55,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=1.80, gap_threshold_r=1.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.40, bounding_extrap=0.30)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "\n",
    "    validated = validate_dispersion_and_reprocess(merged, dispersion_threshold=0.6920,\n",
    "                                                  reproc_theta_diff=1.05, reproc_r_diff=0.4)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "\n",
    "    # ─── Visualizaciones ────────────────────────────────────────────────\n",
    "    plot_final_segments_over_60(validated, df_f, line_extrap=0.15)   # cartesiano\n",
    "    plot_final_segments_over_60_polar(validated, df_f, line_extrap=0.15)  # ⭐ NUEVO ⭐\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 9) Run\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "if __name__ == \"__main__\":\n",
    "    main_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0d7c43-0da8-4141-9b77-10736b364374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 0)  PITCH‑ANGLE  (versión solicitada)  🔥\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    Pitch‑angle en grados según la fórmula proporcionada por el usuario:\n",
    "        PA = arctan[(slope·180/π) / intercept]   (si intercept ≠ 0)\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 1) Cargar y filtrar datos\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    df['r']     = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r']     = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 2) Grafo rectangular (BFS)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 3) Subdividir por “gaps”\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    col = 'theta' if mode == 'theta' else 'r'\n",
    "    df  = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 4) Generar semillas BFS  (💥 VALIDACIÓN ADAPTATIVA 💥)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50, theta_max=250,\n",
    "    quartile_threshold=0.155,\n",
    "    theta_diff=3.0, r_diff=0.5,\n",
    "    gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "\n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 5) Ajustar rectas y fusionar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa   = calculate_pa(s, b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.20, bounding_extrap=0.20):\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "\n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0,t1,r0,r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'],g1['theta_max'],g1['r_min'],g1['r_max'])\n",
    "        b = exp(g2['theta_min'],g2['theta_max'],g2['r_min'],g2['r_max'])\n",
    "        return not (a[1]<b[0] or b[1]<a[0]) and not (a[3]<b[2] or b[3]<a[2])\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1; continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb-g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb-g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb; recalc(g1)\n",
    "                            groups.pop(j); merged = True; continue\n",
    "                j += 1\n",
    "            new.append(g1); i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 6) Validar dispersión y re‑procesar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=0.6920, reproc_theta_diff=1.05, reproc_r_diff=0.4):\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g); continue\n",
    "        res = g['points']['r'] - (g['slope']*g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "\n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph,_ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑A) Plot cartesiano grupos grandes (PA visible)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_final_segments_over_60(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i%len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            plt.plot(t, g['slope']*t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\"); plt.ylabel(\"r\"); plt.grid(True)\n",
    "    plt.title(\"Grupos ≥60 puntos – extrapolación 15 % (con PA)\")\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑B) Plot POLAR grupos grandes (PA visible)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_final_segments_over_60_polar(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax  = plt.subplot(111, projection='polar')\n",
    "\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i%len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c,\n",
    "                   label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            r_line = g['slope']*t_deg + g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Grupos ≥60 puntos – proyección polar (15 % extrap.)\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20,1.0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 8) MAIN – pipeline completo subhalo 11\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def main_example():\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"11\",\n",
    "        theta_min=20, theta_max=350,\n",
    "        quartile_threshold=0.55,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=1.80, gap_threshold_r=1.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.40, bounding_extrap=0.30)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "\n",
    "    validated = validate_dispersion_and_reprocess(merged, dispersion_threshold=0.6920,\n",
    "                                                  reproc_theta_diff=1.05, reproc_r_diff=0.4)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "\n",
    "    # ─── Visualizaciones ────────────────────────────────────────────────\n",
    "    plot_final_segments_over_60(validated, df_f, line_extrap=0.15)        # cartesiano\n",
    "    plot_final_segments_over_60_polar(validated, df_f, line_extrap=0.15)  # polar\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 9) Run\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "if __name__ == \"__main__\":\n",
    "    main_example()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "612b493e-8883-45d6-8aa4-0af4c5ef841c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### halo 11 version camino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c098ba57-8754-4399-86d9-ea9ef7cc4a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 0)  PITCH‑ANGLE \n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def calculate_pa(slope, intercept):\n",
    "    \"\"\"\n",
    "    PA = arctan[(slope·180/π) / intercept]  en grados, si intercept ≠ 0.\n",
    "    Devuelve NaN si intercept == 0.\n",
    "    \"\"\"\n",
    "    if intercept != 0:\n",
    "        return np.degrees(np.arctan((slope * (180 / np.pi)) / intercept))\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 1) Funciones para Cargar/Filtrar Datos\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def load_and_filter_data(id_halo, theta_min, theta_max, file_prefix='data_rho'):\n",
    "    df = pd.read_csv(f\"{file_prefix}_{id_halo}_filtered.csv\")\n",
    "    # Agregar columna de id para preservar el índice original\n",
    "    df['id'] = df.index\n",
    "    df['r']     = np.sqrt(df['x']**2 + df['y']**2)\n",
    "    df['theta'] = np.degrees(np.arctan2(df['y'], df['x']))\n",
    "    df.loc[df['theta'] < 0, 'theta'] += 360\n",
    "    df = df[(df['theta'] >= theta_min) & (df['theta'] <= theta_max)].copy()\n",
    "    df['theta'] = df['theta'].astype(float)\n",
    "    df['r']     = df['r'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 2) Construir Grafo (Rectangular BFS)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def build_graph_rectangular(df_points, theta_diff, r_diff):\n",
    "    th, rr = df_points['theta'].values, df_points['r'].values\n",
    "    n = len(df_points)\n",
    "    graph = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        mask = (np.abs(th[i] - th) <= theta_diff) & (np.abs(rr[i] - rr) <= r_diff) & (np.arange(n) != i)\n",
    "        graph[i] = list(np.where(mask)[0])\n",
    "    return graph, n\n",
    "\n",
    "\n",
    "def bfs_components(graph, df_points):\n",
    "    visited, clusters = [False]*len(graph), []\n",
    "    for s in range(len(graph)):\n",
    "        if not visited[s]:\n",
    "            q, comp = deque([s]), [s]\n",
    "            visited[s] = True\n",
    "            while q:\n",
    "                u = q.popleft()\n",
    "                for v in graph[u]:\n",
    "                    if not visited[v]:\n",
    "                        visited[v] = True\n",
    "                        q.append(v)\n",
    "                        comp.append(v)\n",
    "            clusters.append(df_points.iloc[comp].copy())\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 3) Subdividir por \"gaps\"\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def subdivide_by_gap(df_cluster, gap_threshold=2.5, mode='theta'):\n",
    "    col = 'theta' if mode == 'theta' else 'r'\n",
    "    df  = df_cluster.sort_values(col)\n",
    "    dif = np.diff(df[col].values)\n",
    "    if np.all(dif <= gap_threshold):\n",
    "        return [df]\n",
    "    subs, start = [], 0\n",
    "    for i, d in enumerate(dif):\n",
    "        if d > gap_threshold:\n",
    "            subs.append(df.iloc[start:i+1].copy())\n",
    "            start = i + 1\n",
    "    subs.append(df.iloc[start:].copy())\n",
    "    return subs\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 4) Generar Semillas BFS  (validación adaptativa)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def _adaptive_factor(n_pts, ref=2000):\n",
    "    return np.sqrt(max(n_pts / ref, 1.0))\n",
    "\n",
    "\n",
    "def generate_bfs_seeds(\n",
    "    id_halo,\n",
    "    theta_min=50, theta_max=250,\n",
    "    quartile_threshold=0.55,\n",
    "    theta_diff=3.0, r_diff=0.5,\n",
    "    gap_threshold_theta=2.0, gap_threshold_r=2.0,\n",
    "    file_prefix='data_rho'\n",
    "):\n",
    "    df = load_and_filter_data(id_halo, theta_min, theta_max, file_prefix)\n",
    "    thr = df['rho_resta_final_exp'].quantile(quartile_threshold)\n",
    "    df_f = df[df['rho_resta_final_exp'] > thr].copy().reset_index(drop=True)\n",
    "\n",
    "    f = _adaptive_factor(len(df_f))\n",
    "    th_diff, r_d = theta_diff/f, r_diff/f\n",
    "    gap_th, gap_r = gap_threshold_theta/f, gap_threshold_r/f\n",
    "\n",
    "    graph, _ = build_graph_rectangular(df_f, th_diff, r_d)\n",
    "    clusters = bfs_components(graph, df_f)\n",
    "\n",
    "    final_cl = []\n",
    "    for cl in clusters:\n",
    "        for st in subdivide_by_gap(cl, gap_th, 'theta'):\n",
    "            final_cl.extend(subdivide_by_gap(st, gap_r, 'r'))\n",
    "    return final_cl, df_f\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 5) Ajustar rectas y Fusionar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def fit_line_to_cluster(df_cluster):\n",
    "    if len(df_cluster) < 2:\n",
    "        return (None,)*7\n",
    "    mdl = LinearRegression().fit(df_cluster[['theta']], df_cluster['r'])\n",
    "    s, b = mdl.coef_[0], mdl.intercept_\n",
    "    pa   = calculate_pa(s, b)\n",
    "    return s, b, pa, df_cluster['theta'].min(), df_cluster['theta'].max(), df_cluster['r'].min(), df_cluster['r'].max()\n",
    "\n",
    "\n",
    "def adjust_and_merge_seeds(bfs_clusters, slope_variation_threshold=0.40, bounding_extrap=0.30):\n",
    "    groups = []\n",
    "    for cl in bfs_clusters:\n",
    "        groups.append(dict(zip(\n",
    "            ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "            (*fit_line_to_cluster(cl), cl)\n",
    "        )))\n",
    "\n",
    "    def recalc(g):\n",
    "        g['slope'], g['intercept'], g['pa'], g['theta_min'], g['theta_max'], g['r_min'], g['r_max'] = \\\n",
    "            fit_line_to_cluster(g['points'])\n",
    "\n",
    "    def boxes_overlap(g1, g2, e):\n",
    "        def exp(t0,t1,r0,r1):\n",
    "            dt, dr = (t1-t0)*e, (r1-r0)*e\n",
    "            return t0-dt, t1+dt, r0-dr, r1+dr\n",
    "        a = exp(g1['theta_min'],g1['theta_max'],g1['r_min'],g1['r_max'])\n",
    "        b = exp(g2['theta_min'],g2['theta_max'],g2['r_min'],g2['r_max'])\n",
    "        return not (a[1]<b[0] or b[1]<a[0]) and not (a[3]<b[2] or b[3]<a[2])\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged, new = False, []\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            g1, j = groups[i], i+1\n",
    "            while j < len(groups):\n",
    "                g2 = groups[j]\n",
    "                if g1['slope'] is None or g2['slope'] is None:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                if boxes_overlap(g1, g2, bounding_extrap):\n",
    "                    comb = pd.concat([g1['points'], g2['points']], ignore_index=True)\n",
    "                    s_comb, b_comb, *_ = fit_line_to_cluster(comb)\n",
    "                    if s_comb is not None:\n",
    "                        v1 = abs(s_comb - g1['slope'])/(abs(g1['slope'])+1e-12)\n",
    "                        v2 = abs(s_comb - g2['slope'])/(abs(g2['slope'])+1e-12)\n",
    "                        if v1 < slope_variation_threshold and v2 < slope_variation_threshold:\n",
    "                            g1['points'] = comb\n",
    "                            recalc(g1)\n",
    "                            groups.pop(j)\n",
    "                            merged = True\n",
    "                            continue\n",
    "                j += 1\n",
    "            new.append(g1)\n",
    "            i += 1\n",
    "        groups = new\n",
    "    return [g for g in groups if len(g['points']) >= 2]\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 6) Validar dispersión y re‑procesar\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def validate_dispersion_and_reprocess(groups, dispersion_threshold=1.80, reproc_theta_diff=1.25, reproc_r_diff=0.4):\n",
    "    ok, todo = [], []\n",
    "    for g in groups:\n",
    "        if g['slope'] is None or len(g['points']) < 2:\n",
    "            ok.append(g)\n",
    "            continue\n",
    "        res = g['points']['r'] - (g['slope'] * g['points']['theta'] + g['intercept'])\n",
    "        (todo if res.std() > dispersion_threshold else ok).append(g)\n",
    "\n",
    "    new = []\n",
    "    for g in todo:\n",
    "        subdf = g['points'].reset_index(drop=True)\n",
    "        graph, _ = build_graph_rectangular(subdf, reproc_theta_diff, reproc_r_diff)\n",
    "        for c in bfs_components(graph, subdf):\n",
    "            new.append(dict(zip(\n",
    "                ['slope','intercept','pa','theta_min','theta_max','r_min','r_max','points'],\n",
    "                (*fit_line_to_cluster(c), c)\n",
    "            )))\n",
    "    return ok + new\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑A) Plot cartesiano grupos grandes (PA visible)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_groups_cartesian(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos: {len(big)}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_all['theta'], df_all['r'], s=3, alpha=0.3, color='gray')\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        plt.scatter(pts['theta'], pts['r'], s=10, color=c,\n",
    "                    label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t = np.linspace(g['theta_min']-line_extrap*dt, g['theta_max']+line_extrap*dt, 200)\n",
    "            plt.plot(t, g['slope'] * t + g['intercept'], '--', color=c)\n",
    "    plt.xlabel(\"θ (°)\")\n",
    "    plt.ylabel(\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Subhalo 17 – grupos ≥60 pts (cartesiano, PA visible)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 7‑B) Plot POLAR grupos grandes (PA visible)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def plot_groups_polar(groups, df_all, line_extrap=0.15):\n",
    "    big = [g for g in groups if len(g['points']) >= 60]\n",
    "    print(f\"Grupos ≥60 puntos (polar): {len(big)}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(9,8))\n",
    "    ax  = plt.subplot(111, projection='polar')\n",
    "    ax.scatter(np.radians(df_all['theta']), df_all['r'], s=3, alpha=0.3, color='gray', label='Datos base')\n",
    "\n",
    "    colors = ['red','blue','green','purple','orange','brown','magenta','cyan','gold','gray','lime']\n",
    "    for i, g in enumerate(big):\n",
    "        c, pts = colors[i % len(colors)], g['points']\n",
    "        pa_txt = f\"{g['pa']:.2f}°\" if g['pa'] is not None else \"—\"\n",
    "        ax.scatter(np.radians(pts['theta']), pts['r'], s=10, color=c,\n",
    "                   label=f\"G{i+1} ({len(pts)}) | PA={pa_txt}\")\n",
    "\n",
    "        if g['slope'] is not None:\n",
    "            dt = g['theta_max'] - g['theta_min']\n",
    "            t_deg = np.linspace(g['theta_min'] - line_extrap * dt, g['theta_max'] + line_extrap * dt, 200)\n",
    "            r_line = g['slope'] * t_deg + g['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(\"Subhalo 17 – proyección polar (PA visible)\", y=1.05)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.20, 1.0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 8) MAIN – pipeline completo subhalo 17 (modificado para almacenar grupos y background)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def main_example_modificado():\n",
    "    # Genera las semillas BFS y carga los datos filtrados\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"11\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.425,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=1.80, gap_threshold_r=1.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "\n",
    "    # Visualizaciones (si se desea mantener)\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "\n",
    "    # Almacenar grupos finales: solo aquellos con 60 o más puntos\n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "\n",
    "    # Identificar los IDs de los puntos que pertenecen a los grupos finales.\n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        # Se espera que la columna \"id\" esté en los datos originales\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "\n",
    "    # Los puntos \"background\" son aquellos de df_f que no pertenecen a ningún grupo final.\n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "\n",
    "    # Se puede devolver un diccionario con los dos conjuntos de información:\n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 9) Run\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "if __name__ == \"__main__\":\n",
    "    datos_finales = main_example_modificado()\n",
    "    # Ahora, datos_finales['grupos_finales'] contiene los grupos con ≥ 60 pts, \n",
    "    # y datos_finales['background'] contiene la información original de los puntos\n",
    "    # que no fueron asignados a ningún grupo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d1e4c4-b29a-40bd-9c43-247b98670afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Histogram Segmenter\n",
    "===================\n",
    "Módulo autocontenido y reutilizable para:\n",
    "1. Detección de \"islas\" en un histograma 2D (θ, r).\n",
    "2. Cálculo de esqueletos por bins angulares (θ).\n",
    "3. Generación **siempre** de centroides adicionales sobre *todas* las rectas\n",
    "   candidatas (pares de esqueletos contiguos e inter‑isla), muestreando la\n",
    "   densidad de fondo.\n",
    "4. Segmentación de contornos: las conexiones se parten si Δr > *dr_multiplier*·μ,\n",
    "   donde μ es el promedio de Δr de los segmentos generados.\n",
    "5. Clasificación de objetos resultantes:\n",
    "   • ``isla‑conexion‑isla``  (contornos válidos)\n",
    "   • ``isla‑conexiones‑adicionales``  (segmentos descartados > umbral)\n",
    "6. Salida estructurada + visualización opcional.\n",
    "\n",
    "El diseño busca **estabilidad** ante pequeños cambios en parámetros para poder\n",
    "aplicarlo a múltiples datasets sin retocar el código.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import heapq\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter, binary_closing, label\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dataclasses de apoyo\n",
    "# ---------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class Connection:\n",
    "    a: Tuple[float, float]\n",
    "    b: Tuple[float, float]\n",
    "    delta_r: float\n",
    "    euclidean: float\n",
    "\n",
    "@dataclass\n",
    "class IslandObject:\n",
    "    type: str  # \"isla-conexion-isla\" | \"isla-conexiones-adicionales\"\n",
    "    boundary: List[Tuple[float, float]]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Algoritmo principal\n",
    "# ---------------------------------------------------------------------------\n",
    "class HistogramSegmenter:\n",
    "    \"\"\"Segmentador configurable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *bins_theta*, *bins_r* : resolución del histograma (θ, r).\n",
    "    *smooth_sigma*        : σ del filtro gaussiano.\n",
    "    *density_percentile*  : umbral de densidad para binarización.\n",
    "    *closing_size*        : tamaño del cierre morfológico.\n",
    "    *min_cluster_size*    : puntos mínimos por isla.\n",
    "    *max_gap_dist*        : distancia máx. para unir islas en BFS.\n",
    "    *theta_bin_size*      : paso angular para esqueletos.\n",
    "    *density_ratio*       : radio *relative* para muestreo de densidad.\n",
    "    *theta_step*          : Δθ entre muestras sobre la recta.\n",
    "    *r_threshold*         : tolerancia radial al validar centroides.\n",
    "    *dr_multiplier*       : factor de corte (p.e. 1.3 o 3.1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 bins_theta: int = 100,\n",
    "                 bins_r: int = 80,\n",
    "                 smooth_sigma: float = 1.0,\n",
    "                 density_percentile: float = 80,\n",
    "                 closing_size: int = 2,\n",
    "                 min_cluster_size: int = 40,\n",
    "                 max_gap_dist: float = 0.45,\n",
    "                 theta_bin_size: float = 8,\n",
    "                 density_ratio: float = 0.25,\n",
    "                 theta_step: float = 3,\n",
    "                 r_threshold: float = 0.9,\n",
    "                 dr_multiplier: float = 3.0):\n",
    "        self.bins_theta = bins_theta\n",
    "        self.bins_r = bins_r\n",
    "        self.smooth_sigma = smooth_sigma\n",
    "        self.density_percentile = density_percentile\n",
    "        self.closing_size = closing_size\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.max_gap_dist = max_gap_dist\n",
    "        self.theta_bin_size = theta_bin_size\n",
    "        self.density_ratio = density_ratio\n",
    "        self.theta_step = theta_step\n",
    "        self.r_threshold = r_threshold\n",
    "        self.dr_multiplier = dr_multiplier\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 1 – Histograma + suavizado\n",
    "    # ------------------------------------------------------------------\n",
    "    def _histogram(self, pts: np.ndarray):\n",
    "        hist, te, re = np.histogram2d(\n",
    "            pts[:, 0], pts[:, 1],\n",
    "            bins=[self.bins_theta, self.bins_r]\n",
    "        )\n",
    "        hist_s = gaussian_filter(hist, sigma=self.smooth_sigma)\n",
    "        return hist_s, te, re\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 2 – BFS ponderado para unir islas próximas\n",
    "    # ------------------------------------------------------------------\n",
    "    def _weighted_bfs(self, hist_s, te, re):\n",
    "        thr = np.percentile(hist_s[hist_s > 0], self.density_percentile)\n",
    "        mask = binary_closing(hist_s > thr,\n",
    "                              structure=np.ones((self.closing_size,\n",
    "                                                 self.closing_size)))\n",
    "        labels, n_lab = label(mask)\n",
    "\n",
    "        theta_res, r_res = te[1] - te[0], re[1] - re[0]\n",
    "        neigh = [(-1, 0), (1, 0), (0, -1), (0, 1),\n",
    "                 (-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
    "\n",
    "        parent = list(range(n_lab + 1))\n",
    "        def find(x):\n",
    "            while parent[x] != x:\n",
    "                parent[x] = parent[parent[x]]\n",
    "                x = parent[x]\n",
    "            return x\n",
    "        def union(a, b):\n",
    "            ra, rb = find(a), find(b)\n",
    "            if ra != rb:\n",
    "                parent[rb] = ra\n",
    "\n",
    "        h, w = labels.shape\n",
    "        dist_grid = np.full((h, w), np.inf)\n",
    "        pq: List[Tuple[float, int, int, int]] = []\n",
    "\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                lab = labels[i, j]\n",
    "                if lab:\n",
    "                    for di, dj in neigh:\n",
    "                        ni, nj = i + di, j + dj\n",
    "                        if 0 <= ni < h and 0 <= nj < w and labels[ni, nj] == 0:\n",
    "                            d0 = math.hypot(di * theta_res, dj * r_res)\n",
    "                            if d0 <= self.max_gap_dist and d0 < dist_grid[ni, nj]:\n",
    "                                dist_grid[ni, nj] = d0\n",
    "                                heapq.heappush(pq, (d0, ni, nj, lab))\n",
    "\n",
    "        while pq:\n",
    "            dist, i, j, lab = heapq.heappop(pq)\n",
    "            if dist > dist_grid[i, j] or dist > self.max_gap_dist:\n",
    "                continue\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i + di, j + dj\n",
    "                if 0 <= ni < h and 0 <= nj < w:\n",
    "                    lab2 = labels[ni, nj]\n",
    "                    if lab2 and lab2 != lab:\n",
    "                        union(lab, lab2)\n",
    "            for di, dj in neigh:\n",
    "                ni, nj = i + di, j + dj\n",
    "                if 0 <= ni < h and 0 <= nj < w and labels[ni, nj] == 0:\n",
    "                    nd = dist + math.hypot(di * theta_res, dj * r_res)\n",
    "                    if nd <= self.max_gap_dist and nd < dist_grid[ni, nj]:\n",
    "                        dist_grid[ni, nj] = nd\n",
    "                        heapq.heappush(pq, (nd, ni, nj, lab))\n",
    "        return find, labels\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 3 – Clusters y esqueletos\n",
    "    # ------------------------------------------------------------------\n",
    "    def _clusters_and_skeletons(self, pts, find, labels, te, re):\n",
    "        tidx = np.clip(np.digitize(pts[:, 0], te) - 1, 0, self.bins_theta - 1)\n",
    "        ridx = np.clip(np.digitize(pts[:, 1], re) - 1, 0, self.bins_r - 1)\n",
    "        merged: Dict[int, List[np.ndarray]] = {}\n",
    "        for p, (ti, ri) in enumerate(zip(tidx, ridx)):\n",
    "            lab = labels[ti, ri]\n",
    "            if lab:\n",
    "                merged.setdefault(find(lab), []).append(pts[p])\n",
    "        clusters = [np.vstack(v) for v in merged.values()\n",
    "                    if len(v) >= self.min_cluster_size]\n",
    "\n",
    "        skeletons: List[List[Tuple[float, float]]] = []\n",
    "        for cl in clusters:\n",
    "            thetas, rs = cl[:, 0], cl[:, 1]\n",
    "            bins_arr = np.arange(thetas.min(),\n",
    "                                 thetas.max() + self.theta_bin_size,\n",
    "                                 self.theta_bin_size)\n",
    "            ske = []\n",
    "            for k in range(len(bins_arr) - 1):\n",
    "                m = (thetas >= bins_arr[k]) & (thetas < bins_arr[k + 1])\n",
    "                if m.any():\n",
    "                    ske.append((thetas[m].mean(), rs[m].mean()))\n",
    "            skeletons.append(sorted(ske, key=lambda x: -x[0]))\n",
    "        return clusters, skeletons\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 4 – Centroides adicionales sobre *todas* las rectas candidatas\n",
    "    # ------------------------------------------------------------------\n",
    "    def _density_path(self, a, b, hist_s, te, re):\n",
    "        \"\"\"Genera centroides extra a lo largo del segmento a‑b.\"\"\"\n",
    "        theta_res, r_res = te[1] - te[0], re[1] - re[0]\n",
    "        steps = max(int(abs(b[0] - a[0]) / self.theta_step) + 1, 2)\n",
    "        thetas = np.linspace(a[0], b[0], steps)\n",
    "        rs_seg = np.linspace(a[1], b[1], steps)\n",
    "        radius = math.hypot(b[0] - a[0], b[1] - a[1]) * self.density_ratio\n",
    "\n",
    "        out: List[Tuple[float, float]] = []\n",
    "        for θ_i, r_i in zip(thetas, rs_seg):\n",
    "            ti = int(np.clip(np.digitize(θ_i, te) - 1, 0, hist_s.shape[0] - 1))\n",
    "            ri0 = int(np.clip(np.digitize(r_i, re) - 1, 0, hist_s.shape[1] - 1))\n",
    "            max_off = int(math.ceil(radius / r_res))\n",
    "            vals, offs = [], []\n",
    "            for off in range(-max_off, max_off + 1):\n",
    "                idx = ri0 + off\n",
    "                if 0 <= idx < hist_s.shape[1]:\n",
    "                    vals.append(hist_s[ti, idx]); offs.append(off)\n",
    "            for k in range(1, len(vals) - 1):\n",
    "                if vals[k] > vals[k - 1] and vals[k] > vals[k + 1] and vals[k] > 0:\n",
    "                    rc = re[ri0 + offs[k]] + r_res / 2.0\n",
    "                    if abs(rc - r_i) <= self.r_threshold:\n",
    "                        out.append((θ_i, rc))\n",
    "        return out\n",
    "\n",
    "    def _candidate_segments(self, skeletons):\n",
    "        segs = []\n",
    "        # 4a) internos (pares contiguos)\n",
    "        for ske in skeletons:\n",
    "            segs.extend(list(zip(ske, ske[1:])))\n",
    "        # 4b) mejores conexiones inter‑isla (criterio dr>0, dθ<0, distancia mínima)\n",
    "        for i, ske in enumerate(skeletons):\n",
    "            if not ske:\n",
    "                continue\n",
    "            a = ske[-1]\n",
    "            best, bd = None, math.inf\n",
    "            for j, ske2 in enumerate(skeletons):\n",
    "                if i == j or not ske2:\n",
    "                    continue\n",
    "                b = ske2[0]\n",
    "                dr, dtheta = b[1] - a[1], b[0] - a[0]\n",
    "                if dr > 0 and dtheta < 0:\n",
    "                    d = math.hypot(dr, dtheta)\n",
    "                    if d < bd:\n",
    "                        bd, best = d, (a, b)\n",
    "            if best:\n",
    "                segs.append(best)\n",
    "        return segs\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Paso 5 – Construir rutas (contours) y filtrar por Δr\n",
    "    # ------------------------------------------------------------------\n",
    "    def _build_contours(self, segs, hist_s, te, re):\n",
    "        contours: List[List[Tuple[float, float]]] = []\n",
    "        connections: List[Connection] = []\n",
    "        for a, b in segs:\n",
    "            path = self._density_path(a, b, hist_s, te, re)\n",
    "            route = [a] + path + [b]\n",
    "            contours.append(route)\n",
    "            # registrar Δr entre a y b (para estadística global)\n",
    "            connections.append(Connection(a, b, abs(b[1] - a[1]),\n",
    "                                           math.hypot(b[0] - a[0], b[1] - a[1])))\n",
    "        return contours, connections\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_by_threshold(contours, thr):\n",
    "        \"\"\"Parte las rutas cuando Δr > thr.\"\"\"\n",
    "        out = []\n",
    "        for cnt in contours:\n",
    "            tmp = [cnt[0]]\n",
    "            for a, b in zip(cnt, cnt[1:]):\n",
    "                if abs(b[1] - a[1]) <= thr:\n",
    "                    tmp.append(b)\n",
    "                else:\n",
    "                    if len(tmp) > 1:\n",
    "                        out.append(tmp)\n",
    "                    tmp = [b]\n",
    "            if len(tmp) > 1:\n",
    "                out.append(tmp)\n",
    "        return out\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Método público -----------------------------------------------------------------\n",
    "    # ------------------------------------------------------------------\n",
    "    def run(self, datos: Dict[str, Any]):\n",
    "        # Entrada esperada\n",
    "        bg = datos['background'][['theta', 'r']].values\n",
    "        grp = np.vstack([g['points'][['theta', 'r']].values\n",
    "                          for g in datos['grupos_finales']])\n",
    "        pts = np.vstack([bg, grp])\n",
    "\n",
    "        # 1) Histograma\n",
    "        hist_s, te, re = self._histogram(pts)\n",
    "\n",
    "        # 2) BFS + 3) clusters + esqueletos\n",
    "        find, labels = self._weighted_bfs(hist_s, te, re)\n",
    "        clusters, skeletons = self._clusters_and_skeletons(pts, find, labels, te, re)\n",
    "\n",
    "        # 4) segmentos candidatos + rutas con centroides extra\n",
    "        segs = self._candidate_segments(skeletons)\n",
    "        raw_contours, conns = self._build_contours(segs, hist_s, te, re)\n",
    "\n",
    "        # 5) filtrado por Δr\n",
    "        mean_dr = float(np.mean([c.delta_r for c in conns])) if conns else 0.0\n",
    "        thr = mean_dr * self.dr_multiplier\n",
    "        final_contours = self._split_by_threshold(raw_contours, thr)\n",
    "\n",
    "        # Objetos\n",
    "        objects = [IslandObject('isla-conexion-isla', cnt) for cnt in final_contours]\n",
    "        for c in conns:\n",
    "            if c.delta_r > thr:\n",
    "                objects.append(IslandObject('isla-conexiones-adicionales', [c.a, c.b]))\n",
    "\n",
    "        return {\n",
    "            'hist_s': hist_s, 'te': te, 're': re,\n",
    "            'clusters': clusters, 'skeletons': skeletons,\n",
    "            'contours': final_contours, 'connections': conns,\n",
    "            'mean_dr': mean_dr, 'threshold': thr, 'objects': objects\n",
    "        }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Utilidad rápida para visualizar\n",
    "    # ------------------------------------------------------------------\n",
    "    def quick_plot(self, result, figsize=(14, 6)):\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(result['hist_s'].T, origin='lower',\n",
    "                   extent=[result['te'][0], result['te'][-1],\n",
    "                           result['re'][0], result['re'][-1]],\n",
    "                   aspect='auto', cmap='inferno')\n",
    "        plt.colorbar(label='Densidad suavizada')\n",
    "        for idx, cl in enumerate(result['clusters']):\n",
    "            plt.scatter(cl[:, 0], cl[:, 1], s=8, alpha=0.6,\n",
    "                        label=f'Isla {idx}')\n",
    "        for ske in result['skeletons']:\n",
    "            arr = np.array(ske)\n",
    "            if arr.size:\n",
    "                plt.scatter(arr[:, 0], arr[:, 1], c='w', s=40,\n",
    "                            edgecolors='k')\n",
    "        for cnt in result['contours']:\n",
    "            xs, ys = zip(*cnt)\n",
    "            plt.plot(xs, ys, '-o', color='cyan', markersize=5)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  Importa el módulo y los datos de ejemplo\n",
    "# ------------------------------------------------------------\n",
    "#from histogram_segmenter import HistogramSegmenter   #  <-- el archivo del canvas\n",
    "#from your_data_module import main_example_modificado #  <-- tu función de carga\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Carga datos y crea la instancia con los parámetros deseados\n",
    "# ------------------------------------------------------------\n",
    "datos = main_example_modificado()          # debe devolver un dict con background y grupos_finales\n",
    "segmenter = HistogramSegmenter(            # puedes ajustar cualquier parámetro aquí\n",
    "    bins_theta=120,\n",
    "    bins_r=80,\n",
    "    smooth_sigma=1.0,\n",
    "    theta_bin_size=8,\n",
    "    dr_multiplier=2.5                      # 3.0 × μ para partir conexiones\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Ejecuta el algoritmo\n",
    "# ------------------------------------------------------------\n",
    "result = segmenter.run(datos)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  Consulta la salida estructurada\n",
    "# ------------------------------------------------------------\n",
    "print(\"Islas encontradas:\", len(result['clusters']))\n",
    "print(\"Promedio Δr:\", result['mean_dr'])\n",
    "print(\"Umbral usado:\", result['threshold'])\n",
    "print(\"Objetos clasificados:\", len(result['objects']))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5.  Visualiza (opcional)\n",
    "# ------------------------------------------------------------\n",
    "segmenter.quick_plot(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d413767a-7cea-43ae-a5d9-053d4e5953a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as PathEffects\n",
    "def plot_islands_and_paths_polar(result, datos, id_halo=\"11\", line_extrap=0.5):\n",
    "    \"\"\"\n",
    "    Función para crear un gráfico polar destacando las islas encontradas, los caminos de los centroides,\n",
    "    los contornos y añadiendo los puntos de fondo con un color gris muy tenue.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtrar islas grandes\n",
    "    big_islands = [cl for cl in result['clusters'] if len(cl) >= 60]\n",
    "    print(f\"Islas ≥60 puntos (polar): {len(big_islands)}\")\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 8))\n",
    "    ax = fig.add_subplot(111, projection='polar')\n",
    "\n",
    "    # Añadir puntos de fondo con gris muy tenue\n",
    "    bg = datos['background'][['theta', 'r']].values\n",
    "    ax.scatter(np.radians(bg[:, 0]), bg[:, 1], s=3, color='gray', edgecolor='black' ,alpha=0.21, label='Puntos de fondo')\n",
    "\n",
    "    # Colores para las islas\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'magenta', 'cyan', 'gold', 'gray', 'lime']\n",
    "\n",
    "    # Resaltar islas y caminos\n",
    "    for idx, cl in enumerate(big_islands):\n",
    "        c = colors[idx % len(colors)]\n",
    "        theta = np.radians(cl[:, 0])  # Convertir theta de grados a radianes\n",
    "        r = cl[:, 1]\n",
    "        ax.scatter(theta, r, s=10, color=c, alpha=0.34, label=f'Isla {idx+1} ({len(cl)})')\n",
    "\n",
    "        # Resaltar el camino de los centroides (esqueletos) con borde\n",
    "        for ske in result['skeletons']:\n",
    "            arr = np.array(ske)\n",
    "            if arr.size:\n",
    "                theta_ske = np.radians(arr[:, 0])  # Convertir theta a radianes\n",
    "                r_ske = arr[:, 1]\n",
    "                line = ax.plot(theta_ske, r_ske, color='yellow', alpha=0.95, linewidth=2, label='Camino centroides')\n",
    "\n",
    "                # Añadir borde a la línea\n",
    "                for l in line:\n",
    "                    l.set_path_effects([PathEffects.withStroke(linewidth=3, alpha=0.51,foreground='black')])\n",
    "\n",
    "        #Resaltar las conexiones entre los centroides\n",
    "        for cnt in result['contours']:\n",
    "            theta_cnt, r_cnt = zip(*cnt)\n",
    "            ax.plot(np.radians(theta_cnt), r_cnt, 'o', color='cyan',markersize=2.5, alpha=0.85, label='Contorno')\n",
    "            ax.plot(np.radians(theta_cnt), r_cnt, '-o', color='cyan',markersize=1.0, alpha=0.65, label='Contorno')\n",
    "\n",
    "        # Extrapolación de la línea (si es necesario)\n",
    "        if 'slope' in cl and cl['slope'] is not None:  # Usamos 'cl' aquí en lugar de 'g'\n",
    "            dt = cl['theta_max'] - cl['theta_min']\n",
    "            t_deg = np.linspace(cl['theta_min'] - line_extrap * dt, cl['theta_max'] + line_extrap * dt, 200)\n",
    "            r_line = cl['slope'] * t_deg + cl['intercept']\n",
    "            ax.plot(np.radians(t_deg), r_line, '.-', color=\"gray\", alpha=0.20)\n",
    "            ax.plot(np.radians(t_deg), r_line, '--', color=c)\n",
    "\n",
    "    # Configuración de los ejes polares\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_title(f\"Halo{id_halo} - Caminos de Centroides\", y=1.05)\n",
    "    ax.grid(True, alpha=0.1, color='gray')\n",
    "    #ax.legend(loc='upper right', bbox_to_anchor=(1.20, 1.0))\n",
    "\n",
    "    # Mostrar gráfico\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Llamada a la nueva celda después de ejecutar el algoritmo\n",
    "# ------------------------------------------------------------\n",
    "plot_islands_and_paths_polar(result, datos, id_halo=\"11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06c403a-a3ee-4b68-b935-8a329e5c5cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97136e7a-df9a-47c0-9547-88910f13ec96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# halo 418336....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b63ee1a-edd9-4e74-8bd3-d786f87c99b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 8) MAIN – pipeline completo subhalo 17 (modificado para almacenar grupos y background)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def main_example_modificado():\n",
    "    # Genera las semillas BFS y carga los datos filtrados\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"418336\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.525,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=1.80, gap_threshold_r=1.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "\n",
    "    # Visualizaciones (si se desea mantener)\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "\n",
    "    # Almacenar grupos finales: solo aquellos con 60 o más puntos\n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "\n",
    "    # Identificar los IDs de los puntos que pertenecen a los grupos finales.\n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        # Se espera que la columna \"id\" esté en los datos originales\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "\n",
    "    # Los puntos \"background\" son aquellos de df_f que no pertenecen a ningún grupo final.\n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "\n",
    "    # Se puede devolver un diccionario con los dos conjuntos de información:\n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 9) Run\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "if __name__ == \"__main__\":\n",
    "    datos_finales = main_example_modificado()\n",
    "    # Ahora, datos_finales['grupos_finales'] contiene los grupos con ≥ 60 pts, \n",
    "    # y datos_finales['background'] contiene la información original de los puntos\n",
    "    # que no fueron asignados a ningún grupo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e99abc0-3fa9-4fc2-96f8-c532bf1b09b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Carga datos y crea la instancia con los parámetros deseados\n",
    "# ------------------------------------------------------------\n",
    "datos = main_example_modificado()          # debe devolver un dict con background y grupos_finales\n",
    "segmenter = HistogramSegmenter(            # puedes ajustar cualquier parámetro aquí\n",
    "    bins_theta=120,\n",
    "    bins_r=80,\n",
    "    smooth_sigma=1.0,\n",
    "    theta_bin_size=8,\n",
    "    dr_multiplier=2.5                      # 3.0 × μ para partir conexiones\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Ejecuta el algoritmo\n",
    "# ------------------------------------------------------------\n",
    "result = segmenter.run(datos)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  Consulta la salida estructurada\n",
    "# ------------------------------------------------------------\n",
    "print(\"Islas encontradas:\", len(result['clusters']))\n",
    "print(\"Promedio Δr:\", result['mean_dr'])\n",
    "print(\"Umbral usado:\", result['threshold'])\n",
    "print(\"Objetos clasificados:\", len(result['objects']))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5.  Visualiza (opcional)\n",
    "# ------------------------------------------------------------\n",
    "segmenter.quick_plot(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485f9180-e432-4258-b4ec-d060e55fedcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# Llamada a la nueva celda después de ejecutar el algoritmo\n",
    "# ------------------------------------------------------------\n",
    "plot_islands_and_paths_polar(result, datos, id_halo=\"418336\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5de64589-b640-451e-a34a-69df97ee843f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### halo 198185\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e088e8-164c-4f99-bd8b-4aaf2c103bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 8) MAIN – pipeline completo subhalo 17 (modificado para almacenar grupos y background)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def main_example_modificado():\n",
    "    # Genera las semillas BFS y carga los datos filtrados\n",
    "    bfs, df_f = generate_bfs_seeds(\n",
    "        id_halo=\"198185\",\n",
    "        theta_min=0, theta_max=360,\n",
    "        quartile_threshold=0.625,\n",
    "        theta_diff=3.0, r_diff=0.5,\n",
    "        gap_threshold_theta=1.80, gap_threshold_r=1.0,\n",
    "        file_prefix='data_rho'\n",
    "    )\n",
    "    print(f\"BFS inicial: {len(bfs)} clusters\")\n",
    "\n",
    "    merged = adjust_and_merge_seeds(bfs, slope_variation_threshold=0.30, bounding_extrap=0.35)\n",
    "    print(f\"Tras fusión: {len(merged)} grupos\")\n",
    "\n",
    "    validated = validate_dispersion_and_reprocess(merged)\n",
    "    print(f\"Tras validación: {len(validated)} grupos\")\n",
    "\n",
    "    # Visualizaciones (si se desea mantener)\n",
    "    plot_groups_cartesian(validated, df_f, line_extrap=0.15)\n",
    "    plot_groups_polar(validated, df_f, line_extrap=0.15)\n",
    "\n",
    "    # Almacenar grupos finales: solo aquellos con 60 o más puntos\n",
    "    grupos_finales = [g for g in validated if len(g['points']) >= 60]\n",
    "\n",
    "    # Identificar los IDs de los puntos que pertenecen a los grupos finales.\n",
    "    puntos_grupos = set()\n",
    "    for g in grupos_finales:\n",
    "        # Se espera que la columna \"id\" esté en los datos originales\n",
    "        puntos_grupos.update(g['points']['id'].tolist())\n",
    "\n",
    "    # Los puntos \"background\" son aquellos de df_f que no pertenecen a ningún grupo final.\n",
    "    background = df_f[~df_f['id'].isin(puntos_grupos)].copy()\n",
    "    print(f\"Puntos background: {len(background)}\")\n",
    "\n",
    "    # Se puede devolver un diccionario con los dos conjuntos de información:\n",
    "    return {'grupos_finales': grupos_finales, 'background': background}\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# 9) Run\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "if __name__ == \"__main__\":\n",
    "    datos_finales = main_example_modificado()\n",
    "    # Ahora, datos_finales['grupos_finales'] contiene los grupos con ≥ 60 pts, \n",
    "    # y datos_finales['background'] contiene la información original de los puntos\n",
    "    # que no fueron asignados a ningún grupo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f4279f0-117b-4a9d-8ea2-2b7168b2568f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Carga datos y crea la instancia con los parámetros deseados\n",
    "# ------------------------------------------------------------\n",
    "datos = main_example_modificado()          # debe devolver un dict con background y grupos_finales\n",
    "segmenter = HistogramSegmenter(            # puedes ajustar cualquier parámetro aquí\n",
    "    bins_theta=120,\n",
    "    bins_r=80,\n",
    "    smooth_sigma=1.0,\n",
    "    theta_bin_size=8,\n",
    "    dr_multiplier=2.5                      # 3.0 × μ para partir conexiones\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Ejecuta el algoritmo\n",
    "# ------------------------------------------------------------\n",
    "result = segmenter.run(datos)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  Consulta la salida estructurada\n",
    "# ------------------------------------------------------------\n",
    "print(\"Islas encontradas:\", len(result['clusters']))\n",
    "print(\"Promedio Δr:\", result['mean_dr'])\n",
    "print(\"Umbral usado:\", result['threshold'])\n",
    "print(\"Objetos clasificados:\", len(result['objects']))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5.  Visualiza (opcional)\n",
    "# ------------------------------------------------------------\n",
    "segmenter.quick_plot(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e60ec1e-c50c-48e7-8d07-6652cb7738cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# Llamada a la nueva celda después de ejecutar el algoritmo\n",
    "# ------------------------------------------------------------\n",
    "plot_islands_and_paths_polar(result, datos, id_halo=\"198185\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30ecba0e-362c-423a-8c65-9a2f5f225f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc46c91a-6f45-4766-bb1c-cdd00bee54b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Prueba_ArmTracing_PolarSpace",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}